---
title: "ACCESSPACE"
subtitle: "Helping Visually Impaired People travel autonomously"
date: 2017-01
doi: "CCAH & FIRAH" # TODO: make specific param

author:
  - "[LITIS (Rouen, France)](https://www.litislab.fr/en/)"
  - "[CERREV (Caen, France)](http://ufrhss.unicaen.fr/recherche/cerrev/)"

image: feature.png
image-alt: "Logo of the ACCESSPACE project"

description: "Developing a wearable vibro-tactile electronic Orientation & Travel Aid for the autonomous navigation of VIP, based on Spatial Cognition models."

website:
  open-graph:
    image: feature.png
    description: "Wearable vibro-tactile Electronic Travel Aid for the autonomous navigation of VIP"
  twitter-card:
    image: feature.png
    description: "Wearable vibro-tactile Electronic Travel Aid for the autonomous navigation of VIP"

abstract: |
  The ACCESSPACE project aims to develop an electronic Orientation and Travel Aid that will provide Visually Impaired People (VIP) with a dynamic and specialized representation of their environment‚Äôs spatial topography, through egocentric vibrotactile feedback. Information on the user's location & surroundings will be acquired using their smartphone, processed, and communicated back to them through a waist belt fitted with vibrators. With some training, this substituted spatial information will allow VIP to intuitively form mental maps of their surroundings, and navigate autonomously towards their destination.

categories:
  - "Assistive Devices"
  - "Accessibility"
  - "Visual Impairment"
  - "Augmented Reality"
  - "Sensory Substitution"
  - "Haptic Interface"
  - "Computer Vision"

# Dirty trick to get some links/buttons
about:
  links:
    - text: "Project's homepage ({{< iconify twemoji flag-france >}})"
      icon: globe
      url: http://accesspace.univ-rouen.fr/index.php
      aria-label: "See the project's official website (in French)"
    - text: "Conference Article"
      url: content/pubs/ICCHP18-TactiBelt/
      icon: file-pdf
      aria-label: "See a conference article about the TactiBelt"
    - text: "Journal Article"
      url: content/pubs/JEP22/
      icon: file-pdf
      aria-label: "See a journal article about the TactiBelt"
    - text: "{{< iconify file-icons keynote >}} Presentation ({{< iconify twemoji flag-france >}})"
      url: content/outreach/talks/RUNN19/
      aria-label: "See a presentation illustrating the project (in French)"
    - text: "Poster ({{< iconify twemoji flag-france >}})"
      icon: file-image
      url: content/projects/ACCESSPACE/poster.pdf
      aria-label: "See a poster illustrating the project (in French)"
---

<hr style="margin-bottom: 30px; margin-top: -12px">

![](header.jpg){.preview-image fig-alt="Banner illustrating the project"}

# Introduction
***

ACCESSPACE's high-level goal was to allow VIP to navigate indoors or outdoors in autonomy, helping them intuitively perceive where they are, where they want to go, to choose how to get there, and avoid the incoming obstacles on their way. 

**ACCESSPACE had three main axes of research:**  
**1)** Devise an intuitive and efficient way to provide real-time spatial cues through tactile signals  
**2)** Design a haptic interface that allows to communicate said spatial representation easily to the user  
**3)** Implement the software require to support the various functions of this interface (e.g. Indoor Localisation, Obstacle Detection, Mapping, ...)   

The guiding theoretical principle of this project was to use the brain's navigation system as an inspiration source for what information to provide VIP to make navigation the most intuitive. The gist of the idea is that our (biological) **navigation system** combine and distills information from multiple senses into a set of key **spatial properties** (e.g. where the center of the current room is located). This **spatial representation** allows us to reason on the structure of our environment, and to navigate it efficiently. For VIP, this process is impaired due to our navigation system's heavy reliance on visual information. To make our assistive device more intuitive, instead of substituing vision in its entirety, we focus on directly providing the information that our navigation system would extract from vision.

![ACCESSPACE guiding principle](neuro.png){#fig-neuro fig-alt="Illustration showing the general theoretical principles behind ACCESSPACE transcoding principles"}

Using those principles, we devised an **encoding scheme** that provides the user with 3 types of information through egocentered tactile feedback:

- The orientation and distance to the destination of the journey (as the crow flies)  
- The available path possibilities around the user (i.e. the various branching streets they could take, the current room's center), which form a navigation graph, and will allow the VIP to mentally visualize the layout of its immediate environment  
- The closest obstacles

![Illustration of the navigation graph idea](nav.png "An illustration showcasing the navigation graph used with the TactiBelt"){fig-alt="An illustration showcasing the navigation graph used with the TactiBelt" fig-align="center" width=60%}


# Outcomes
***

## Our interface: the TactiBelt

To provide the proposed egocentric encoding scheme to the user, we designed a vibro-tactile belt, the **TactiBelt**: it comprises of 46 ERM motors spread into three layers, controlled by an Arduino Mega, through a specialized software written in Java:

![First prototype of the TactiBelt](TactiBelt.png "A photography showing the first version of the TactiBelt"){#fig-tactibelt-v1 fig-alt="A photography showing the first version of the TactiBelt" fig-align="center"}


## Software tools

To capture and extract the information we need from the VIP's environment, we devised a series of software tools relying mostly on Computer Vision:

**1)** Obstacle detection and indoor localisation using the [ORB-SLAM](https://github.com/raulmur/ORB_SLAM2) algorithm:

![](ORB-SLAM.png "Image showing the ORB-SLAM algorithm running inside our office"){fig-alt="Image showing the ORB-SLAM algorithm running inside our office" width=60% fig-align="center"}

**2)** Depth estimation from a monocular RGB camera, using the [MonoDepth](https://github.com/mrharicot/monodepth) algorithm:

![](MonoDepth.png "Image showing the MonoDepth algorithm running just outside our lab"){fig-alt="Image showing the MonoDepth algorithm running just outside our lab" width=60% fig-align="center"}

**3)** Generating a mobility graph of the environment during movement using Reinforcement Learning:

Applied to an artificial agent exploring a virtual maze, looking for food:

![](graph.png "Image showing the navigation graph generated by an artificial agent exploring a virtual maze"){fig-alt="Image showing the navigation graph generated by an artificial agent exploring a virtual maze"}

Applied to a real agent (human pushing a cart with a camera and the computer running the algorithm around a meeting table):

![](graph2.png "Image showing the navigation graph generated by moving the camera around a metting table in a room"){fig-alt="Image showing the navigation graph generated by moving the camera around a metting table in a room" fig-align="center"}

**4)** A virtual environment to test the TactiBelt and our candidate spatial encoding schemes:

![](VR.png){fig-alt="A screenshot of our virtual environment where the player has to find a virtual target solely relying on tactile feedback"}

## Project dissemination

The ACCESSPACE project was promoted in various mainstream and technical media, such as:  
- On [RTL](https://www.rtl.fr/actu/sciences-tech/tactbelt-la-ceinture-qui-aide-les-non-voyants-a-se-diriger-7797538766), a French national radio station  ({{< iconify twemoji flag-france >}})  
- On [PhDTalent](https://app.phdtalent.fr/publication/accesspace-un-outil-pour-ce-deplacer-sans-la-vue_3/details), a platform and network for PhD Students who wish to transition to industry ({{< iconify twemoji flag-france >}})  
- On [Guide N√©ret](https://guideneret.com/content/une-ceinture-intelligente-cr%c3%a9%c3%a9e-pour-que-les-malvoyants-se-d%c3%a9placent-sans-la-vue), a specialized website on Handicap in France ({{< iconify twemoji flag-france >}})  
- On [Acuit√©](https://www.acuite.fr/actualite/sante/157415/un-nouveau-dispositif-ad-hoc-pour-les-malvoyants), a specialized website dedicated to Opticians and news around visual impairment ({{< iconify twemoji flag-france >}})  
- On [Oxytude](https://www.oxytude.org/hebdoxytude-129-lactualite-de-la-semaine-en-technologies-et-accessibilite/), a weekly podcast reviewing news related to visual impairment ({{< iconify twemoji flag-france >}})  
- On [FIRAH](https://www.firah.org/fr/accesspace.html), the French Foundation on Applied Research for Handicap ({{< iconify twemoji flag-france >}})

This project has been warmly welcomed by the VIP community and was awarded the **‚ÄúApplied research on disability‚Äù award from the CCAH** in 2017 ü•á.


# My role in this project
***

**ACCESSPACE was my main PhD project:**

**1)** Managed the literature review for all axes of the project (spatial cognition, assistive devices, sensory substitution, computer vision, ...).

**2)** Designed the TactiBelt and participated in its conception (Arduino).

**3)** Participated in the development of a [Java application](https://github.com/ma-riviere/BeltControl) to control and test the TactiBelt.

**4)** Handled the preliminary experimental evaluations of the TactiBelt.

**5)** Presenting the project a a [journal paper](/content/pubs/JEP22/), a [conference paper](/content/pubs/ICCHP18-TactiBelt/), a talk at an [international conference](/content/outreach/talks/ICCHP18/), and various outreach events (see the *dissemination section* above for more details).
