---
title: "ACCESSPACE"
subtitle: "Helping Visually Impaired People travel autonomously"
date: 2017-01

author:
  - name: "LITIS"
    url: "https://www.litislab.fr/"
  - name: "CERREV"
    url: "https://cerrev.unicaen.fr/"

image: feature.png
image-alt: "Logo of the ACCESSPACE project"

description: "Developing a wearable vibro-tactile electronic wearable system for the autonomous navigation of visually impaired people through neuroscience-inspired spatial feedback."

website:
  open-graph:
    image: feature.png
    description: "Neuroscience-inspired vibro-tactile wearable for autonomous navigation by visually impaired people"
  twitter-card:
    image: feature.png
    description: "Neuroscience-inspired vibro-tactile wearable for autonomous navigation by visually impaired people"

abstract: |
  The ACCESSPACE project developed an innovative electronic Orientation and Travel Aid (ETA) that enables Visually Impaired People (VIP) to navigate autonomously through intuitive vibro-tactile feedback. Drawing inspiration from neuroscientific understanding of spatial cognition, the project created the TactiBelt, a wearable device with 46 vibro-tactile motors arranged in three layers around the waist. Unlike traditional ETAs that attempt to substitute vision entirely, ACCESSPACE directly provides the spatial properties that the brain's navigation system requires: destination orientation and distance, available path options, and obstacle locations. The system integrates advanced computer vision algorithms including ORB-SLAM for indoor localization, MonoDepth for depth estimation, and reinforcement learning for dynamic navigation graph generation. Preliminary evaluations demonstrated users' ability to form mental maps and navigate virtual environments using only tactile feedback. This award-winning research (CCAH Applied Research on Disability Award 2017) represents a paradigm shift from sensory substitution to spatial cognition augmentation, offering a more intuitive approach to assistive navigation technology.

categories:
  - Research
  - Software Engineering
  - Java
  - Arduino
  - Human-Computer Interaction
  - Augmented Reality
  - Computer Vision

# Dirty trick to get some links/buttons
about:
  template: solana
  links:
    - text: "Official homepage [{{< iconify twemoji flag-france >}}]"
      icon: globe
      url: http://accesspace.univ-rouen.fr/index.php
      aria-label: "See the project's official website (in French)"
    - text: "Article"
      file: content/pubs/ICCHP18-TactiBelt/
      icon: file-pdf
      aria-label: "See a conference article about the TactiBelt"
    - text: "{{< iconify file-icons keynote >}} Slides [{{< iconify twemoji flag-france >}}]"
      file: content/projects/ACCESSPACE/RUNN19.pdf
      aria-label: "See a presentation illustrating the project (in French)"
    - text: "Poster [{{< iconify twemoji flag-france >}}]"
      icon: file-image
      file: content/projects/ACCESSPACE/poster.pdf
      aria-label: "See a poster illustrating the project (in French)"
---

{{< include /content/_hr.qmd >}}

![](header.jpg){fig-alt="Banner illustrating the project"}

# Summary

ACCESSPACE's high-level goal was to allow VIP to navigate indoors or outdoors in autonomy, helping them intuitively perceive where they are, where they want to go, to choose how to get there, and avoid the incoming obstacles on their way. 

**ACCESSPACE had three main axes of research:**  
**1)** Devise an intuitive and efficient way to provide real-time spatial cues through tactile signals  
**2)** Design a haptic interface that allows to communicate said spatial representation easily to the user  
**3)** Implement the software require to support the various functions of this interface (e.g. Indoor Localisation, Obstacle Detection, Mapping, ...)   

The guiding theoretical principle of this project was to use the brain's navigation system as an inspiration source for what information to provide VIP to make navigation the most intuitive. The gist of the idea is that our (biological) **navigation system** combines and distills information from multiple senses into a set of key **spatial properties** (e.g. where the center of the current room is located). This **spatial representation** allows us to reason on the structure of our environment, and to navigate it efficiently. For VIP, this process is impaired due to our navigation system's heavy reliance on visual information. To make our assistive device more intuitive, instead of substituing vision in its entirety, we focus on directly providing the information that our navigation system would extract from vision.

![ACCESSPACE guiding principle](neuro.png){#fig-neuro fig-alt="Illustration showing the general theoretical principles behind ACCESSPACE transcoding principles" width=80% fig-align="center"}

Thus, we devised an **encoding scheme** that provides the user with 3 types of information through egocentered tactile feedback:  
**1)** The orientation and distance to the destination of the journey (as the crow flies)  
**2)** The available path possibilities around the user (i.e. the various branching streets they could take, the current room's center)  
**3)** The closest obstacles

![Illustration of the navigation graph idea](nav.png "An illustration showcasing the navigation graph used with the TactiBelt"){fig-alt="An illustration showcasing the navigation graph used with the TactiBelt" fig-align="center" width=60%}

::: {.callout-note appearance="simple"}

# My role in this project

**1)** Managed the literature review for all axes of the project (spatial cognition, assistive devices, sensory substitution, computer vision, ...).

**2)** Designed the TactiBelt and participated in its conception (Arduino).

**3)** Participated in the development of a [Java application](https://github.com/ma-riviere/BeltControl) to control and evaluate the TactiBelt in a pacman-like game.

**4)** Handled the preliminary experimental evaluations of the TactiBelt, and the analyses of the collected data.

**5)** Was involved in writing a [journal paper](/content/pubs/JEP22/) [@faugloire2022] and a [conference paper](/content/pubs/ICCHP18-TactiBelt/) [@rivi√®re2018]

**6)** Disseminated the project through a talk at an international conference, and various outreach events.

:::

::: {.column-margin}
:::: {.callout-note appearance="simple"}
### Was awarded the **‚ÄúApplied research on disability‚Äù award from the CCAH** in 2017 ü•á
::::
:::

# Details

## Our interface: the TactiBelt

To provide the proposed egocentric encoding scheme to the user, we designed a vibro-tactile belt, the **TactiBelt**: it comprises of 46 ERM motors spread into three layers, controlled by an Arduino Mega, through a specialized software written in Java:

![First prototype of the TactiBelt](TactiBelt.png "A photography showing the first version of the TactiBelt"){#fig-tactibelt-v1 fig-alt="A photography showing the first version of the TactiBelt" fig-align="center"}


## Software tools

To capture and extract the information we need from the VIP's environment, we devised a series of software tools relying mostly on Computer Vision:

**1)** Obstacle detection and indoor localisation using the [ORB-SLAM](https://github.com/raulmur/ORB_SLAM2) algorithm:

![](ORB-SLAM.png "Image showing the ORB-SLAM algorithm running inside our office"){fig-alt="Image showing the ORB-SLAM algorithm running inside our office" width=60% fig-align="center"}

**2)** Depth estimation from a monocular RGB camera, using the [MonoDepth](https://github.com/mrharicot/monodepth) algorithm:

![](MonoDepth.png "Image showing the MonoDepth algorithm running just outside our lab"){fig-alt="Image showing the MonoDepth algorithm running just outside our lab" width=60% fig-align="center"}

**3)** Generating a mobility graph of the environment during movement using Reinforcement Learning:

Applied to an artificial agent exploring a virtual maze, looking for food:

![](graph.png "Image showing the navigation graph generated by an artificial agent exploring a virtual maze"){fig-alt="Image showing the navigation graph generated by an artificial agent exploring a virtual maze"}

Applied to a real agent (human pushing a cart with a camera and the computer running the algorithm around a meeting table):

![](graph2.png "Image showing the navigation graph generated by moving the camera around a metting table in a room"){fig-alt="Image showing the navigation graph generated by moving the camera around a metting table in a room" fig-align="center"}

**4)** A virtual environment to test the TactiBelt and our candidate spatial encoding schemes:

![](VR.png){fig-alt="A screenshot of our virtual environment where the player has to find a virtual target solely relying on tactile feedback"}

# Project dissemination

The ACCESSPACE project was promoted in various mainstream and technical media, such as:  
- On [RTL](https://www.rtl.fr/actu/sciences-tech/tactbelt-la-ceinture-qui-aide-les-non-voyants-a-se-diriger-7797538766), a French national radio station  ({{< iconify twemoji flag-france >}})  
- On [PhDTalent](https://app.phdtalent.fr/publication/accesspace-un-outil-pour-ce-deplacer-sans-la-vue_3/details), a platform and network for PhD Students who wish to transition to industry ({{< iconify twemoji flag-france >}})  
- On [Guide N√©ret](https://guideneret.com/content/une-ceinture-intelligente-cr%c3%a9%c3%a9e-pour-que-les-malvoyants-se-d%c3%a9placent-sans-la-vue), a specialized website on Handicap in France ({{< iconify twemoji flag-france >}})  
- On [Acuit√©](https://www.acuite.fr/actualite/sante/157415/un-nouveau-dispositif-ad-hoc-pour-les-malvoyants), a specialized website dedicated to Opticians and news around visual impairment ({{< iconify twemoji flag-france >}})  
- On [Oxytude](https://www.oxytude.org/hebdoxytude-129-lactualite-de-la-semaine-en-technologies-et-accessibilite/), a weekly podcast reviewing news related to visual impairment ({{< iconify twemoji flag-france >}})  
- On [FIRAH](https://www.firah.org/fr/accesspace.html), the French Foundation on Applied Research for Handicap ({{< iconify twemoji flag-france >}})
