---
title: "SAM-Guide"
subtitle: "Spatial Awareness for Multimodal Guidance"
date: 2021-05

author:
  - name: "LPNC"
    url: "https://lpnc.univ-grenoble-alpes.fr/"
  - name: "GIPSA-Lab"
    url: "https://gipsa-lab.grenoble-inp.fr/"
  - name: "CMAP"
    url: "https://portail.polytechnique.edu/cmap/en"
  - name: "LITIS"
    url: "https://www.litislab.fr/"
  - name: "CERREV"
    url: "https://cerrev.unicaen.fr/"

image: feature.png
image-alt: "Image illustrating the SAM-Guide project"

description: "Designing an efficient multi-modal interface to help VIP during spatial interactions and sports."

website:
  open-graph:
    image: feature.png
    description: "An efficient multi-modal interface to help VIP during spatial interactions and sports"
  twitter-card:
    image: feature.png
    description: "An efficient multi-modal interface to help VIP during spatial interactions and sports"

abstract: |
  SAM-Guide’s high level objective is to efficiently assist Visually Impaired People (VIP) in tasks that require interactions with space. It aims to develop a multimodal interface to assist VIP during different types of spatial interactions, from object reaching, large-scale navigation (indoor and outdoor) to outdoor sports activities (e.g. laser-run). It also aims to study and model how to optimally supplement vision with both auditory and tactile feedback, reframing spatial interactions as target-reaching affordances, and symbolizing spatial properties by 3D ego-centered beacons. Candidate encoding schemes will be evaluated through Augmented Reality (AR) serious games relying on motion capture platforms and indoor localisation solutions to track the user’s movements.
  
  SAM-Guide is a inter-disciplinary collaboration project (ANR 2021 PRC) between three sites: (1) the [LPNC](https://lpnc.univ-grenoble-alpes.fr/) and [GIPSA](http://www.gipsa-lab.fr/en/about-gipsa-lab.php) laboratories from the Grenoble-Alpes University, (2) the [CMAP](https://portail.polytechnique.edu/cmap/en) from Ecole Polytechnique in Paris-Saclay, and the [LITIS](https://www.litislab.fr/en/) and [CERREV](http://ufrhss.unicaen.fr/recherche/cerrev/) from Normandy University.

categories:
  - Research
  - Software Engineering
  - C#
  - Unity
  - Human-Computer Interaction
  - Augmented Reality

# Dirty trick to get some links/buttons
about:
  template: solana
  links:
    - text: "Official homepage"
      icon: globe
      url: https://sam-guide.univ-rouen.fr/index.html
      aria-label: "See the project's official website"
    - text: "ANR Grant"
      icon: globe
      url: https://anr.fr/Project-ANR-21-CE33-0011
      aria-label: "See the ANR grant official summary"
    - text: "Code"
      icon: code
      url: https://github.com/sam-guide/exp-plat
      aria-label: "See the project's code"
---

{{< include /content/_hr.qmd >}}

![](feature.png){fig-alt="Banner illustrating the project"}

The SAM-Guide project aims to help visually impaired people interact with the world around them by creating smart devices that convert visual information into sounds and vibrations they can feel and hear. Instead of trying to replace vision entirely, the project focuses on giving people the specific spatial information they need for everyday tasks like finding objects, navigating spaces, and even participating in sports activities. The team of researchers from multiple French universities is developing wearable devices like vibrating belts and audio systems that can guide users toward targets or help them understand their surroundings. They're testing these technologies through virtual reality games and real-world activities, including a new sport called laser-run designed for people with visual impairments. The ultimate goal is to create a common "language" of sounds and vibrations that can help visually impaired people gain more independence in various activities, from simple daily tasks to recreational sports.

::: {.callout-note appearance="simple"}

# My role in this project

**1)** I was a major actor behind the birth of this project, by connecting the consortium members together and writing most of the grant proposal (ANR AAPG 2021, funding of 609k€).

**2)** I designed and participated in the development of the second prototype of our vibro-tactile belt, which features wireless communication (thanks to an ESP32 module) and amovible vibrators:

![Second prototype of the TactiBelt](TactiBelt-v2.jpg "Photography of the second iteration of the TactiBelt"){#fig-tactibelt-v2 fig-alt="Photography of the second iteration of the TactiBelt"}

**3)** I lead the design and development of the project's experimental platform. The platform uses Unity, connects to various motion tracking devices used by the consortium (Polhemus, VICON, pozyx), uses [PureData](https://puredata.info/) for sound-wave generation and [Steam Audio](https://valvesoftware.github.io/steam-audio/) for 3D audio modeling, and communicates with the consortium's non-visual interfaces wirelessly.

::::{#fig-unity layout='[503,-5,492]'}

![Testing environment with a PureData audio beacon](Unity.png){#fig-unity-testenv fig-alt="Screenshot of the testing environment of the experimental platform of SAM-Guide"}

![Auto-generated maze with 3D audio beacons on waypoints](Unity-maze.png){#fig-unity-maze fig-alt="Screenshot of the maze generator of the experimental platform of SAM-Guide"}

Screenshots from SAM-Guide's experimental platform *(in development)*

::::

This platform allows one to **easily spin up experimental trials** by specifying the desired characteristics in a JSON file (based on the [OpenMaze](https://openmaze.duncanlab.org/documentation) project). Unity will automatically generate the trial's environment according to those specifications and populate it with the relevant items (e.g. a tactile-signal emitting beacon signalling a target to reach in a maze), handle the transition between successive trials and blocks of trials, and log all the relevant user metrics into a data file.

::::{#fig-specs layout='[505,-5,490]'}

![Specifying the avatar and the experimental blocks' characteristics](Unity-protocol 1.png){#fig-unity-protocol1 fig-alt="Screenshot of the experimental protocol file specifying the avatar and the experimental blocks' characteristics"}

![Specifying experimental trials, which can be repeated and randomized within blocks](Unity-protocol 3.png){#fig-unity-protocol3 fig-alt="Screenshot of the experimental protocol file specifying experimental trials, which can be repeated and randomized within blocks"}

Examples of settings used to generate experimental trials on the fly.

::::

**4)** Handled the experimental design of the first wave of experiments using the TactiBelt for "blind" navigation.

**5)** Built the first version of the SAM-Guide's website, using Quarto and hosted on GitHub Pages ([code](https://github.com/sam-guide/sam-guide.github.io)).

:::
