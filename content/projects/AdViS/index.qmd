---
title: "AdViS"
subtitle: "Adaptative Visual Substitution"
date: 2014-07

author:
  - name: "Psychology and NeuroCognition Lab (LPNC)"
    url: "https://lpnc.univ-grenoble-alpes.fr/"
  - name: "GIPSA-Lab"
    url: "https://gipsa-lab.grenoble-inp.fr/"

image: feature.png
image-alt: "Schema illustrating the AdViS project"

description: "Developing a wearable visuo-auditive substitution system to assist Visually Impaired People in navigation and object-reaching tasks."

abstract: |
  The AdViS project aims to develop a wearable device that will assist Visually Impaired People during **spatial interaction tasks** (such as **indoor navigation** and **object-reaching**), using an audio-based Human-Machine Interface which converts relevant spatial metrics into ergonomic auditory cues.

website:
  open-graph:
    image: feature.png
    description: "Wearable visuo-auditive substitution system for blind navigation and object-reaching"
  twitter-card:
    image: feature.png
    description: "Wearable visuo-auditive substitution system for blind navigation and object-reaching"

categories:
  - "Assistive Devices"
  - "Accessibility"
  - "Visual Impairment"
  - "Augmented Reality"
  - "Sensory Substitution"
  - "Auditory Interface"
  - "Computer Vision"

# Dirty trick to get some links/buttons
about:
  links:
    - text: "Official homepage [{{< iconify twemoji flag-france >}}]"
      icon: globe
      url: https://persyval-lab.org/fr/exploratory-project/myssiv-mouvements-des-yeux-substitution-sensorielle-immersion-virtuelle
      aria-label: "See the project's official website (in French)"
    - text: "{{< ai hal >}} Article (2015)"
      url: https://hal.archives-ouvertes.fr/hal-01128384/document
      aria-label: "See the PDF of a journal article illustrating the project (2015)"
    - text: "{{< ai hal >}} Article (2018)"
      url: https://hal.archives-ouvertes.fr/hal-01663686/document
      aria-label: "See the PDF of a journal article illustrating the project (2018)"
    - text: "Poster"
      icon: file-image
      file: /content/projects/AdViS/ADVIS.pdf
      aria-label: "See the PDF of a poster illustrating the project"
---

{{< include /content/_hr.qmd >}}

![](feature.png){.preview-image fig-alt="Image illustrating the project"}

# Introduction
***

AdViS aims to explore various ways to provide visuo-spatial information through auditory feedback, based on the Sensory Substitution framework. Since its inception, the project investigated visuo-auditive substitution possibilities for multiple tasks in which vision plays a crucial role:

-   Navigating a small maze using a depth-map-to-auditory-cues transcoding  
-   Finger-guided image exploration (on a touch screen)  
-   Eye-movement guided image exploration (on a turned off computer screen)  
-   Pointing towards and reaching a virtual target in 3D space using a motion capture environment

:::{layout="[47,53]"}

![AdViS - Depth Map navigation](advis-diagram.png){alt="AdViS system's operating diagram"}

![AdViS - Motion Capture](feature.png){.preview-image alt="AdViS system's operating diagram with motion capture"}

:::

The AdViS system is coded in C++, uses [PureData](https://puredata.info/) for complex sound generation, and relies on the [VICON](https://www.vicon.com/) system to track participant's movements in an augmented reality environment.


# My role in this project
***

**1)**  Proposed a new model for image exploration relying on a touch-mediated visuo-auditive feedback loop, where a VIP explores an image by moving its finger across a screen and gets audio feedback based on the contents of the explored region.

**2)**  Modified the existing AdViS system to include the ability to transcode grey-scale images into soundscapes, based on captured finger-motion information on a touchscreen.

**3)**  Organized experimental evaluations with blindfolded students, tasked with recognizing geometrical shapes on a touchscreen, and analyzed the results.

![](geom-finger.jpg){alt="Participant exploring a geometrical shape by moving their finger on a touchscreen"}

**4)**  Participated in implementing an occular-motion-to-audio-feedback loop in order to evaluate the possibility of exploring images (on a turned off screen) with eye-movements (which are still controllable by most of the non-congenital VIP).

![](geom-eye.jpg){alt="Participant exploring a geometrical shape by moving their eyes across a black computer screen"}
