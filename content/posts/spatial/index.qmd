---
title: "Fast spatial data matching in R"
subtitle: "How to match locations based on their coordinates"

author:
  name: "Marc-Aurèle Rivière"
  orcid: 0000-0002-5108-3382

date: 2022-06-18

abstract: |
  This document was prompted by [a reddit question](https://www.reddit.com/r/rstats/comments/vdwi6o/finding_records_based_on_approximate_lattitude/) on how to fill missing location names by matching them to other known locations by their geographical proximity (using lat/long coordinates). The question is apparently linked to a case study of **Google's Data Analytics certificate**.  
  
  The gist of it is that the proposed matching won't yield anything with this particular dataset. It appears that the locations missing a name or id are missing one *because* their coordinates lack the precision necessary to be reliably matched to a known station by proximity alone.
  
  This post showcases different solutions/packages to achieve the distance-based matching, with timings to compare their speed. The SQL (DuckDB) & `dbplyr` solutions are on-disk (instead of in-memory), meaning they sacrifice some speed but could handle bigger-than-RAM datasets.

website:
  open-graph:
    description: "Exploring various solutions to quickly match locations by their geographical proximity in R"
  twitter-card:
    description: "Exploring various solutions to quickly match locations by their geographical proximity in R"

categories:
  - "Big Data"
  - "Data Manipulation"
  - "Spatial"
  - "R"
  - "SQL"
  - "DuckDB"

format:
  html:
    code-tools:
      source: true
      toggle: false
---

<hr style="margin-bottom: 30px; margin-top: -12px">

:::{.callout-tip}
You can check the source code by clicking on the **</> Code** button at the top-right.

Data can be found [here](https://divvy-tripdata.s3.amazonaws.com/index.html) (June 2021 to May 2022).
:::

:::{.callout-tip collapse="true"}

# Expand for Version History

- **V1:** 2022-06-18  
- **V2:** 2022-10-29   
  - Added a `DuckDB` + `arrow` example for the data loading.  
  - Added data cleaning section (removing implausible rides).  
  - Added `sf`, `dbplyr` and `data.table` (manual) examples for the spatial join.
:::


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
# Setup {.unnumbered}
***

```{r}
#| echo: false
#| output: false

source(here::here("src", "init_min.R"), echo = FALSE)

config <- config::get(file = here("_config.yml"))
```

```{r}
#| echo: false
#| eval: false

renv::install(c(
  "here",
  "fs",
  "pipebind",
  "Rdatatable/data.table", # >= 1.14.5
  "Tidyverse/dplyr",
  "tidyr",
  "dtplyr",
  "DBI",
  "Tidyverse/dbplyr",
  "arrow",
  "duckdb",
  "stringr",
  "Tidyverse/purrr", # >= 1.0.0 or dev version (0.9000)
  "fuzzyjoin",
  "r-spatial/sf"
))
```

```{r}
#| output: false

library(here)        # File path management
library(fs)          # File & folder manipulation
library(pipebind)    # Piping goodies

library(readr)       # Reading data from files           (Tidyverse)
library(dplyr)       # Manipulating data.frames - core   (Tidyverse)
library(tidyr)       # Manipulating data.frames - extras (Tidyverse)
library(stringr)     # Manipulating strings              (Tidyverse)
library(purrr)       # Manipulating lists                (Tidyverse)
library(lubridate)   # Manipulating date/time            (Tidyverse)

library(furrr)       # Manipulating lists in parallel

library(data.table)  # Fast data manipulation
library(dtplyr)      # data.table backend for dplyr      (Tidyverse)

library(DBI)         # Database connection
library(dbplyr)      # SQL back-end for dplyr            (Tidyverse)
library(duckdb)      # Quack Stack
library(arrow)       # Fast an efficient data reading

library(sf)          # Spatial data manipulation

library(fuzzyjoin)   # Non-equi joins & coordinates-based joins

options(
  dplyr.strict_sql = FALSE,
  scipen = 999L, 
  digits = 4L,
  knitr.max_rows_print = 10
)

data.table::setDTthreads(parallel::detectCores())
```

:::{.callout-tip collapse="true"}

# Expand for Session Info

```{r}
#| echo: false
#| results: markup

si <- sessioninfo::session_info(pkgs = "attached")

si$platform$Quarto <- system("quarto --version", intern = TRUE)

si$platform$pandoc <- strsplit(si$platform$pandoc, "@")[[1]][1]

si
```

:::

```{r}
#| echo: false

## This section is for the html output (code-linking, ...)

library(knitr)
library(quarto)
library(downlit)
library(xml2)
library(withr)

#-------------------------#
#### Custom knit_hooks ####
#-------------------------#

TIMES <- list()
knitr::knit_hooks$set(time_it = local({
  start <- NULL
  function(before, options) {
    if (before) start <<- Sys.time()
    else TIMES[[options$label]] <<- difftime(Sys.time(), start)
  }
}))
```

```{r}
#| echo: false
#| output: false
#| file: !expr here("src", "common", "knitr", "knit_print_gt_mono.R")
```

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Loading the data
***

```{r}
#| echo: false

data_path <- here("res", "data", "stations")
files <- dir_ls(data_path, glob = "*.csv")
duckdb_path <- here(data_path, "stations.db")
```


::: {.callout-caution appearance="simple" collapse="true"}

#### Note on the choice of Database: 

A more logical choice would have been `PostGIS`, an extension of `Postgres` for spatial data, which includes native support for geometry/geography data types, as well as methods for distance calculations. However, setting up and interfacing `PostGIS` with R is significantly more convoluted than `DuckDB`, and analytical queries on `PostGIS` are much slower than on `DuckDB`. The only downside to using `DuckDB` for this example is that we will need to define our haversine (i.e. distance-on-a-sphere) function ourselves.

:::


:::{.panel-tabset}

#### Tidyverse

```{r}
#| label: loading_tidy
#| time_it: true
#| output: false

map(files, \(f) read_csv(f, show_col_types = FALSE)) |> list_rbind()
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_tidy
```


#### data.table

```{r}
#| label: loading_dt
#| time_it: true
#| output: false

rides <- lapply(files, \(f) fread(f, na.strings = "")) |> rbindlist()
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_dt
```


#### DuckDB + arrow

```{r}
con_duck <- dbConnect(duckdb(), dbdir = duckdb_path)

knitr::opts_chunk$set(connection = "con_duck")
```

```{r}
#| label: loading_sql
#| output: false
#| time_it: true

open_dataset(data_path, format = "csv", factory_options = list(exclude_invalid_files = TRUE)) |> 
  to_duckdb(con_duck) |> 
  mutate(across(everything(), \(x) if_else(length(x) == 0, NA, x))) |> 
  compute("rides")
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_sql
```

<!-- ALTERNATIVES --->

```{r}
#| eval: false
#| code-fold: true
#| code-summary: Importing using the `duckdb` package directly (slower)

duckdb_read_csv(con_duck, "rides", files)
```

```{r}
#| eval: false
#| code-fold: true
#| code-summary: Importing while including the file names in the data

## Courtesy of Tristan Mahr (@tjmahr)

withr::with_dir(
  dir_path, 
  dbSendQuery(con,"CREATE TABLE rides AS SELECT * FROM read_csv_auto('*.csv', FILENAME = TRUE)")
)
```

```{r}
#| eval: false
#| code-fold: true
#| code-summary: Saving tables in a custom schema (i.e. not `main`)

dbExecute(con_duck, "CREATE SCHEMA IF NOT EXISTS rds")

duckdb_read_csv(con_duck, "rds.rides", files)
```

```{r}
#| eval: false
#| code-fold: true
#| code-summary: Importing an existing `df` from the R env

dbWriteTable(con_duck, "rides", rides_df) # As a TABLE

copy_to(con_duck, rides_df, "rides", indexes = "ride_id")

duckdb_register(con_duck, "rides", rides_df) # As a VIEW
```


:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Cleaning implausible rides
***

We are going to filter out rides that are implausible (too short, too long, made too quickly, ...). Some of the thresholds used here are a bit arbitrary.

::: {.callout-tip appearance="simple" collapse="true"}

#### Defining our distance function (haversine)

```{r}
haversine <- function(lng1, lat1, lng2, lat2) {
  as.numeric(
    6371 * acos(
      cos(lat1 * pi / 180) * cos(lat2 * pi / 180) * cos((lng1 - lng2) * pi / 180) + 
      sin(lat1 * pi / 180) * sin(lat2 * pi / 180)
    ) * 1000
  )
}
```

```{sql connection = "con_duck"}
CREATE FUNCTION haversine(lng1, lat1, lng2, lat2) 
  AS 6371 * acos( 
    cos(radians(lat1)) * cos(radians(lat2)) * cos(radians(lng2) - radians(lng1)) +
    sin(radians(lat1)) * sin(radians(lat2))
  ) * 1000;
```

:::

:::{.panel-tabset}

#### data.table

```{r}
#| label: rides_clean_dt
#| time_it: true
#| output: false

rides_clean <- rides[, let(
    is_looped = start_station_id == end_station_id | (start_lat == end_lat & start_lng == end_lng), # Does a ride loop on itself
    ride_dur_min = abs(difftime(ended_at, started_at, units = "mins")) # Ride duration (min)
  )
  ][, ride_dist_m := fifelse(
    is_looped & !is.na(is_looped), 0, 
    haversine(start_lng, start_lat, end_lng, end_lat) # Ride distance in meters 
  )
  ][, ride_speed_km_h := fifelse(
      ride_dist_m == 0 | ride_dur_min == 0, NA_real_, 
      (ride_dist_m/1000) / (as.double(ride_dur_min)/60)
    )
  ][, .SD, .SDcols = !patterns("_at$")
  # Rides too fast or too long
  ][ride_dur_min %between% c(5, 1440)
  # Loops that are too fast (<= 10 min)
  ][!(!is.na(is_looped) & is_looped & ride_dur_min <= 10)
  # Rides too short (<= 100m) or too long (>= 30km)
  ][is.na(is_looped) | is_looped | is.na(ride_dist_m) | between(ride_dist_m, 100, 30000)
  # Rides with implausible speeds
  ][(rideable_type == "classic_bike" & ride_speed_km_h <= 40)
    | (rideable_type == "electric_bike" & ride_speed_km_h <= 60)
  ]
```

```{r}
#| echo: false

if (!interactive()) TIMES$rides_clean_dt
```

#### dbplyr

```{r}
#| label: rides_clean_dbp
#| time_it: true
#| output: false

(tbl(con_duck, "rides")
  |> mutate(is_looped = start_station_id == end_station_id | (start_lat == end_lat & start_lng == end_lng))
  |> mutate(across(ends_with("_at"), \(x) strptime(x, "%Y-%m-%d %H:%M:%S")))
  |> mutate(ride_dur_min = abs(date_sub("second", started_at, ended_at) / 60))
  |> mutate(ride_dist_m = case_when(
    is_looped ~ 0, 
    is.na(start_lng) | is.na(start_lat) | is.na(end_lng) | is.na(end_lat) ~ NA_real_,
    .default = haversine(start_lng, start_lat, end_lng, end_lat)) # This will call the SQL haversine function
  )
  |> mutate(ride_speed_km_h = case_when(
    ride_dist_m == 0 | ride_dur_min == 0 ~ NA_real_, 
    .default = (ride_dist_m / 1000) / (ride_dur_min/60))
  )
  |> select(-ends_with("_at"))
  # Rides too fast or too long
  |> filter(between(ride_dur_min, 5, 1440))
  # Loops that are too fast (<= 10 min)
  |> filter(!(!is.na(is_looped) & is_looped & ride_dur_min <= 10))
  # Rides too short (<= 100m) or too long (>= 30km)
  |> filter(is.na(is_looped) | is_looped | is.na(ride_dist_m) | between(ride_dist_m, 100, 30000))
  # Rides with implausible speeds
  |> filter(
    (rideable_type == "classic_bike" & ride_speed_km_h <= 40)
    | (rideable_type == "electric_bike" & ride_speed_km_h <= 60)
  )
  |> compute("rides_clean")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$rides_clean_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides_clean
```


<!-- CLEANING --->

```{r}
#| echo: false

## We'll use the cleaned data for the rest of the document

dbRemoveTable(con_duck, "rides")
rm(rides)
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Pivoting the data
***

We need to pivot each station (start and end) into its own row in order to facilitate subsequent operations based on their coordinates.

:::{.panel-tabset}

#### data.table

```{r}
#| label: long_dt
#| time_it: true

rides_clean_l <- melt(
  rides_clean,
  measure = measure(way, value.name, pattern = "(end|start).*_(name|id|lat|lng)")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dt
```


#### dtplyr

```{r}
#| label: long_dtp
#| time_it: true
#| output: false

pivot_longer(
  rides_clean, matches("^end_|^start_"),
  names_pattern = "(end|start).*_(name|id|lat|lng)", names_to = c("way", ".value")
) |> as.data.table()
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "long_sql", time_it = TRUE}
CREATE TABLE rides_clean_l AS 
(
  SELECT
    ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h
    , 'start' AS way
    , start_station_name AS "name"
    , start_station_id AS id
    , start_lat AS lat
    , start_lng AS lng
  FROM rides_clean
)
UNION ALL
(
  SELECT
    ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h
    , 'end' AS way
    , end_station_name AS "name"
    , end_station_id AS id
    , end_lat AS lat
    , end_lng AS lng
  FROM rides_clean
);
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_sql
```


#### dbplyr

```{r}
#| label: long_dbp
#| time_it: true
#| output: false

(pivot_longer(
    tbl(con_duck, "rides_clean"),
    matches("^end_|^start_"),
    names_pattern = "(end|start).*_(name|id|lat|lng)", 
    names_to = c("way", ".value")
  )
  |> compute("rides_clean_l_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| column: page
#| output: asis
#| total_rows: !expr tbl(con_duck, "rides_clean_l") |> count() |> pull(n)

tbl(con_duck, "rides_clean_l") |> head(10) |> collect()
```


<!-- CLEANING --->

```{r}
#| echo: false
#| output: false

## We'll use the long format data for the rest of the document

rm(rides_clean)
dbRemoveTable(con_duck, "rides_clean")
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Cleaning imprecise coordinates
***

Removing entries were `lat`/`lng` do not have sufficient precision to be reliably matched to a station (i.e. entries having less than 4 decimals, which corresponds to a 11 meters "radius" at the equator).

:::{.callout-tip collapse="true"}

#### Degrees to distance equivalence

| **Decimal** | **Distance at the equator (m)** |
|-------------|---------------------------------|
| 0           | 111,120                         |
| 1           | 11,112                          |
| 2           | 1,111.2                         |
| 3           | 111.12                          |
| 4           | 11.112                          |
| 5           | 1.1112                          |

:::

```{r}
decp <- function(x) str_length(str_remove(as.character(abs(x)), ".*\\.")) >= 4
```

```{sql connection = "con_duck"}
CREATE FUNCTION decp(x) AS length(str_split(CAST(abs(x) AS VARCHAR(10)), '.')[2]) >= 4
```

```{sql connection = "con_duck", eval = FALSE, echo = FALSE}
CREATE FUNCTION decp(x) AS length(regexp_replace(CAST(abs(x) AS VARCHAR(50)), '(.*)[.]', '')) >= 4 -- Slower
```


:::{.panel-tabset}

#### data.table

```{r}
#| label: acc_dt
#| time_it: true

rides_clean_l_acc <- rides_clean_l[decp(lat) & decp(lng), ]
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_dt
```


#### dtplyr

```{r}
#| label: acc_dtp
#| output: false
#| time_it: true

rides_clean_l |> filter(decp(lat) & decp(lng)) |> as.data.table()
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "acc_duck", time_it = TRUE}
CREATE TABLE rides_clean_l_acc AS
SELECT * FROM rides_clean_l 
WHERE decp(lat) AND decp(lng)
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_duck
```


#### dbplyr

:::{.callout-note}
Here, `dbplyr` will leave the `decp` call as-is in the SQL translation, but since we have previously defined a `decp` SQL function, this function will get called when the SQL query is executed.
:::

```{r}
#| label: acc_dbp
#| output: false
#| time_it: true

(tbl(con_duck, "rides_clean_l_dbp") 
  |> filter(if_all(c(lat, lng), \(x) decp(x)))
  |> compute("rides_clean_l_acc_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| column: page
#| output: asis

rides_clean_l_acc
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Exploring the data
***

<!-------------------------------------------------------->
## Known stations missing coordinates

**Stations missing either `lat` or `lng`, but having an `id` or `name`:**

:::{.panel-tabset}

#### data.table

```{r}
#| eval: false

rides_clean_l_acc[(is.na(lat) | is.na(lng)) & (!is.na(id) | !is.na(name)), ]
```


#### SQL (DuckDB)

```{sql connection = "con_duck", eval = FALSE}
SELECT * FROM rides_clean_l_acc 
WHERE ((lat IS NULL) OR (lng IS NULL) 
  AND (NOT((id IS NULL)) OR NOT((name IS NULL))))
```


#### dbplyr

```{r}
rides_clean_l_no_coords <- (tbl(con_duck, "rides_clean_l_acc_dbp")
  |> filter(if_any(c(lat, lng), \(v) is.na(v)) & (!is.na(id) | !is.na(name)))
  |> collect()
)
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_clean_l_no_coords

rm(rides_clean_l_no_coords)
```


<!-------------------------------------------------------->
## Unknown stations (filtered)

**Stations missing either `name` or `id`, but having coordinates with sufficient accuracy:**

:::{.panel-tabset}

#### data.table

```{r}
rides_clean_l_acc[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name))] -> rides_clean_l_acc_unk
```


#### SQL (DuckDB)

```{sql connection = "con_duck"}
CREATE TABLE rides_clean_l_acc_unk AS
SELECT * FROM rides_clean_l_acc
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "con_duck", echo = FALSE, output.var = "rides_clean_l_acc_unk_count"}
SELECT COUNT(*) AS N FROM rides_clean_l_acc_unk
```


#### dbplyr

```{r}
#| output: false

(tbl(con_duck, "rides_clean_l_acc_dbp") 
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> compute("rides_clean_l_acc_unk_dbp")
)
```


:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_clean_l_acc_unk_count$N

DBI::dbGetQuery(con_duck, "SELECT * FROM rides_clean_l_acc_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

It seems there are only `r rides_clean_l_acc_unk_count$N` entries missing identification that could be matched based on their coordinates at the level of precision we use (11m / 4 decimals).

:::{.callout-warning}
Although, if we look at the original dataset (before filtering the inaccurate coordinates):
:::

<!-------------------------------------------------------->
## Unknown stations (unfiltered)

**Stations missing either `name` or `id`, but having coordinates:**

:::{.panel-tabset}

#### data.table

```{r}
rides_clean_l[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name))] -> rides_clean_l_unk
```


#### SQL (DuckDB)

```{sql connection = "con_duck"}
CREATE TABLE rides_clean_l_unk AS
SELECT * FROM rides_clean_l
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "con_duck", echo = FALSE, output.var = "rides_clean_l_unk_count"}
SELECT COUNT(*) AS N FROM rides_clean_l_unk
```


#### dbplyr

```{r}
#| output: false

(tbl(con_duck, "rides_clean_l_dbp") 
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> compute("rides_clean_l_unk_dbp")
)
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_clean_l_unk_count$N

DBI::dbGetQuery(con_duck, "SELECT * FROM rides_clean_l_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

There were `r scales::label_comma()(rides_clean_l_unk_count$N)` missing stations' `id` or `name` that could have been filled in the original data, but it seems that all of them disappeared when we filtered the coordinates with less than 4 decimals of precision. It would seem that those entries were missing their `id`/`name` in the source data **because** their coordinates were too imprecise to be matched to any station in the first place.



::: {.callout-caution appearance="simple" icon=false}
## If one were to do the macthing anyway, here's how:
:::

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Stations data
***

<!-------------------------------------------------------->
## Creating `stations` data

First, we need to assemble a reference/look-up table, i.e. a dataset linking each unique station `id` (and `name`) with a set of coordinates (here, we use the average `lat` & `lng`.

:::{.panel-tabset}

#### data.table

```{r}
#| label: stations_clean_dt
#| time_it: true

stations_clean <- (rides_clean_l_acc
  |> na.omit(cols = c("id", "name"))
  |> dcast(id+name ~ ., fun.agg = list(min, max, mean), value.var = c("lat", "lng"))
  |> bind(x, setcolorder(x, c("id", "name", str_subset(colnames(x), "lat_|_lng"))))
  |> unique(by = "id")
  |> setorder(id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_dt
```

#### SQL (DuckDB)

```{sql connection = "con_duck", label = "stations_clean_sql", time_it = TRUE}
CREATE TABLE stations_clean AS 
SELECT
  id
  , FIRST("name") AS "name"
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_clean_l_acc
WHERE (NOT((id IS NULL))) AND (NOT(("name" IS NULL)))
GROUP BY id
ORDER BY id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_sql
```

#### dbplyr

```{r}
#| output: false
#| label: stations_clean_dbp
#| time_it: true

(tbl(con_duck, "rides_clean_l_acc_dbp")
  |> filter(!is.na(id), !is.na(name))
  |> group_by(id)
  |> summarize(
    name = first(name),
    across(c(lat, lng), list("min" = min, "max" = max, "mean" = mean), .names = "{.col}_{.fn}")
  )
  |> arrange(id)
  |> compute("stations_clean_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr DBI::dbGetQuery(con_duck, "SELECT COUNT(*) AS N FROM stations_clean")$N

dbGetQuery(con_duck, "SELECT * FROM stations_clean ORDER BY id", n = getOption("knitr.max_rows_print"))
```



<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Spatial join
***

Let's match the entries of `rides_clean_l_acc` with `stations_clean` by proximity:

<!-------------------------------------------------------->
## Matching on the cleaned data

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. `rides_clean_l_acc_unk`, the ones having coordinates but missing either `name` or `id`). There are `r scales::label_comma()(nrow(rides_clean_l_acc_unk))` entries from `rides_clean_l_acc` that could be position-matched to a known station.


:::{.panel-tabset}

#### data.table

```{r}
#| label: match_clean_manual
#| time_it: true
#| output: false

stations_clean[, c(k = 1, .SD)
             ][rides_clean_l_acc_unk[, c(k = 1, .SD)], on = "k", allow.cartesian = TRUE
             ][, dist := haversine(lng, lat, lng_mean, lat_mean)
             ][dist <= 11, .SD[which.min(dist)], by = c("ride_id", "way")
             ][, let(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
             ][, .SD, .SDcols = c(colnames(rides_clean_l_acc_unk), "dist")] -> matched_clean
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_manual
```


#### fuzzyjoin

```{r}
#| label: match_clean_fuzzy
#| time_it: true
#| output: false

(geo_inner_join(
    as.data.frame(rides_clean_l_acc_unk),
    as.data.frame(stations_clean),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> lazy_dt()
  |> group_by(ride_id, way)
  |> filter(which.min(dist))
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_clean_l_acc_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_fuzzy
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "match_clean_sql", time_it = TRUE}
CREATE TABLE matched_clean AS
SELECT ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h, way, name, id, lat, lng, dist
FROM (
  SELECT *, RANK() OVER (PARTITION BY ride_id, way ORDER BY dist) AS dist_rank
  FROM (
    SELECT
      ride_id, rideable_type, member_casual, way, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h  
      , COALESCE(r.name, s.name) AS name
      , COALESCE(r.id, s.id) AS id
      , r.lat AS lat
      , r.lng AS lng
      , haversine(s.lng_mean, s.lat_mean, r.lng, r.lat) AS dist
    FROM rides_clean_l_acc_unk r, stations_clean s
    WHERE dist <= 11
  )
)
WHERE dist_rank <= 1
ORDER BY ride_id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_sql
```


#### dbplyr

```{r}
#| label: match_clean_dbp
#| time_it: true
#| output: false

(inner_join(
    tbl(con_duck, "rides_clean_l_acc_unk_dbp"),
    tbl(con_duck, "stations_clean_dbp"),
    sql_on = "haversine(LHS.lng, LHS.lat, RHS.lng_mean, RHS.lat_mean) <= 11"
  )
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> group_by(ride_id, way)
  |> slice_min(dist)
  |> ungroup()
  |> select(colnames(tbl(con_duck, "rides_clean_l_acc_unk_dbp")), dist)
  |> arrange(ride_id)
  |> compute("matched_clean_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_dbp
```

#### sf

```{r}
#| label: match_clean_sf
#| time_it: true
#| output: false

(st_join(
    st_as_sf(rides_clean_l_acc_unk, coords = c("lat", "lng"), remove = FALSE, crs = 4326),
    st_as_sf(stations_clean, coords = c("lat_mean", "lng_mean"), remove = FALSE, crs = 4326),
    join = st_is_within_distance,
    dist = 11, # In meters
    left = FALSE # Does an inner_join
  )
  |> st_drop_geometry()
  |> lazy_dt()
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> group_by(ride_id, way)
  |> filter(which.min(dist))
  |> ungroup()
  |> select(colnames(rides_clean_l_acc_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_sf
```

:::

```{r}
#| echo: false
#| output: asis
#| column: page

matched_clean
```

And our `r nrow(matched_clean)` rows with missing ids get matched !


:::{.callout-note}
But those `r nrow(matched_clean)` already had an `id`, so we could probably have filled their missing `name` using `stations_clean` directly, instead of a convoluted proximity-based matching (which is more ressource intensive and less precise).
:::

```{r}
#| output: asis

stations_clean[matched_clean, on = .(id)
             ][, .(id, name.stations = name, name.proximity = i.name, lat, lng)]
```

At least, we can see that the proximity-based matched name and the one associated to that station in `stations_clean` are the same, so the proximity-matching method works reasonably well.


<!-------------------------------------------------------->
## Matching on the original data

What if we did the same procedure on the non-cleaned data (the one with coordinates less precise than our criteria for matching) ?

### Unfiltered `stations` data

First, we need to recompute the `stations` data from `rides_clean_l` (i.e. rides data before removing inaccurate coordinates):

:::{.panel-tabset}

#### data.table

```{r}
#| label: stations_dt
#| time_it: true

stations <- (rides_clean_l
  |> na.omit(cols = c("id", "name"))
  |> dcast(id+name ~ ., fun.agg = list(min, max, mean), value.var = c("lat", "lng"))
  |> bind(x, setcolorder(x, c("id", "name", str_subset(colnames(x), "lat_|_lng"))))
  |> unique(by = "id")
  |> setorder(id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_dt
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "stations_sql", time_it = TRUE}
CREATE TABLE stations AS 
SELECT
  id
  , FIRST("name") AS "name"
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_clean_l
WHERE (NOT((id IS NULL))) AND (NOT(("name" IS NULL)))
GROUP BY id
ORDER BY id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_sql
```


#### dbplyr

```{r}
#| output: false
#| label: stations_dbp
#| time_it: true

(tbl(con_duck, "rides_clean_l_dbp")
  |> filter(!is.na(id), !is.na(name))
  |> group_by(id)
  |> summarize(
    name = first(name),
    across(c(lat, lng), list("min" = min, "max" = max, "mean" = mean), .names = "{.col}_{.fn}")
  )
  |> arrange(id)
  |> compute("stations_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr dbGetQuery(con_duck, "SELECT COUNT(*) AS N FROM stations")$N

dbGetQuery(con_duck, "SELECT * FROM stations ORDER BY id", n = getOption("knitr.max_rows_print"))
```


**Cleaning the results:**

Notice we get a lot more entries in our `stations`: `r nrow(stations)` entries vs `r nrow(stations_clean)` entries in the filtered version.

Which entries are in `stations` but not in `stations_clean` ?

```{r}
#| output: asis

(stations_diff <- stations[!stations_clean, on = .(id, name)])
```

And which of those `r nrow(stations_diff)` entries have the necessary coordinate precision to be used later on to match against the unknown stations ?

```{r}
#| output: asis

stations_diff[stations_diff[, Reduce(`&`, lapply(.SD, decp)), .SDcols = patterns("^lat|^lng")]]
```

As expected, none. But we're still going to do the matching, for posterity !


### Position-matching on `stations`

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. the ones having coordinates but missing either `name` or `id`). There are `r scales::label_comma()(nrow(rides_clean_l_unk))` entries from `rides_clean_l` that could be position-matched to a known station.

:::{.panel-tabset}

#### data.table

We have to split the `rides_clean_l_unk` dataset in chunks of 10k rows, because its full cross-join with `stations` would be 1.3 billion rows (with 12 columns), which would definitely not fit in my RAM. The chunks are processed in parallel to speed it up (4 at a time, to avoid saturating my RAM). You might need to adjust those values on your machine.

```{r}
#| label: match_manual
#| time_it: true
#| output: false

future::plan(future::multisession, workers = 4)

matched <- future_map_dfr(
  copy(rides_clean_l_unk)[, let(ID = ceiling(.I/10000), k = 1)] |> split(by = "ID"),
  \(dat) 
    stations[, c(k = 1, .SD)][dat, on = "k", allow.cartesian = TRUE][
            , dist := haversine(lng, lat, lng_mean, lat_mean)
            ][dist <= 11, .SD[which.min(dist)], by = c("ride_id", "way")
            ][, let(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
            ][, .SD, .SDcols = c(colnames(rides_clean_l_unk), "dist")]
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_manual
```


#### fuzzyjoin

```{r}
#| label: match_fuzzy
#| time_it: true
#| output: false

(geo_inner_join(
    as.data.frame(rides_clean_l_unk),
    as.data.frame(stations),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> lazy_dt()
  |> group_by(ride_id, way)
  |> filter(which.min(dist))
  |> ungroup()
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_clean_l_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_fuzzy
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "match_sql", time_it = TRUE}
CREATE TABLE matched AS
SELECT ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h, way, name, id, lat, lng, dist
FROM (
  SELECT *, RANK() OVER (PARTITION BY ride_id, way ORDER BY dist) AS dist_rank
  FROM (
    SELECT
      ride_id, rideable_type, member_casual, way, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h  
      , COALESCE(r.name, s.name) AS name
      , COALESCE(r.id, s.id) AS id
      , r.lat AS lat
      , r.lng AS lng
      , haversine(s.lng_mean, s.lat_mean, r.lng, r.lat) AS dist
    FROM rides_clean_l_unk r, stations s
    WHERE dist <= 11
  )
)
WHERE dist_rank <= 1
ORDER BY ride_id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_sql
```

#### dbplyr

```{r}
#| label: match_dbp
#| time_it: true
#| output: false

(inner_join(
    tbl(con_duck, "rides_clean_l_unk_dbp"),
    tbl(con_duck, "stations_dbp"),
    sql_on = "haversine(LHS.lng, LHS.lat, RHS.lng_mean, RHS.lat_mean) <= 11"
  )
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> group_by(ride_id, way)
  |> slice_min(dist)
  |> ungroup()
  |> select(colnames(tbl(con_duck, "rides_clean_l_unk_dbp")), dist)
  |> arrange(ride_id)
  |> compute("matched_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_dbp
```

#### sf

```{r}
#| label: match_sf
#| time_it: true
#| output: false

(st_join(
    st_as_sf(rides_clean_l_unk, coords = c("lat", "lng"), remove = FALSE, crs = 4326),
    st_as_sf(stations, coords = c("lat_mean", "lng_mean"), remove = FALSE, crs = 4326),
    join = st_is_within_distance,
    dist = 11, # In meters
    left = FALSE # Does an inner_join
  )
  |> st_drop_geometry()
  |> lazy_dt()
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> group_by(ride_id, way)
  |> filter(which.min(dist))
  |> ungroup()
  |> select(colnames(rides_clean_l_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_sf
```

:::

<!-- Results --->

```{r}
#| echo: false
#| output: asis
#| column: page

matched
```


:::{.callout-note}
Notice how fast the procedure is, even if the results are mostly "garbage".
:::

**What's inside those matches ?**

```{r}
#| output: asis

matched[, .(`Number of matches for an entry` = .N), by = .(ride_id, way)
      ][, .(`Number of times it happens` = .N), by = `Number of matches for an entry`]
```

We can see that more than half of the matches are coordinates that matched 2 or more stations, which we should definitely not keep.


But, among the ones with only one match, how many have coordinates precise enough to make that match in the first place (i.e. have 4 or more decimals or precision) ?

```{r}
matched[, if(.N == 1) .SD, by = .(ride_id, way)][decp(lat) & decp(lng)] -> matched_valid
```

```{r}
#| echo: false
#| output: asis

matched_valid
```


As it turns out ? Only `r nrow(matched_valid)`. And those are the same `r nrow(matched_clean)` matches we got from the filtered data.  

In the end, those `r nrow(matched_clean)` are the only position-based matches we should reasonably keep !


<!-- Cleaning to free up memory --->

```{r}
#| echo: false
#| output: false

tables_to_remove <- c("stations", "stations_clean", "rides_clean_l_unk", "rides_clean_l_acc_unk")

walk(tables_to_remove, \(x) dbRemoveTable(con_duck, x, fail_if_missing = FALSE))

rm(rides_clean_l_unk)
rm(rides_clean_l_acc_unk)
rm(stations)
rm(stations_clean)
rm(stations_diff)
rm(matched)
rm(matched.sql)
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Updating the original dataset
***

Finally, we need to update the original dataset (`rides_clean_l`) with the entries that were position-matched (`matched_clean`):


<!-------------------------------------------------------->
## Merging the two datasets

:::{.callout-note}
We can update the initial (cleaned) data with the position-matched in two ways:  
- A `join` + `coalesce`: join the two tables and merge the two `name` & `id` columns (new and old) together.  
- A `rows_patch`: only replace the missing values (`name` & `id`) by matching new and old rows together.
:::

### join + coalesce

:::{.panel-tabset}

#### data.table

```{r}
#| label: merge_dt
#| time_it: true
#| output: false

matched_clean[rides_clean_l_acc, on = setdiff(colnames(rides_clean_l_acc), c("id", "name"))
            ][, let(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
            ][, nms, env = list(nms = I(colnames(rides_clean_l_acc)))]
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dt
```


#### dtplyr

```{r}
#| label: merge_dtp
#| output: false
#| time_it: true

rides_clean_l_merged <- (right_join(
    matched_clean,
    rides_clean_l_acc,
    by = setdiff(colnames(rides_clean_l_acc), c("id", "name"))
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y))
  |> select(-matches("\\.x|\\.y"))
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "merge_sql", time_it = TRUE}
CREATE TABLE rides_clean_l_merged AS
SELECT 
  ride_id, rideable_type, member_casual, way, ride_dur_min, ride_dist_m, ride_speed_km_h, is_looped  
  , COALESCE(id_x, id_y) AS id
  , COALESCE(name_x, name_y) AS name
  , lat, lng
FROM (
  SELECT
    r.ride_id AS ride_id
    , r.rideable_type AS rideable_type
    , r.member_casual AS member_casual
    , r.way AS way
    , r.ride_dur_min AS ride_dur_min
    , r.ride_dist_m AS ride_dist_m
    , r.ride_speed_km_h AS ride_speed_km_h
    , r.is_looped AS is_looped
    , m.name AS name_x
    , m.id AS id_x
    , r.lat AS lat
    , r.lng AS lng
    , r.name AS name_y
    , r.id AS id_y
  FROM matched_clean AS m
  RIGHT JOIN rides_clean_l AS r
  ON m.ride_id = r.ride_id 
     AND m.rideable_type = r.rideable_type 
     AND m.member_casual = r.member_casual
     AND m.way = r.way
);
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_sql
```


#### dbplyr

```{r}
#| label: merge_dbp
#| output: false
#| time_it: true

(right_join(
    tbl(con_duck, "matched_clean_dbp"),
    tbl(con_duck, "rides_clean_l_dbp"),
    by = tbl(con_duck, "rides_clean_l_dbp") |> select(-name, -id) |> colnames()
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y))
  |> select(-matches("\\.x|\\.y"))
  |> compute("rides_clean_l_merged_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dbp
```

:::

### rows_patch

```{r}
#| label: merge_dp_row
#| output: false
#| time_it: true

rows_patch(
  rides_clean_l,
  matched_clean[, !"dist"],
  by = setdiff(colnames(rides_clean_l), c("name", "id")),
  unmatched = "ignore"
)
```

```{r}
#| echo: false

rm(rides_clean_l)

if (!interactive()) TIMES$merge_dp_row
```


<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_clean_l_merged
```


<!-------------------------------------------------------->
## Validating the merge:

```{r}
#| output: asis

rides_clean_l_merged[(is.na(id) | is.na(name)) & (!is.na(lat) & !is.na(lng))]
```

We can see that the resulting dataset no longer has any entries that have coordinates but miss a `name` or an `id`, whereas there were `r nrow(matched_clean)` before. We have successfully updated them !


<!-------------------------------------------------------->
## Pivoting back to the original (wide) format

To finish, let's pivot the resulting data back into the wider format it was originally in:

:::{.panel-tabset}

#### data.table

```{r}
#| label: wide_dt
#| time_it: true

rides_merged <- dcast(
  rides_clean_l_merged, 
  ... ~ way, sep = "_station_", value.var = c("name", "id", "lat", "lng")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_dt
```


#### dtplyr

```{r}
#| label: wide_dtp
#| output: false
#| time_it: true

pivot_wider(
  rides_clean_l_merged, names_from = "way", names_glue = "{way}_station_{.value}",
  values_from = c("name", "id", "lat", "lng")
) |> as.data.table()
```

```{r}
#| echo: false

rm(rides_clean_l_merged)

if (!interactive()) TIMES$wide_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "wide_sql", time_it = TRUE}
CREATE TABLE rides_merged AS
SELECT
  ride_id, rideable_type, member_casual, ride_dur_min, ride_dist_m, ride_speed_km_h
  , MAX(CASE WHEN (way = 'start') THEN "name" END) AS start_station_name
  , MAX(CASE WHEN (way = 'end') THEN "name" END) AS end_station_name
  , MAX(CASE WHEN (way = 'start') THEN id END) AS start_station_id
  , MAX(CASE WHEN (way = 'end') THEN id END) AS end_station_id
  , MAX(CASE WHEN (way = 'start') THEN lat END) AS start_lat
  , MAX(CASE WHEN (way = 'end') THEN lat END) AS end_lat
  , MAX(CASE WHEN (way = 'start') THEN lng END) AS start_lng
  , MAX(CASE WHEN (way = 'end') THEN lng END) AS end_lng
FROM rides_clean_l_merged
GROUP BY ride_id, rideable_type, member_casual, ride_dur_min, ride_dist_m, ride_speed_km_h;
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_sql
```


#### dbplyr

```{r}
#| label: wide_dbp
#| output: false
#| time_it: true

pivot_wider(
  tbl(con_duck, "rides_clean_l_merged_dbp"), names_from = "way", 
  names_glue = "{way}_station_{.value}", values_from = c("name", "id", "lat", "lng")
) |> compute("rides_merged_dbp")
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides_merged
```


```{r}
#| echo: false

dbDisconnect(con_duck, shutdown = TRUE)
file_delete(duckdb_path)
```

***

![](http://vignette2.wikia.nocookie.net/creepypasta/images/1/11/Thats_all_folks.svg.png)