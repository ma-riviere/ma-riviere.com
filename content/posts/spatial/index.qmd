---
title: "Fast spatial data matching in R"
subtitle: "How to match locations based on their coordinates"

author:
  name: "Marc-Aurèle Rivière"
  orcid: 0000-0002-5108-3382

date: 2022-06-18

abstract: |
  This document was prompted by [a reddit question](https://www.reddit.com/r/rstats/comments/vdwi6o/finding_records_based_on_approximate_lattitude/) on how to fill missing location names by matching them to other known locations by their geographical proximity (using lat/long coordinates). The question is apparently linked to a case study of **Google's Data Analytics certificate**.  
  
  The gist of it is that the proposed matching won't yield anything with this particular dataset. It appears that the locations missing a name or id are missing one *because* their coordinates lack the precision necessary to be reliably matched to a known station by proximity alone.
  
  This post showcases different solutions/packages to achieve the distance-based matching, with timings to compare their speed. The SQL (DuckDB) & `dbplyr` solutions are on-disk (instead of in-memory), meaning they sacrifice some speed but could handle bigger-than-RAM datasets.

website:
  open-graph:
    description: "Exploring various solutions to quickly match locations by their geographical proximity in R"
  twitter-card:
    description: "Exploring various solutions to quickly match locations by their geographical proximity in R"

categories:
  - "Big Data"
  - "Data Manipulation"
  - "Spatial"
  - "R"
  - "SQL"

format:
  html:
    code-tools:
      source: true
      toggle: false
---

<hr style="margin-bottom: 30px; margin-top: -12px">

:::{.callout-tip}
You can check the source code by clicking on the **</> Code** button at the top-right.

Data can be found [here](https://divvy-tripdata.s3.amazonaws.com/index.html) (June 2021 to May 2022).
:::

```{r}
#| echo: false
#| eval: false
#| output: false

## TODO:
# - st_centroid() to compute lat/lng_mean
```


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
# Setup {.unnumbered}
***

```{r}
#| echo: false
#| output: false

source(here::here("src", "init_min.R"), echo = FALSE)

config <- config::get(file = here("_config.yml"))
```

```{r}
#| echo: false
#| eval: false

renv::install(c(
  "here",
  "fs",
  "pipebind",
  "Rdatatable/data.table", # >= 1.14.5
  "Tidyverse/dplyr",
  "tidyr",
  "dtplyr",
  "DBI",
  "Tidyverse/dbplyr",
  "duckdb",
  "stringr",
  "Tidyverse/purrr", # >= 1.0.0 or dev version (0.9000)
  "fuzzyjoin",
  "sf"
))
```

```{r}
#| output: false

library(here)        # File path management
library(fs)          # File & folder manipulation
library(pipebind)    # Piping goodies

library(dplyr)       # Manipulating data.frames - core (Tidyverse)
library(tidyr)       # Manipulating data.frames - extras (Tidyverse)
library(stringr)     # Manipulating strings (Tidyverse)
library(purrr)       # Manipulating lists (Tidyverse)

library(data.table)  # Fast data manipulation
library(dtplyr)      # data.table backend for dplyr (Tidyverse)

library(DBI)         # Database connection
library(dbplyr)      # SQL back-end for dplyr (Tidyverse)
library(duckdb)      # Quack Stack

library(sf)          # Spatial data manipulation

library(fuzzyjoin)   # Non-equi joins & coordinates-based joins

options(
  dplyr.strict_sql = FALSE,
  scipen = 999L, 
  digits = 4L,
  knitr.max_rows_print = 10
)

data.table::setDTthreads(parallel::detectCores(logical = TRUE))
```

:::{.callout-tip collapse="true"}

# Expand for Session Info

```{r}
#| echo: false
#| results: markup

si <- sessioninfo::session_info(pkgs = "attached")

si$platform$Quarto <- system("quarto --version", intern = TRUE)

si$platform$pandoc <- strsplit(si$platform$pandoc, "@")[[1]][1]

si
```

:::

```{r}
#| echo: false

## This section is for the html output (code-linking, ...)

library(knitr)
library(quarto)
library(downlit)
library(xml2)
library(withr)

#-------------------------#
#### Custom knit_hooks ####
#-------------------------#

TIMES <- list()
knitr::knit_hooks$set(time_it = local({
  start <- NULL
  function(before, options) {
    if (before) start <<- Sys.time()
    else TIMES[[options$label]] <<- difftime(Sys.time(), start)
  }
}))
```

```{r}
#| echo: false
#| output: false
#| file: !expr here("src", "common", "knitr", "knit_print_gt_mono.R")
```

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Loading the data
***

```{r}
#| echo: false

data_path <- here::here("res", "data", "stations")
duckdb_path <- here::here("res", "data", "stations", "stations.db")
```


<!-------------------------------------------------------->
## Wide format (original)

:::{.panel-tabset}

#### data.table

```{r}
#| label: loading_dtp
#| time_it: true

rides <- (
  map(dir_ls(data_path, glob = "*.csv"), \(f) fread(f, na.strings = ""))
  |> list_rbind()
  |> setkey(ride_id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_dtp
```


#### SQL (DuckDB)

```{r}
rides_con <- dbConnect(duckdb(), dbdir = duckdb_path)
```

```{r}
#| echo: false

knitr::opts_chunk$set(connection = "rides_con")
```

```{r}
#| label: loading_sql
#| output: false
#| time_it: true

duckdb_read_csv(rides_con, "rides", dir_ls(data_path, glob = "*.csv"))

db_create_index(rides_con, "rides", "ride_id")
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_sql
```

:::{.callout-note collapse="true"}

##### Alternatively

We can reuse an existing `df` and directly add it (or bind it as a view) to the database:

```{r}
#| eval: false

dbWriteTable(rides_con, "rides", rides) # As a TABLE

duckdb_register(rides_con, "rides", rides) # As a VIEW
```

:::

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset
#| total_rows: !expr DBI::dbGetQuery(rides_con, "SELECT COUNT(*) AS N FROM rides")$N

dbGetQuery(rides_con, "SELECT * FROM rides ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

<!-------------------------------------------------------->
## Long format

:::{.panel-tabset}

#### data.table

```{r}
#| label: long_dt
#| time_it: true

rides_l <- (melt(
    rides,
    measure = measure(way, value.name, pattern = "(end|start).*(name|id|lat|lng)")
  )
  |> setkey(ride_id, id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dt
```


#### dtplyr

```{r}
#| label: long_dtp
#| time_it: true
#| output: false

(pivot_longer(
    rides,
    matches("^end_|^start_"),
    names_pattern = "(end|start).*(name|id|lat|lng)",
    names_to = c("way", ".value")
  ) 
  |> as.data.table()
  |> setkey(ride_id, id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dtp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "long_sql", time_it = TRUE}
CREATE TABLE rides_l AS 
(
  SELECT
    ride_id, rideable_type, started_at, ended_at,member_casual
    , 'start' AS way
    , start_station_name AS "name"
    , start_station_id AS id
    , start_lat AS lat
    , start_lng AS lng
  FROM rides
)
UNION ALL
(
  SELECT
    ride_id, rideable_type, started_at, ended_at, member_casual
    , 'end' AS way
    , end_station_name AS "name"
    , end_station_id AS id
    , end_lat AS lat
    , end_lng AS lng
  FROM rides
);

CREATE INDEX station_idx ON rides_l (id);
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_sql
```


#### dbplyr

```{r}
#| label: long_dbp
#| time_it: true
#| output: false

(tbl(rides_con, "rides") 
  |> pivot_longer(
    matches("^end_|^start_"),
    names_pattern = "(end|start).*(name|id|lat|lng)", 
    names_to = c("way", ".value")
  )
  |> compute("rides_l.dbp")
)

db_create_index(rides_con, "rides_l.dbp", "id")
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dbp
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| column: page
#| output: asis
#| total_rows: !expr tbl(rides_con, "rides_l.dbp") |> summarize(N = n()) |> collect() |> pull(N)

tbl(rides_con, "rides_l") |> head(10) |> collect()
```

<!-- Cleaning --->

```{r}
#| echo: false

## We'll use rides_l for the rest of the document

dbRemoveTable(rides_con, "rides")
rm(rides)
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Exploring the data
***

## Cleaning useless coordinates

Removing entries were `lat`/`lng` do not have sufficient precision to be reliably matched to a station (i.e. entries having less than 4 decimals, which corresponds to a 11 meters "radius" at the equator).

:::{.callout-tip collapse="true"}

#### Degrees to distance equivalence

| **Decimal** | **Distance at the equator (m)** |
|-------------|---------------------------------|
| 0           | 111,120                         |
| 1           | 11,112                          |
| 2           | 1,111.2                         |
| 3           | 111.12                          |
| 4           | 11.112                          |
| 5           | 1.1112                          |

:::

```{r}
decp <- \(x) str_length(str_remove(as.character(abs(x)), ".*\\.")) >= 4
```

```{sql connection = "rides_con"}
CREATE FUNCTION decp(x) AS length(str_split(CAST(abs(x) AS VARCHAR(10)), '.')[2]) >= 4
```

```{sql connection = "rides_con", eval = FALSE, echo = FALSE}
CREATE FUNCTION decp(x) AS length(regexp_replace(CAST(abs(x) AS VARCHAR(50)), '(.*)[.]', '')) >= 4 -- Slower
```


:::{.panel-tabset}

#### data.table

```{r}
#| label: clean_dt
#| time_it: true

rides_l_clean <- rides_l[decp(lat) & decp(lng), ] |> setkey(ride_id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$clean_dt
```


#### dtplyr

```{r}
#| label: clean_dtp
#| output: false
#| time_it: true

rides_l |> filter(decp(lat) & decp(lng)) |> as.data.table() |> setkey(ride_id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$clean_dtp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "clean_duck", time_it = TRUE}
CREATE TABLE rides_l_clean AS 
SELECT * FROM rides_l 
WHERE decp(lat) AND decp(lng)
```

```{r}
#| echo: false

if (!interactive()) TIMES$clean_duck
```


#### dbplyr

:::{.callout-note}
Here, `dbplyr` will leave the `decp` call as-is in the SQL translation, but since we have previously defined a `decp` SQL function, this function will get called when the SQL query is executed.
:::

```{r}
#| label: clean_dbp
#| output: false
#| time_it: true

(tbl(rides_con, "rides_l.dbp") 
  |> filter(if_all(c(lat, lng), \(x) decp(x)))
  |> compute("rides_l_clean.dbp")
)

db_create_index(rides_con, "rides_l_clean.dbp", "ride_id")
```

```{r}
#| echo: false

if (!interactive()) TIMES$clean_dbp
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| column: page
#| output: asis

rides_l_clean
```


<!-------------------------------------------------------->
## What's missing ?

**Entries missing one or both coordinates but having an `id` or `name`:**

:::{.panel-tabset}

#### data.table

```{r}
#| eval: false

rides_l_clean[(is.na(lat) | is.na(lng)) & (!is.na(id) | !is.na(name)), ]
```


#### SQL (DuckDB)

```{sql connection = "rides_con", output.var = "rides_l_clean_lost.sql"}
SELECT * FROM rides_l_clean 
WHERE ((lat IS NULL) OR (lng IS NULL) 
  AND (NOT((id IS NULL)) OR NOT((name IS NULL))))
```


#### dbplyr

```{r}
#| eval: false

(tbl(rides_con, "rides_l_clean.dbp")
  |> filter(if_any(c(lat, lng), \(v) is.na(v)) & (!is.na(id) | !is.na(name)))
  |> collect()
)
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_l_clean_lost.sql

rm(rides_l_clean_lost.sql)
```



**Entries missing either `name` or `id`, but having coordinates:**

:::{.panel-tabset}

#### data.table

```{r}
rides_l_clean[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name))] -> rides_l_clean_unk
```


#### dtplyr

```{r}
#| eval: false

(rides_l_clean 
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> as.data.table()
)
```


#### SQL (DuckDB)

```{sql connection = "rides_con"}
CREATE TABLE rides_l_clean_unk AS
SELECT * FROM rides_l_clean
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "rides_con", echo = FALSE, output.var = "rides_l_unk_clean_count"}
SELECT COUNT(*) AS N FROM rides_l_clean_unk
```


#### dbplyr

```{r}
#| eval: false

(tbl(rides_con, "rides_l_clean.dbp") 
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> compute("rides_l_clean_unk.dbp")
)
```


:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_l_unk_clean_count$N

DBI::dbGetQuery(rides_con, "SELECT * FROM rides_l_clean_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

It seems there are only `r rides_l_unk_clean_count$N` entries missing identification that could be matched based on their coordinates at the level of precision we use (11m / 4 decimals).

:::{.callout-warning}
Although, if we look at the original dataset (before filtering the inaccurate coordinates):
:::


:::{.panel-tabset}

#### data.table

```{r}
rides_l[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name))] -> rides_l_unk
```

#### dtplyr

```{r}
#| eval: false

(rides_l
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> as.data.table()
)
```


#### SQL (DuckDB)

```{sql connection = "rides_con"}
CREATE TABLE rides_l_unk AS
SELECT * FROM rides_l
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "rides_con", echo = FALSE, output.var = "rides_l_unk_count"}
SELECT COUNT(*) AS N FROM rides_l_unk
```


#### dbplyr

```{r}
#| eval: false

(tbl(rides_con, "rides_l.dbp") 
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> compute("rides_l_unk.dbp")
)
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_l_unk_count$N

DBI::dbGetQuery(rides_con, "SELECT * FROM rides_l_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

There were `r scales::label_comma()(rides_l_unk_count$N)` missing stations' `id` or `name` that could have been filled in the original data, but it seems that all of them disappeared when we filtered the coordinates with less than 4 decimals of precision. It would seem that those entries were missing their `id`/`name` in the source data **because** their coordinates were too imprecise to be matched to any station in the first place.



::: {.callout-caution appearance="simple" icon=false}
#### If one were to do the macthing anyway, here's how:
:::

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Stations data
***

<!-------------------------------------------------------->
## Creating `stations` data

First, we need to assemble a dataset linking each unique station `id` (and `name`) with a set of coordinates (here, we use the average `lat` & `lng`)

:::{.panel-tabset}

#### data.table

```{r}
#| label: stations_clean_dt
#| time_it: true

stations_clean <- (rides_l_clean
  |> na.omit(cols = c("id", "name"))
  |> dcast(id+name ~ ., fun.agg = list(min, max, mean), value.var = c("lat", "lng"))
  |> bind(x, setcolorder(x, c("id", "name", str_subset(colnames(x), "lat_|_lng"))))
  |> unique(by = "id")
  |> setkey(id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_dt
```

#### SQL (DuckDB)

```{sql connection = "rides_con", label = "stations_clean_sql", time_it = TRUE}
CREATE TABLE stations_clean AS 
SELECT
  id
  , FIRST("name") AS "name"
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_l_clean
WHERE (NOT((id IS NULL))) AND (NOT(("name" IS NULL)))
GROUP BY id
ORDER BY id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_sql
```

#### dbplyr

```{r}
#| output: false
#| label: stations_clean_dbp
#| time_it: true

(tbl(rides_con, "rides_l_clean.dbp")
  |> filter(!is.na(id), !is.na(name))
  |> group_by(id)
  |> summarize(
    name = first(name),
    across(c(lat, lng), list(min, max, mean), .names = "{.col}_{.fn}")
  )
  |> arrange(id)
  |> compute("stations_clean.dbp")
)

db_create_index(rides_con, "stations_clean.dbp", "id")
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_dbp
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr DBI::dbGetQuery(rides_con, "SELECT COUNT(*) AS N FROM stations_clean")$N

dbGetQuery(rides_con, "SELECT * FROM stations_clean ORDER BY id", n = getOption("knitr.max_rows_print"))
```

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Matching missing `id` by position
***

Let's match the entries of `rides_l_clean` with `stations_clean` by proximity:

```{r}
#| echo: false
#| eval: false

haversine <- \(ref_lat, ref_lng, lat, lng) {
  111.045 * (acos(sin(ref_lat * pi / 180) * sin(lat * pi / 180) + cos(ref_lat * pi / 180) * cos(lat * pi / 180) * cos((ref_lng - lng) * pi / 180))) * 180 / pi
}
```

<!-------------------------------------------------------->
## Matching on the cleaned data

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. the ones having coordinates but missing either `name` or `id`).


:::{.panel-tabset}

#### fuzzyjoin

There are `r nrow(rides_l_clean_unk)` entries from `rides_l_clean` that could be position-matched to a known station.

```{r}
#| label: match_clean_fuzzy
#| time_it: true

matched_clean <- (fuzzyjoin::geo_join(
    as.data.frame(rides_l_clean_unk),
    as.data.frame(stations_clean),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    mode = "inner",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_l_clean), dist)
  |> arrange(ride_id)
  |> drop_na(ride_id, id, name)
  |> as.data.table()
  |> setkey(ride_id, id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_fuzzy
```


#### SQL (DuckDB)

There are `r rides_l_unk_clean_count$N` entries from `rides_l_clean` that could be position-matched to a known station.

**Creating the `haversine` distance function:**

<!-- The difference in output between SQL & R might be from the fact that rides_l_clean.sql has ~40 more rows --->

```{sql connection = "rides_con"}
CREATE FUNCTION haversine(lat1, lng1, lat2, lng2) 
    AS ( 6371 * acos( cos( radians(lat1) ) *
       cos( radians(lat2) ) * cos( radians(lng2) - radians(lng1) ) +
       sin( radians(lat1) ) * sin( radians(lat2) ) ) 
    );
```


**Doing the matching:**

```{sql connection = "rides_con", label = "match_clean_sql", time_it = TRUE}
CREATE TABLE matched_clean AS
SELECT
  ride_id, rideable_type, started_at, ended_at, member_casual, way  
  , COALESCE(r.name, s.name) AS name
  , COALESCE(r.id, s.id) AS id
  , r.lat, r.lng
  , haversine(s.lat_mean, s.lng_mean, r.lat, r.lng) AS dist
FROM rides_l_clean_unk r, stations_clean s
WHERE dist <= 0.011
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_sql
```


#### sf

```{r}
#| label: match_clean_sf
#| time_it: true

matched_clean_sf <- (st_join(
    st_as_sf(rides_l_clean_unk, coords = c("lat", "lng"), remove = FALSE, crs = 4326),
    st_as_sf(stations_clean, coords = c("lat_mean", "lng_mean"), remove = FALSE, crs = 4326),
    join = st_is_within_distance,
    dist = 11, # In meters
    left = FALSE # Does an inner_join
  )
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_l_clean_unk))
  |> arrange(ride_id)
  |> st_drop_geometry()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_sf
```

:::

```{r}
#| echo: false
#| output: asis
#| column: page

matched_clean
```

And we indeed get three matches !


:::{.callout-note}
But those three already had an `id`, so we could probably have filled their missing `name` using `stations_clean` directly, instead of a convoluted proximity-based matching (which is more ressource intensive and less precise).
:::

```{r}
#| output: asis

stations_clean[matched_clean, on = .(id)
             ][, .(id, name.stations = name, name.proximity = i.name, lat, lng)]
```

At least, we can see that the proximity-based matched name and the one associated to that station in `stations_clean` are the same, so the proximity-matching method works reasonably well.


<!-------------------------------------------------------->
## Matching on the original data

What if we did the same procedure on the non-cleaned data (the one with coordinates less precise than our criteria for matching) ?

### Unfiltered `stations` data

First, we need to recompute the `stations` data from `rides_l` (i.e. rides data before cleaning):

:::{.panel-tabset}

#### data.table

```{r}
#| label: stations_dt
#| time_it: true

stations <- (rides_l
  |> na.omit(cols = c("id", "name"))
  |> dcast(id+name ~ ., fun.agg = list(min, max, mean), value.var = c("lat", "lng"))
  |> bind(x, setcolorder(x, c("id", "name", str_subset(colnames(x), "lat_|_lng"))))
  |> unique(by = "id")
  |> setkey(id, name)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_dt
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "stations_sql", time_it = TRUE}
CREATE TABLE stations AS 
SELECT
  id
  , FIRST("name") AS "name"
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_l
WHERE (NOT((id IS NULL))) AND (NOT(("name" IS NULL)))
GROUP BY id
ORDER BY id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_sql
```


#### dbplyr

```{r}
#| output: false
#| label: stations_dbp
#| time_it: true

(tbl(rides_con, "rides_l.dbp")
  |> filter(!is.na(id), !is.na(name))
  |> group_by(id)
  |> summarize(
    name = first(name),
    across(c(lat, lng), list(min, max, mean), .names = "{.col}_{.fn}")
  )
  |> arrange(id)
  |> compute("stations.dbp")
)

db_create_index(rides_con, "stations.dbp", "id")
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_dbp
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr DBI::dbGetQuery(rides_con, "SELECT COUNT(*) AS N FROM stations")$N

DBI::dbGetQuery(rides_con, "SELECT * FROM stations ORDER BY id", n = getOption("knitr.max_rows_print"))
```


**Cleaning the results:**

Notice we get a lot more entries in our `stations`: `r nrow(stations)` entries vs `r nrow(stations_clean)` entries in the filtered version.

Which entries are in `stations` but not in `stations_clean` ?

```{r}
#| output: asis

(stations_diff <- stations[!stations_clean, on = .(id, name)])
```

And which of those `r nrow(stations_diff)` entries have the necessary coordinate precision to be used later on to match against the unknown stations ?

```{r}
#| output: asis

stations_diff[stations_diff[, Reduce(`&`, lapply(.SD, decp)), .SDcols = patterns("^lat|^lng")]]
```

As expected, none. But we're still going to do the matching, for posterity !


### Position-matching on `stations`

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. the ones having coordinates but missing either `name` or `id`):


:::{.panel-tabset}

#### fuzzyjoin

There are `r scales::label_comma()(nrow(rides_l_unk))` entries from `rides_l` that could be position-matched to a known station.

```{r}
#| label: match_fuzzy
#| time_it: true

matched <- (fuzzyjoin::geo_inner_join(
    as.data.frame(rides_l_unk),
    as.data.frame(stations),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_l), dist)
  |> arrange(ride_id)
  |> as.data.table()
  |> setkey(ride_id, id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_fuzzy
```


#### SQL (DuckDB)

There are `r scales::label_comma()(rides_l_unk_count$N)` entries from `rides_l` that could be position-matched to a known station.

```{sql connection = "rides_con", label = "match_sql", time_it = TRUE, output.var = "matched.sql"}
SELECT
  ride_id, rideable_type, started_at, ended_at, member_casual, way  
  , COALESCE(r.name, s.name) AS name
  , COALESCE(r.id, s.id) AS id
  , r.lat, r.lng
  , haversine(s.lat_mean, s.lng_mean, r.lat, r.lng) AS dist
FROM rides_l_unk r, stations s
WHERE dist <= 0.011
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_sql
```


#### sf

```{r}
#| label: match_sf
#| time_it: true

matched_sf <- (st_join(
    st_as_sf(rides_l_unk, coords = c("lat", "lng"), remove = FALSE, crs = 4326),
    st_as_sf(stations, coords = c("lat_mean", "lng_mean"), remove = FALSE, crs = 4326),
    join = st_is_within_distance,
    dist = 11, # In meters
    left = FALSE # Does an inner_join
  )
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_l_unk))
  |> arrange(ride_id)
  |> st_drop_geometry()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_sf
```

:::

<!-- Results --->

```{r}
#| echo: false
#| output: asis
#| column: page

matched
```


:::{.callout-note}
Notice how fast the procedure is, even if the results are mostly "garbage".
:::

**What's inside those matches ?**

```{r}
#| output: asis

matched[, .(`Number of matches for an entry` = .N), by = .(ride_id, way)
      ][, .(`Number of times it happens` = .N), by = `Number of matches for an entry`]
```

We can see that more than half of the matches are coordinates that matched 2 or more stations, which we should definitely not keep.


But, among the ones with only one match, how many have coordinates precise enough to make that match in the first place (i.e. have 4 or more decimals or precision) ?

```{r}
#| output: asis

matched[, if(.N == 1) .SD, by = .(ride_id, way)][decp(lat) & decp(lng)]
```

As it turns out ? Only 3. And those are the same three matches we got from the filtered data.  

In the end, those three are the only three position-based matches we should reasonably keep !


<!-- Cleaning to free up memory --->

```{r}
#| echo: false
#| output: false

tables_to_remove <- c("rides_l", "stations", "rides_l_unk", "rides_l_clean_unk")

walk(tables_to_remove, \(x) dbRemoveTable(rides_con, x, fail_if_missing = FALSE))

rm(rides_l)
rm(rides_l_unk)
rm(rides_l_clean_unk)
rm(stations)
rm(stations_diff)
rm(matched)
rm(matched.sql)
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Updating the original dataset
***

Finally, we need to update the original dataset (`rides_l_clean`) with the entries that were position-matched (`matched_clean`):


<!-------------------------------------------------------->
## Merging the two datasets

:::{.callout-note}
We can update the initial (cleaned) data with the position-matched in two ways:  
- A `join` + `coalesce`: join the two tables and merge the two `name` & `id` columns (new and old) together.  
- A `rows_patch`: only replace the missing values (`name` & `id`) by matching new and old rows together.
:::

### join + coalesce

:::{.panel-tabset}

#### data.table

```{r}
#| label: merge_dt
#| time_it: true

matched_clean[rides_l_clean, on = setdiff(colnames(rides_l_clean), c("id", "name"))
            ][, let(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
            ][, nms, env = list(nms = I(colnames(rides_l_clean)))] -> rides_l_merged

setkey(rides_l_merged, ride_id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dt
```


#### dtplyr

```{r}
#| label: merge_dtp
#| output: false
#| time_it: true

(right_join(
    matched_clean,
    rides_l_clean,
    by = setdiff(colnames(rides_l_clean), c("id", "name"))
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y))
  |> select(-matches("\\.x|\\.y"), -dist)
  |> as.data.table()
  |> setkey(ride_id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dtp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "merge_sql", time_it = TRUE}
CREATE TABLE rides_l_merged AS
SELECT 
  ride_id, rideable_type, started_at, ended_at, member_casual, way,
  COALESCE(id_x, id_y) AS id,
  COALESCE(name_x, name_y) AS name,
  lat, lng
FROM (
  SELECT
    r.ride_id AS ride_id,
    r.rideable_type AS rideable_type,
    r.started_at AS started_at,
    r.ended_at AS ended_at,
    r.member_casual AS member_casual,
    r.way AS way,
    m.name AS name_x,
    m.id AS id_x,
    r.lat AS lat,
    r.lng AS lng,
    r.name AS name_y,
    r.id AS id_y
  FROM matched_clean AS m
  RIGHT JOIN rides_l_clean AS r
  ON m.ride_id = r.ride_id 
     AND m.rideable_type = r.rideable_type 
     AND m.started_at = r.started_at
     AND m.ended_at = r.ended_at
     AND m.member_casual = r.member_casual
     AND m.way = r.way
);
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_sql
```


#### dbplyr

```{r}
#| label: merge_dbp
#| output: false
#| time_it: true

(right_join(
    tbl(rides_con, "matched_clean"),
    tbl(rides_con, "rides_l_clean.dbp"),
    by = tbl(rides_con, "rides_l_clean.dbp") |> select(-name, -id) |> colnames()
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y))
  |> select(-matches("\\.x|\\.y"), -dist)
  |> compute("rides_l_merged.dbp")
)

db_create_index(rides_con, "rides_l_merged.dbp", "ride_id")
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dbp
```

:::

### rows_patch

:::{.panel-tabset}

#### dplyr

```{r}
#| label: merge_dp_row
#| output: false
#| time_it: true

rows_patch(
  as.data.frame(rides_l_clean),
  as.data.frame(matched_clean) |> select(-dist),
  by = setdiff(colnames(rides_l_clean), c("name", "id")),
  unmatched = "ignore"
)
```

```{r}
#| echo: false

rm(matched_clean)
rm(rides_l_clean)

if (!interactive()) TIMES$merge_dp_row
```

#### dbplyr

```{r}
#| label: merge_dbp_row
#| eval: false
#| output: false
#| time_it: true

(tbl(rides_con, "rides_l_clean.dbp")
 |> rows_patch(
    tbl(rides_con, "matched_clean") |> select(-dist),
    by = tbl(rides_con, "rides_l_clean.dbp") |> select(-name, -id) |> colnames(),
    unmatched = "ignore"
  )
  |> compute("rides_l_merged.rows_dbp")
)
```

```{r}
#| echo: false
#| eval: false

if (!interactive()) TIMES$merge_dbp_row
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_l_merged
```


<!-------------------------------------------------------->
## Validating the merge:

```{r}
#| output: asis

rides_l_merged[(is.na(id) | is.na(name)) & (!is.na(lat) & !is.na(lng))]
```

We can see that the resulting dataset no longer has any entries that have coordinates but miss a `name` or an `id`, whereas there were three before. We have successfully updated them !


<!-------------------------------------------------------->
## Pivoting back to the original (wide) format

To finish, let's pivot the resulting data back into the wider format it was originally in:

:::{.panel-tabset}

#### data.table

```{r}
#| label: wide_dt
#| time_it: true

rides_merged <- (rides_l_merged
  |> dcast(... ~ way, sep = "_station_", value.var = c("name", "id", "lat", "lng"))
  |> setkey(ride_id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_dt
```


#### dtplyr

```{r}
#| label: wide_dtp
#| output: false
#| time_it: true

(rides_l_merged
 |> pivot_wider(
    names_from = "way", names_glue = "{way}_station_{.value}",
    values_from = c("name", "id", "lat", "lng")
  )
  |> as.data.table()
  |> setkey(ride_id)
)
```

```{r}
#| echo: false

rm(rides_l_merged)

if (!interactive()) TIMES$wide_dtp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "wide_sql", time_it = TRUE}
CREATE TABLE rides_merged AS
SELECT
  ride_id, rideable_type, started_at, ended_at, member_casual,
  MAX(CASE WHEN (way = 'start') THEN "name" END) AS start_station_name,
  MAX(CASE WHEN (way = 'end') THEN "name" END) AS end_station_name,
  MAX(CASE WHEN (way = 'start') THEN id END) AS start_station_id,
  MAX(CASE WHEN (way = 'end') THEN id END) AS end_station_id,
  MAX(CASE WHEN (way = 'start') THEN lat END) AS start_lat,
  MAX(CASE WHEN (way = 'end') THEN lat END) AS end_lat,
  MAX(CASE WHEN (way = 'start') THEN lng END) AS start_lng,
  MAX(CASE WHEN (way = 'end') THEN lng END) AS end_lng
FROM rides_l_merged
GROUP BY ride_id, rideable_type, started_at, ended_at, member_casual
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_sql
```


#### dbplyr

```{r}
#| label: wide_dbp
#| output: false
#| time_it: true

(tbl(rides_con, "rides_l_merged.dbp")
 |> pivot_wider(
    names_from = "way", names_glue = "{way}_station_{.value}", 
    values_from = c("name", "id", "lat", "lng")
  )
  |> compute("rides_merged.dbp")
)

db_create_index(rides_con, "rides_merged.dbp", "ride_id")
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_dbp
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides_merged
```


:::{.callout-note}
We get less entries than in the original (wide format) dataset due to having removed (filtered) the entries with bad coordinates.
:::

```{r}
#| echo: false

DBI::dbDisconnect(rides_con, shutdown = TRUE)
fs::file_delete(duckdb_path)
```

***

![](http://vignette2.wikia.nocookie.net/creepypasta/images/1/11/Thats_all_folks.svg.png)