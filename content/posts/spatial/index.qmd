---
title: "Fast spatial data matching in R"
subtitle: "How to match locations based on their coordinates"

author:
  name: "Marc-Aurèle Rivière"
  orcid: 0000-0002-5108-3382

date: 2022-06-18

abstract: |
  This document was made in response to the following [reddit question](https://www.reddit.com/r/rstats/comments/vdwi6o/finding_records_based_on_approximate_lattitude/) on how to fill missing location names by matching them to other known locations by their geographical proximity (using lat/long coordinates). The question is linked to a case study of Google's Data Analytics certificate.

categories:
  - "Big Data"
  - "Data Manipulation"
  - "Spatial Data"
  - "R"
  - "SQL"

format:
  html:
    code-tools:
      source: true
      toggle: false
---

:::{.callout-tip}
You can check the source code by clicking on the **</> Code** button at the top-right.
:::

```{r}
#| echo: false
#| eval: false
#| output: false

## TODO: sf::st_join
```


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
# Setup
***

```{r}
#| echo: false
#| file: !expr here::here("src", "quarto", "renv_setup.R")
```

```{r}
#| output: false

library(here)        # File path management
library(fs)          # File & folder manipulation
library(pipebind)    # Piping goodies

library(data.table)  # Fast data manipulation
library(dplyr)       # Slower (but more readable) data manipulation
library(dtplyr)      # data.table backend for dplyr
library(tidyr)       # Extra convenience functions for data manipulation

library(DBI)         # Database connection
library(dbplyr)      # SQL back-end for dplyr
library(duckdb)      # Quack Stack

library(stringr)     # Manipulating strings
library(purrr)       # Manipulating lists

library(fuzzyjoin)   # Non-equi joins & coordinates-based joins

options(
  dplyr.strict_sql = FALSE,
  scipen = 999L, 
  digits = 4L,
  knitr.max_rows_print = 10
)

data.table::setDTthreads(parallel::detectCores(logical = TRUE))
```

:::{.callout-tip collapse="true"}

# Expand for Session Info

```{r}
#| echo: false
#| results: markup

si <- sessioninfo::session_info(pkgs = "attached")

si$platform$Quarto <- system("quarto --version", intern = TRUE)

si$platform$pandoc <- strsplit(si$platform$pandoc, "@")[[1]][1]

si
```

:::

```{r}
#| echo: false

## This section is for the html output (code-linking, ...)

library(knitr)
library(quarto)
library(downlit)
library(xml2)
library(withr)

#-------------------------#
#### Custom knit_hooks ####
#-------------------------#

TIMES <- list()
knitr::knit_hooks$set(time_it = local({
  start <- NULL
  function(before, options) {
    if (before) start <<- Sys.time()
    else TIMES[[options$label]] <<- difftime(Sys.time(), start)
  }
}))
```

```{r}
#| echo: false
#| file: !expr c(here("src", "quarto", "quarto_theme.R"), here("src", "quarto", "style_gt_mono.R"))
```

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Loading the data
***

```{r}
#| echo: false

data_path <- here::here("res", "data", "stations")
```


<!-------------------------------------------------------->
## Wide format (original)

:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| label: loading_dtp
#| time_it: true

rides <- (purrr::map_dfr(
    fs::dir_ls(data_path, glob = "*.csv"),
    \(file) fread(file, na.strings = "")
  )
)

setkey(rides, ride_id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_dtp
```


#### SQL (DuckDB)

```{r}
rides_con <- DBI::dbConnect(duckdb::duckdb())
```

```{r}
#| echo: false

knitr::opts_chunk$set(connection = "rides_con")
```

```{r}
#| label: loading_sql1
#| time_it: true

duckdb::duckdb_read_csv(
  rides_con, 
  "rides", 
  fs::dir_ls(here::here("res", "data", "stations"), glob = "*.csv")
)
```

```{sql connection = "rides_con", label = "loading_sql2", time_it = TRUE}
CREATE INDEX ride_idx ON rides (ride_id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_sql1 + TIMES$loading_sql2
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset
#| total_rows: !expr DBI::dbGetQuery(rides_con, "SELECT COUNT(*) AS N FROM rides")$N

DBI::dbGetQuery(rides_con, "SELECT * FROM rides ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```


<!-------------------------------------------------------->
## Long format

:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| label: long_dt
#| time_it: true

rides_l <- melt(
  rides,
  measure = measure(way, value.name, pattern = "(end|start).*(name|id|lat|lng)")
)

setkey(rides_l, ride_id, id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dt
```


#### dtplyr

```{r}
#| label: long_dtp
#| time_it: true

rides_l.dtp <- (pivot_longer(
    rides,
    matches("^end_|^start_"),
    names_pattern = "(end|start).*(name|id|lat|lng)",
    names_to = c("way", ".value")
  ) 
  |> as.data.table()
)

setkey(rides_l.dtp, ride_id, id)
```

```{r}
#| echo: false

rm(rides_l.dtp)

if (!interactive()) TIMES$long_dtp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "long_sql", time_it = TRUE}
CREATE TABLE rides_l AS 
(
  SELECT
    ride_id, rideable_type, started_at, ended_at,member_casual
    , 'start' AS way
    , start_station_name AS name
    , start_station_id AS id
    , start_lat AS lat
    , start_lng AS lng
  FROM rides
)
UNION ALL
(
  SELECT
    ride_id, rideable_type, started_at, ended_at, member_casual
    , 'end' AS way
    , end_station_name AS name
    , end_station_id AS id
    , end_lat AS lat
    , end_lng AS lng
  FROM rides
);

CREATE INDEX station_idx ON rides_l (id);
```

:::{.callout-note .column-margin appearance="simple"}
No need to re-define an index for `ride_id` here, it was passed down from the table `rides`
:::

```{r}
#| echo: false

if (!interactive()) TIMES$long_sql
```


:::{.callout-note collapse="true"}

##### Alternatively

We can reuse an existinf `df` and directly add it (or bind it as a view) to the database:

```{r}
#| eval: false

DBI::dbWriteTable(rides_con, "rides_l", rides_l) # As a TABLE

duckdb::duckdb_register(rides_con, "rides_l", rides_l) # As a VIEW
```

:::


#### dbplyr

```{r}
#| label: long_dbp
#| time_it: true
#| output: false

(dplyr::tbl(rides_con, "rides") 
  |> pivot_longer(
    matches("^end_|^start_"),
    names_pattern = "(end|start).*(name|id|lat|lng)", 
    names_to = c("way", ".value")
  )
  |> dplyr::copy_to(
    rides_con, 
    df = _, 
    "rides_l_dbp",
    temporary = FALSE,
    indexes = list("ride_id", "id")
  )
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dbp
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| column: page
#| output: asis
#| total_rows: !expr dplyr::tbl(rides_con, "rides_l_dbp") |> summarize(N = n()) |> collect() |> pull(N)

dplyr::tbl(rides_con, "rides_l") |> head(10) |> collect()
```

<!-- Cleaning --->

```{sql echo = FALSE}
-- We'll use rides_l for the rest of the document

DROP TABLE IF EXISTS rides;
DROP TABLE IF EXISTS rides_l_dbp;
```

```{r}
#| echo: false

rm(rides)
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Exploring the data
***

## Cleaning useless coordinates

Removing entries were `lat`/`lng` do not have sufficient precision to be reliably matched to a station (i.e. entries having less than 4 decimals, which corresponds to a 11 meters "radius" at the equator).

:::{.callout-tip collapse="true"}

#### Degrees to distance equivalence

| **Decimal** | **Distance at the equator (m)** |
|-------------|---------------------------------|
| 0           | 111,120                         |
| 1           | 11,112                          |
| 2           | 1,111.2                         |
| 3           | 111.12                          |
| 4           | 11.112                          |
| 5           | 1.1112                          |

:::

```{r}
decp <- \(x) str_length(str_remove(as.character(abs(x)), ".*\\.")) >= 4
```

```{sql connection = "rides_con"}
CREATE FUNCTION decp(x) AS length(str_split(CAST(abs(x) AS VARCHAR(10)), '.')[2]) >= 4
```

```{sql connection = "rides_con", eval = FALSE, echo = FALSE}
CREATE FUNCTION dec5(x) AS length(regexp_replace(CAST(abs(x) AS VARCHAR(50)), '(.*)[.]', '')) >= 4
```


:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| label: clean_dt
#| time_it: true

rides_l_clean <- rides_l[decp(lat) & decp(lng), ]

setkey(rides_l_clean, ride_id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$clean_dt
```


#### dtplyr

```{r}
#| label: clean_dtp
#| time_it: true

rides_l_clean.dtp <- (rides_l
  |> filter(decp(lat) & decp(lng))
  |> as.data.table()
)

setkey(rides_l_clean.dtp, ride_id)
```

```{r}
#| echo: false

rm(rides_l_clean.dtp)

if (!interactive()) TIMES$clean_dtp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "clean_duck", time_it = TRUE}
CREATE TABLE rides_l_clean AS 
SELECT * FROM rides_l 
WHERE decp(lat) AND decp(lng)
```

```{r}
#| echo: false

if (!interactive()) TIMES$clean_duck
```


#### dbplyr

:::{.callout-note}
Here, `dbplyr` will leave the `decp` call as-is in the SQL translation, but since we have previously defined a `decp` SQL function, this function will get called when the SQL query is executed.
:::

```{r}
#| label: clean_dbp
#| time_it: true
#| output: false

(dplyr::tbl(rides_con, "rides_l") 
  |> filter(if_all(c(lat, lng), \(x) decp(x)))
  |> dplyr::copy_to(
    rides_con, 
    df = _, 
    "rides_l_clean_dbp",
    temporary = FALSE,
    indexes = list("ride_id", "id")
  )
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$clean_dbp
```

```{sql connection = "rides_con", echo = FALSE}
DROP TABLE IF EXISTS rides_l_clean_dbp
```


:::

<!-- RESULT --->

```{r}
#| echo: false
#| column: page
#| output: asis

rides_l_clean
```


<!-------------------------------------------------------->
## What's missing ?

**Entries missing one or both coordinates but having an `id` or `name`:**

:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| eval: false

rides_l_clean[(is.na(lat) | is.na(lng)) & (!is.na(id) | !is.na(name)), ]
```


#### SQL (DuckDB)

```{sql output.var = "rides_l_clean_lost.sql"}
SELECT * FROM rides_l_clean 
WHERE ((lat IS NULL) OR (lng IS NULL) 
  AND (NOT((id IS NULL)) OR NOT((name IS NULL))))
```


#### dbplyr

```{r}
#| eval: false

(dplyr::tbl(rides_con, "rides_l_clean")
  |> filter(
    if_any(matches("lat$|lng$"), \(v) is.na(v)) & 
    (!is.na(id) | !is.na(name))
  )
  |> collect()
)
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_l_clean_lost.sql

rm(rides_l_clean_lost.sql)
```



**Entries missing either `name` or `id`, but having coordinates:**

:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| eval: false

rides_l_clean[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name)), ]
```


#### dtplyr

```{r}
rides_l_clean_unk <- (rides_l_clean 
  |> filter(
    if_all(matches("lat$|lng$"), \(v) !is.na(v)) & 
    (is.na(id) | is.na(name))
  )
  |> as.data.table()
)
```


#### SQL (DuckDB)

```{sql connection = "rides_con"}
CREATE TABLE rides_l_clean_unk AS
SELECT * FROM rides_l_clean
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "rides_con", echo = FALSE, output.var = "rides_l_unk_clean_count"}
SELECT COUNT(*) AS N FROM rides_l_clean_unk
```


#### dbplyr

```{r}
#| eval: false

(dplyr::tbl(rides_con, "rides_l_clean") 
  |> filter(
    if_all(matches("lat$|lng$"), \(v) !is.na(v)) & 
    (is.na(id) | is.na(name))
  )
  |> collect()
)
```


:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_l_unk_clean_count$N

DBI::dbGetQuery(rides_con, "SELECT * FROM rides_l_clean_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

It seems there are only `r rides_l_unk_clean_count$N` entries missing identification that could be matched based on their coordinates at the level of precision we use (11m / 4 decimals).

:::{.callout-warning}
Although, if we look at the original dataset (before filtering the inaccurate coordinates):
:::


:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| eval: false

rides_l[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name)), ]
```

#### dtplyr

```{r}
rides_l_unk <- (rides_l
  |> filter(
    if_all(matches("lat$|lng$"), \(v) !is.na(v)) & 
    (is.na(id) | is.na(name))
  )
  |> as.data.table()
)
```


#### SQL (DuckDB)

```{sql connection = "rides_con"}
CREATE TABLE rides_l_unk AS
SELECT * FROM rides_l
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "rides_con", echo = FALSE, output.var = "rides_l_unk_count"}
SELECT COUNT(*) AS N FROM rides_l_unk
```


#### dbplyr

```{r}
#| eval: false

(dplyr::tbl(rides_con, "rides_l") 
  |> filter(
    if_all(matches("lat$|lng$"), \(v) !is.na(v)) & 
    (is.na(id) | is.na(name))
  )
  |> collect()
)
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_l_unk_count$N

DBI::dbGetQuery(rides_con, "SELECT * FROM rides_l_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

There were `r scales::label_comma()(rides_l_unk_count$N)` missing stations' `id` or `name` that could have been filled in the original data, but it seems that all of them disappeared when we filtered the coordinates with less than 4 decimals of precision. It would seem that those entries were missing their `id`/`name` in the source data **because** their coordinates were too imprecise to be matched to any station in the first place.



::: {.callout-caution appearance="simple" icon=false}
#### If one were to do the macthing anyway, here's how:
:::

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Stations data
***

## Creating `stations` data

First, we need to assemble a dataset linking each unique station `id` (and `name`) with a set of coordinates (here, we use the average `lat` & `lng`)

:::{.panel-tabset group="framework"}

#### data.table

```{r}
stations_clean <- ((rides_l_clean
  |> na.omit(cols = c("id", "name"))
  |> dcast(id + name ~ ., fun.aggregate = list(min, max, mean), value.var = c("lat", "lng"))
  |> pipebind::bind(x, setcolorder(x, c("id", "name", str_subset(names(x), "lat_|_lng"))))
  |> unique(by = "id")
  )
)

setkey(stations_clean, id)
```


#### SQL (DuckDB)

```{sql connection = "rides_con"}
CREATE TABLE stations_clean AS 
SELECT DISTINCT on(id)
  id, name
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_l_clean
WHERE (NOT((id IS NULL))) AND (NOT((name IS NULL)))
GROUP BY id, name;
```


#### dbplyr

```{r}
#| output: false

(dplyr::tbl(rides_con, "rides_l_clean")
  |> filter(!is.na(id), !is.na(name))
  |> group_by(id, name)
  |> summarize(across(c(lat, lng), list(min, max, mean), .names = "{.col}_{.fn}"))
  |> ungroup()
  |> distinct(id, .keep_all = TRUE)
  |> arrange(id)
  |> collect()
)
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr DBI::dbGetQuery(rides_con, "SELECT COUNT(*) AS N FROM stations_clean")$N

DBI::dbGetQuery(rides_con, "SELECT * FROM stations_clean ORDER BY id", n = getOption("knitr.max_rows_print"))
```

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Matching missing `id` by position
***

Let's match the entries of `rides_l_clean` with `stations_clean` by proximity:

```{r}
#| echo: false
#| eval: false

haversine <- \(ref_lat, ref_lng, lat, lng) {
  111.045 * (acos(sin(ref_lat * pi / 180) * sin(lat * pi / 180) + cos(ref_lat * pi / 180) * cos(lat * pi / 180) * cos((ref_lng - lng) * pi / 180))) * 180 / pi
}
```

<!-------------------------------------------------------->
## Matching on the cleaned data

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. the ones having coordinates but missing either `name` or `id`).


:::{.panel-tabset group="framework"}

#### fuzzyjoin

There are `r nrow(rides_l_clean_unk)` entries from `rides_l_clean` that could be position-matched to a known station.

```{r}
#| label: match_clean_fuzzy
#| time_it: true

matched_clean <- (fuzzyjoin::geo_inner_join(
    as.data.frame(rides_l_clean_unk),
    as.data.frame(stations_clean),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(names(rides_l_clean), dist)
  |> arrange(ride_id)
  |> drop_na(ride_id, id, name)
  |> setDT()
)

setkey(matched_clean, ride_id, id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_fuzzy
```


#### SQL (DuckDB)

There are `r rides_l_unk_clean_count$N` entries from `rides_l_clean` that could be position-matched to a known station.

**Creating the `haversine` distance function:**

<!-- The difference in output between SQL & R might be from the fact that rides_l_clean.sql has ~40 more rows --->

```{sql connection = "rides_con", echo = FALSE, eval = FALSE}
-- Based on [this SO answer](https://stackoverflow.com/a/72730460/14637239)

CREATE FUNCTION haversine(lat1, lng1, lat2, lng2) 
    AS 2 * 6335 * sqrt(
       pow(sin((radians(lat2) - radians(lat1)) / 2), 2)
       + cos(radians(lat1)) * cos(radians(lat2))
       * pow(sin((radians(lng2) - radians(lng1)) / 2), 2)
    );
```

```{sql connection = "rides_con"}
CREATE FUNCTION haversine(lat1, lng1, lat2, lng2) 
    AS ( 6371 * acos( cos( radians(lat1) ) *
       cos( radians(lat2) ) * cos( radians(lng2) - radians(lng1) ) +
       sin( radians(lat1) ) * sin( radians(lat2) ) ) 
    );
```


**Doing the matching:**

```{sql connection = "rides_con", label = "match_clean_sql", time_it = TRUE}
CREATE TABLE matched_clean AS
SELECT
  ride_id, rideable_type, started_at, ended_at, member_casual, way  
  , COALESCE(r.name, s.name) AS name
  , COALESCE(r.id, s.id) AS id
  , r.lat, r.lng
  , haversine(s.lat_mean, s.lng_mean, r.lat, r.lng) AS dist
FROM rides_l_clean_unk r, stations_clean s
WHERE dist <= 0.011
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_sql
```

:::

```{r}
#| echo: false
#| output: asis
#| column: page

matched_clean
```

And we indeed get three matches !


:::{.callout-note}
But those three already had an `id`, so we could probably have filled their missing `name` using `stations_clean` directly, instead of a convoluted proximity-based matching (which is more ressource intensive and less precise).
:::

```{r}
#| output: asis

stations_clean[matched_clean, on = .(id)
             ][, .(id, name.stations = name, name.proximity = i.name, lat, lng)]
```

At least, we can see that the proximity-based matched name and the one associated to that station in `stations_clean` are the same, so the proximity-matching method works reasonably well.


<!-------------------------------------------------------->
## Matching on the original data

What if we did the same procedure on the non-cleaned data (the one with coordinates less precise than our criteria for matching) ?

### Unfiltered `stations` data

First, we need to recompute the `stations` data from `rides_l` (i.e. rides data before cleaning):

:::{.panel-tabset group="framework"}

#### data.table

```{r}
stations <- ((rides_l
  |> na.omit(cols = c("id", "name"))
  |> dcast(id + name ~ ., fun.aggregate = list(min, max, mean), value.var = c("lat", "lng"))
  |> pipebind::bind(x, setcolorder(x, c("id", "name", str_subset(names(x), "lat_|_lng"))))
  |> unique(by = "id")
  )
)

setkey(stations, id, name)
```


#### SQL (DuckDB)

```{sql connection = "rides_con"}
CREATE TABLE stations AS 
SELECT DISTINCT on(id)
  id, name
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_l
WHERE (NOT((id IS NULL))) AND (NOT((name IS NULL)))
GROUP BY id, name;
```


:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr DBI::dbGetQuery(rides_con, "SELECT COUNT(*) AS N FROM stations")$N

DBI::dbGetQuery(rides_con, "SELECT * FROM stations ORDER BY id", n = getOption("knitr.max_rows_print"))
```


**Cleaning the results:**

Notice we get a lot more entries in our `stations`: `r nrow(stations)` entries vs `r nrow(stations_clean)` entries in the filtered version.

Which entries are in `stations` but not in `stations_clean` ?

```{r}
#| output: asis

(stations_diff <- stations[!stations_clean, on = .(id, name)])
```

And which of those `r nrow(stations_diff)` entries have the necessary coordinate precision to be used later on to match against the unknown stations ?

```{r}
#| output: asis

stations_diff[stations_diff[, Reduce(`&`, lapply(.SD, decp)), .SDcols = patterns("^lat|^lng")]]
```

As expected, none. But we're still going to do the matching, for posterity !


### Position-matching on `stations`

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. the ones having coordinates but missing either `name` or `id`):


:::{.panel-tabset group="framework"}

#### fuzzyjoin

There are `r scales::label_comma()(nrow(rides_l_unk))` entries from `rides_l` that could be position-matched to a known station.

```{r}
#| label: match_fuzzy
#| time_it: true

matched <- (fuzzyjoin::geo_inner_join(
    as.data.frame(rides_l_unk),
    as.data.frame(stations),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(names(rides_l), dist)
  |> arrange(ride_id)
  |> drop_na(ride_id, id, name)
  |> setDT()
)

setkey(matched, ride_id, id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_fuzzy
```


#### SQL (DuckDB)

There are `r scales::label_comma()(rides_l_unk_count$N)` entries from `rides_l` that could be position-matched to a known station.

```{sql connection = "rides_con", label = "match_sql", time_it = TRUE, output.var = "matched.sql"}
SELECT
  ride_id, rideable_type, started_at, ended_at, member_casual, way  
  , COALESCE(r.name, s.name) AS name
  , COALESCE(r.id, s.id) AS id
  , r.lat, r.lng
  , haversine(s.lat_mean, s.lng_mean, r.lat, r.lng) AS dist
FROM rides_l_unk r, stations s
WHERE dist <= 0.011
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_sql
```

:::

<!-- Results --->

```{r}
#| echo: false
#| output: asis
#| column: page

matched
```


:::{.callout-note}
Notice how fast the procedure is, with close to 1 million matches (even if the results are mostly garbage).
:::

**What's inside those matches ?**

```{r}
#| output: asis

matched[, .(`Number of matches for an entry` = .N), by = .(ride_id, way)
      ][, .(`Number of times it happens` = .N), by = `Number of matches for an entry`]
```

We can see that more than half of the matches are coordinates that matched 2 or more stations, which we should definitely not keep.


But, among the ones with only one match, how many have coordinates precise enough to make that match in the first place (i.e. have 4 or more decimals or precision) ?

```{r}
#| output: asis

matched[, if(.N == 1) .SD, by = .(ride_id, way)][decp(lat) & decp(lng)]
```

As it turns out ? Only 3. And those are the same three matches we got from the filtered data.  

In the end, those three are the only three position-based matches we should reasonably keep !


<!-- Cleaning to free up memory --->

```{sql connection = "rides_con", echo = FALSE}
DROP TABLE IF EXISTS rides_l;
DROP TABLE IF EXISTS stations;
DROP TABLE IF EXISTS rides_l_unk;
DROP TABLE IF EXISTS rides_l_clean_unk;
```

```{r}
#| echo: false
#| output: false

rm(rides_l)
rm(rides_l_unk)
rm(rides_l_clean_unk)
rm(stations)
rm(stations_diff)
rm(matched)
rm(matched.sql)
gc()
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Updating the original dataset
***

Finally, we need to update the original dataset (`rides_l_clean`) with the entries that were position-matched (`matched_clean`):


<!-------------------------------------------------------->
## Merging the two datasets

:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| label: merge_dt
#| time_it: true

matched_clean[rides_l_clean, on = setdiff(names(rides_l_clean), c("id", "name"))
            ][, `:=`(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
            ][, nms, env = list(nms = as.list(names(rides_l_clean)))] -> rides_l_merged

# setkey(rides_l_merged, ride_id, id)
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dt
```


#### dtplyr

```{r}
#| label: merge_dtp
#| time_it: true

rides_l_merged.dtp <- (dplyr::right_join(
    matched_clean,
    rides_l_clean,
    by = setdiff(names(rides_l_clean), c("id", "name"))
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y))
  |> select(-matches("\\.x|\\.y"), -dist)
  |> collect()
)

# setkey(rides_l_merged.dtp, ride_id, id)
```

```{r}
#| echo: false

rm(rides_l_merged.dtp)

if (!interactive()) TIMES$merge_dtp
```


#### dplyr

:::{.callout-tip}
`dplyr` has the neat `rows_*` series of functions that can easily replace or patch (i.e. only replace missing values) the content of one dataset by another, when the rows match, which is quite fast !
:::

```{r}
#| label: merge_dp
#| time_it: true

rides_l_merged.dp <- (dplyr::rows_patch(
    rides_l_clean,
    matched_clean[, -"dist"],
    by = setdiff(names(rides_l_clean), c("name", "id")),
    unmatched = "ignore"
  )
)
```

```{r}
#| echo: false

rm(rides_l_merged.dp)
rm(matched_clean)

if (!interactive()) TIMES$merge_dp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "merge_sql", time_it = TRUE}
CREATE TABLE rides_l_merged AS
SELECT 
  ride_id, rideable_type, started_at, ended_at, member_casual, way,
  COALESCE(id_x, id_y) AS id,
  COALESCE(name_x, name_y) AS name,
  lat, lng
FROM (
  SELECT
    r.ride_id AS ride_id,
    r.rideable_type AS rideable_type,
    r.started_at AS started_at,
    r.ended_at AS ended_at,
    r.member_casual AS member_casual,
    r.way AS way,
    m.name AS name_x,
    m.id AS id_x,
    r.lat AS lat,
    r.lng AS lng,
    r.name AS name_y,
    r.id AS id_y
  FROM matched_clean AS m
  RIGHT JOIN rides_l_clean AS r
  ON m.ride_id = r.ride_id 
     AND m.rideable_type = r.rideable_type 
     AND m.started_at = r.started_at
     AND m.ended_at = r.ended_at
     AND m.member_casual = r.member_casual
     AND m.way = r.way
);
```

```{sql connection = "rides_con", echo = FALSE}
DROP TABLE IF EXISTS matched_clean;
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_sql
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_l_merged
```


<!-------------------------------------------------------->
## Validating the merge:

```{r}
#| output: asis

rides_l_merged[(is.na(id) | is.na(name)) & (!is.na(lat) & !is.na(lng))]
```

We can see that the resulting dataset no longer has any entries that have coordinates but miss a `name` or an `id`, whereas there were three before. We have successfully updated them !


<!-------------------------------------------------------->
## Pivoting back to the original (wide) format

To finish, let's pivot the resulting data back into the wider format it was originally in:

:::{.panel-tabset group="framework"}

#### data.table

```{r}
#| label: wide_dt
#| time_it: true

rides_merged <- dcast(
  rides_l_merged, 
  ... ~ way, 
  value.var = c("name", "id", "lat", "lng"), sep = "_station_"
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_dt
```


#### dtplyr

```{r}
#| label: wide_dtp
#| time_it: true

rides_merged.dtp <- (rides_l_merged 
  |> pivot_wider(
    names_from = "way", 
    values_from = c("name", "id", "lat", "lng"), 
    names_glue = "{way}_station_{.value}"
  )
  |> collect()
)
```

```{r}
#| echo: false

rm(rides_merged.dtp)
rm(rides_l_merged)

if (!interactive()) TIMES$wide_dtp
```


#### SQL (DuckDB)

```{sql connection = "rides_con", label = "wide_sql", time_it = TRUE}
CREATE TABLE rides_merged AS
SELECT
  ride_id, rideable_type, started_at, ended_at, member_casual,
  MAX(CASE WHEN (way = 'start') THEN name END) AS start_station_name,
  MAX(CASE WHEN (way = 'end') THEN name END) AS end_station_name,
  MAX(CASE WHEN (way = 'start') THEN id END) AS start_station_id,
  MAX(CASE WHEN (way = 'end') THEN id END) AS end_station_id,
  MAX(CASE WHEN (way = 'start') THEN lat END) AS start_lat,
  MAX(CASE WHEN (way = 'end') THEN lat END) AS end_lat,
  MAX(CASE WHEN (way = 'start') THEN lng END) AS start_lng,
  MAX(CASE WHEN (way = 'end') THEN lng END) AS end_lng
FROM rides_l_merged
GROUP BY ride_id, rideable_type, started_at, ended_at, member_casual
```

```{sql connection = "rides_con", echo = FALSE}
DROP TABLE IF EXISTS rides_merged;
DROP TABLE IF EXISTS rides_l_merged;
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_sql
```

:::

<!-- RESULT --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides_merged
```


:::{.callout-note}
We get less than the original (wide format) ~6 millions entries due to having removed (filtered) the entries with bad coordinates.
:::

```{r}
#| echo: false

DBI::dbDisconnect(rides_con)
```

***

![](http://vignette2.wikia.nocookie.net/creepypasta/images/1/11/Thats_all_folks.svg.png)