---
title: "Fast spatial data matching in R"
subtitle: "How to match locations based on their coordinates"

date: 2022-06-18

abstract: |
  This post showcases various solutions to efficiently match unknown locations to known ones by their geographical proximity (using lat/long coordinates), on a dataset with dozens of millions of entries.

website:
  open-graph:
    description: "Exploring various solutions to quickly match locations by their geographical proximity in R"
  twitter-card:
    description: "Exploring various solutions to quickly match locations by their geographical proximity in R"

aliases:
  - /content/posts/spatial/

categories:
  - "Big Data"
  - "Data Manipulation"
  - "Spatial"
  - "R"
  - "SQL"
  - "DuckDB"

page-layout: article
sidebar-width: 250px
body-width: 1000px
margin-width: 250px
---

{{< include /content/_hr.qmd >}}

:::{.callout-tip}
You can check the source code by clicking on the **</> Code** button at the top-right.

Data can be found [here](https://divvy-tripdata.s3.amazonaws.com/index.html) (June 2021 to May 2022).

This post was prompted by [this reddit question](https://www.reddit.com/r/rstats/comments/vdwi6o/finding_records_based_on_approximate_lattitude/).

It is linked to a case study of **Google's Data Analytics certificate**.
:::

:::{.callout-tip collapse="true"}

# Expand for Version History

**V1:** 2022-06-18  
**V2:** 2022-10-29  
  - Added a `DuckDB` + `arrow` example for the data loading.  
  - Added data cleaning section (removing implausible rides).  
  - Added `sf`, `dbplyr` and `data.table` (manual) examples for the spatial join.  
**V3:** 2023-03-12  
  - Updated code to newest tidyverse version (`.by`, `reframe`, `list_rbind`, `join_by`, ...)  
:::


<!------------------------------------------------------------------------------>
<!------------------------------------------------------------------------------>
# Setup {.unnumbered}
***

```{r}
#| echo: false
#| output: false

source(here::here("src", "init_min.R"), echo = FALSE)

config <- config::get(file = here("_config.yml"))
```

```{r}
#| echo: false
#| eval: false

renv::install(c(
  "here",
  "fs",
  "pipebind",
  "Rdatatable/data.table", # >= 1.14.9
  "dplyr", # >= 1.1.0
  "tidyr",
  "dtplyr", # >= 1.3.0
  "DBI",
  "dbplyr", # >= 2.3.0
  "arrow",
  "duckdb", # >= 0.7.1
  "stringr",
  "purrr", # >= 1.0.0
  "fuzzyjoin",
  "r-spatial/sf"
))
```

```{r}
#| output: false

library(here)        # File path management
library(fs)          # File & folder manipulation
library(pipebind)    # Piping goodies

library(readr)       # Reading data from files           (Tidyverse)
library(dplyr)       # Manipulating data.frames - core   (Tidyverse)
library(tidyr)       # Manipulating data.frames - extras (Tidyverse)
library(stringr)     # Manipulating strings              (Tidyverse)
library(purrr)       # Manipulating lists                (Tidyverse)
library(lubridate)   # Manipulating date/time            (Tidyverse)

library(furrr)       # Manipulating lists in parallel

library(data.table)  # Fast data manipulation
library(dtplyr)      # data.table backend for dplyr      (Tidyverse)

library(DBI)         # Database connection
library(dbplyr)      # SQL back-end for dplyr            (Tidyverse)
library(duckdb)      # Quack Stack
library(arrow)       # Fast and efficient data reading

library(sf)          # Spatial data manipulation

library(fuzzyjoin)   # Non-equi joins & coordinates-based joins

options(
  dplyr.strict_sql = FALSE,
  scipen = 999L, 
  digits = 4L,
  knitr.max_rows_print = 10
)

data.table::setDTthreads(parallel::detectCores())
```

:::{.callout-tip collapse="true"}

# Expand for Session Info

```{r}
#| echo: false
#| results: markup

si <- sessioninfo::session_info(pkgs = "attached")

si$platform$Quarto <- system("quarto --version", intern = TRUE)

si$platform$pandoc <- strsplit(si$platform$pandoc, "@")[[1]][1]

si
```

:::

```{r}
#| echo: false

## This section is for the html output (code-linking, ...)

library(knitr)
library(quarto)
library(downlit)
library(xml2)
library(withr)

#-------------------------#
#### Custom knit_hooks ####
#-------------------------#

TIMES <- list()
knitr::knit_hooks$set(time_it = local({
  start <- NULL
  function(before, options) {
    if (before) start <<- Sys.time()
    else TIMES[[options$label]] <<- difftime(Sys.time(), start)
  }
}))
```

```{r}
#| echo: false
#| output: false
#| file: !expr here("src", "common", "knitr", "knit_print_gt_mono.R")
```

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Loading the data
***

```{r}
data_path <- here("res", "data", "stations")
files <- dir_ls(data_path, glob = "*.csv")
```

```{r}
#| echo: false

duckdb_path <- here(data_path, "stations.db")
```


::: {.callout-caution appearance="simple" collapse="true"}

#### Note on the choice of Database: 

A more logical choice would have been `PostGIS`, an extension of `Postgres` for spatial data, which includes native support for geometry/geography data types, as well as methods for distance calculations. However, setting up and interfacing `PostGIS` with R is significantly more convoluted than `DuckDB`, and analytical queries on `PostGIS` are much slower than on `DuckDB`. The only downside to using `DuckDB` for this example is that we will need to define our haversine (i.e. distance-on-a-sphere) function ourselves.

:::


:::{.panel-tabset}

#### Tidyverse

```{r}
#| label: loading_tidy
#| time_it: true
#| output: false

map(files, \(f) read_csv(f, show_col_types = FALSE)) |> list_rbind()
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_tidy
```


#### data.table

```{r}
#| label: loading_dt
#| time_it: true
#| output: false

rides <- lapply(files, \(f) fread(f, na.strings = "")) |> rbindlist()
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_dt
```


#### DuckDB

```{r}
con_duck <- dbConnect(duckdb(), dbdir = ":memory:") # Or a .db file for out-of-RAM storage
```

```{r}
#| echo: false
#| output: false

knitr::opts_chunk$set(connection = "con_duck")

# dbExecute(con_duck, "SET experimental_parallel_csv=true")
```

```{r}
#| label: loading_duck
#| output: false
#| time_it: true

duckdb_read_csv(con_duck, "rides", files)
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_duck
```

<!-- ALTERNATIVES -->

```{r}
#| eval: false
#| code-fold: true
#| code-summary: Importing while including the file names in the data

## Courtesy of Tristan Mahr (@tjmahr)

withr::with_dir(
  data_path, 
  dbSendQuery(con_duck,"CREATE TABLE rides AS SELECT * FROM read_csv_auto('*.csv', FILENAME = TRUE)")
)
```

```{r}
#| eval: false
#| code-fold: true
#| code-summary: Saving tables in a custom schema (i.e. not `main`)

dbExecute(con_duck, "CREATE SCHEMA IF NOT EXISTS rds")

duckdb_read_csv(con_duck, "rds.rides", files)
```

```{r}
#| eval: false
#| code-fold: true
#| code-summary: Importing an existing `df` from the R env

dbWriteTable(con_duck, "rides", rides_df) # As a TABLE

copy_to(con_duck, rides_df, "rides", indexes = "ride_id")

duckdb_register(con_duck, "rides", rides_df) # As a VIEW
```


#### arrow + DuckDB

```{r}
#| label: loading_arrow_duck
#| output: false
#| time_it: true

open_csv_dataset(data_path, convert_options = CsvConvertOptions$create(strings_can_be_null = TRUE)) |> 
  to_duckdb(con_duck, "rides_arr")
```

```{r}
#| echo: false

if (!interactive()) TIMES$loading_arrow_duck
```


:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Cleaning implausible rides
***

We are going to filter out rides that are implausible (too short, too long, made too quickly, etc). Some of the thresholds used here are slightly arbitrary.

::: {.callout-tip appearance="simple" collapse="true"}

#### Defining our distance function (haversine)

```{r}
haversine <- function(lng1, lat1, lng2, lat2) {
  as.numeric(
    6371 * acos(
      cos(lat1 * pi / 180) * cos(lat2 * pi / 180) * cos((lng1 - lng2) * pi / 180) + 
      sin(lat1 * pi / 180) * sin(lat2 * pi / 180)
    ) * 1000
  )
}
```

```{sql connection = "con_duck"}
CREATE FUNCTION haversine(lng1, lat1, lng2, lat2) 
  AS 6371 * acos( 
    cos(radians(lat1)) * cos(radians(lat2)) * cos(radians(lng2) - radians(lng1)) +
    sin(radians(lat1)) * sin(radians(lat2))
  ) * 1000;
```

:::

:::{.panel-tabset}

#### data.table

```{r}
#| label: rides_clean_dt
#| time_it: true
#| output: false

rides_clean <- rides[, let(
    is_looped = start_station_id == end_station_id | (start_lat == end_lat & start_lng == end_lng), # Does a ride loop on itself
    ride_dur_min = abs(difftime(ended_at, started_at, units = "mins")) # Ride duration (min)
  )
  ][, ride_dist_m := fifelse(
    is_looped & !is.na(is_looped), 0, 
    haversine(start_lng, start_lat, end_lng, end_lat) # Ride distance in meters 
  )
  ][, ride_speed_km_h := fifelse(
      ride_dist_m == 0 | ride_dur_min == 0, NA_real_, 
      (ride_dist_m/1000) / (as.double(ride_dur_min)/60)
    )
  ][, .SD, .SDcols = !patterns("_at$")
  # Rides too fast or too long
  ][ride_dur_min %between% c(5, 1440)
  # Loops that are too fast (<= 10 min)
  ][!(!is.na(is_looped) & is_looped & ride_dur_min <= 10)
  # Rides too short (<= 100m) or too long (>= 30km)
  ][is.na(is_looped) | is_looped | is.na(ride_dist_m) | between(ride_dist_m, 100, 30000)
  # Rides with implausible speeds
  ][(rideable_type == "classic_bike" & ride_speed_km_h <= 40)
    | (rideable_type == "electric_bike" & ride_speed_km_h <= 60)
  ]
```

```{r}
#| echo: false

if (!interactive()) TIMES$rides_clean_dt
```

#### dbplyr

```{r}
#| label: rides_clean_dbp
#| time_it: true
#| output: false

(tbl(con_duck, "rides")
  |> mutate(is_looped = start_station_id == end_station_id | (start_lat == end_lat & start_lng == end_lng))
  |> mutate(across(ends_with("_at"), \(x) strptime(x, "%Y-%m-%d %H:%M:%S")))
  |> mutate(ride_dur_min = abs(date_sub("second", started_at, ended_at) / 60))
  |> mutate(ride_dist_m = case_when(
    is_looped ~ 0, 
    is.na(start_lng) | is.na(start_lat) | is.na(end_lng) | is.na(end_lat) ~ NA_real_,
    .default = haversine(start_lng, start_lat, end_lng, end_lat)) # This will call the SQL haversine function
  )
  |> mutate(ride_speed_km_h = case_when(
    ride_dist_m == 0 | ride_dur_min == 0 ~ NA_real_, 
    .default = (ride_dist_m / 1000) / (ride_dur_min/60))
  )
  |> select(-ends_with("_at"))
  # Rides too fast or too long
  |> filter(between(ride_dur_min, 5, 1440))
  # Loops that are too fast (<= 10 min)
  |> filter(!(!is.na(is_looped) & is_looped & ride_dur_min <= 10))
  # Rides too short (<= 100m) or too long (>= 30km)
  |> filter(is.na(is_looped) | is_looped | is.na(ride_dist_m) | between(ride_dist_m, 100, 30000))
  # Rides with implausible speeds
  |> filter(
    (rideable_type == "classic_bike" & ride_speed_km_h <= 40)
    | (rideable_type == "electric_bike" & ride_speed_km_h <= 60)
  )
  |> compute("rides_clean")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$rides_clean_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides_clean
```


<!-- CLEANING --->

```{r}
#| echo: false

## We'll use the cleaned data for the rest of the document

dbRemoveTable(con_duck, "rides")
rm(rides)
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Pivoting the data
***

We need to pivot each station (start and end) into its own row in order to facilitate subsequent operations based on their coordinates.

:::{.panel-tabset}

#### data.table

```{r}
#| label: long_dt
#| time_it: true

rides_clean_l <- melt(
  rides_clean,
  measure = measure(way, value.name, pattern = "(end|start).*_(name|id|lat|lng)")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dt
```


#### dtplyr

```{r}
#| label: long_dtp
#| time_it: true
#| output: false

pivot_longer(
  rides_clean, matches("^end_|^start_"),
  names_pattern = "(end|start).*_(name|id|lat|lng)", names_to = c("way", ".value")
) |> as.data.table()
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "long_sql", time_it = TRUE}
CREATE TABLE rides_clean_l AS 
(
  SELECT
    ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h
    , 'start' AS way
    , start_station_name AS "name"
    , start_station_id AS id
    , start_lat AS lat
    , start_lng AS lng
  FROM rides_clean
)
UNION ALL
(
  SELECT
    ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h
    , 'end' AS way
    , end_station_name AS "name"
    , end_station_id AS id
    , end_lat AS lat
    , end_lng AS lng
  FROM rides_clean
);
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_sql
```


#### dbplyr

```{r}
#| label: long_dbp
#| time_it: true
#| output: false

(pivot_longer(
    tbl(con_duck, "rides_clean"),
    matches("^end_|^start_"),
    names_pattern = "(end|start).*_(name|id|lat|lng)", 
    names_to = c("way", ".value")
  )
  |> compute("rides_clean_l_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$long_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| column: page
#| output: asis
#| total_rows: !expr tbl(con_duck, "rides_clean_l") |> count() |> pull(n)

tbl(con_duck, "rides_clean_l") |> head(10) |> collect()
```


<!-- CLEANING --->

```{r}
#| echo: false
#| output: false

## We'll use the long format data for the rest of the document

rm(rides_clean)
dbRemoveTable(con_duck, "rides_clean")
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Cleaning imprecise coordinates
***

Removing entries were `lat`/`lng` do not have sufficient precision to be reliably matched to a station (i.e. entries having less than 4 decimals, which corresponds to a 11 meters "radius" at the equator).

::: {.callout-tip collapse="true"}

#### Degrees to distance equivalence

| **Decimal** | **Distance at the equator (m)** |
|-------------|---------------------------------|
| 0           | 111,120                         |
| 1           | 11,112                          |
| 2           | 1,111.2                         |
| 3           | 111.12                          |
| 4           | 11.112                          |
| 5           | 1.1112                          |

:::

::: {.callout-tip appearance="simple" collapse="true"}

#### Defining our "decimal precision" function

```{r}
decp <- function(x) str_length(str_remove(as.character(abs(x)), ".*\\.")) >= 4
```

```{sql connection = "con_duck"}
CREATE FUNCTION decp(x) AS length(str_split(CAST(abs(x) AS VARCHAR(10)), '.')[2]) >= 4
```

```{sql connection = "con_duck", eval = FALSE, echo = FALSE}
CREATE FUNCTION decp(x) AS length(regexp_replace(CAST(abs(x) AS VARCHAR(50)), '(.*)[.]', '')) >= 4 -- Slower
```

:::


::: {.panel-tabset}

#### data.table

```{r}
#| label: acc_dt
#| time_it: true

rides_clean_l_acc <- rides_clean_l[decp(lat) & decp(lng), ]
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_dt
```


#### dtplyr

```{r}
#| label: acc_dtp
#| output: false
#| time_it: true

rides_clean_l |> filter(decp(lat) & decp(lng)) |> as.data.table()
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "acc_duck", time_it = TRUE}
CREATE TABLE rides_clean_l_acc AS
SELECT * FROM rides_clean_l 
WHERE decp(lat) AND decp(lng)
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_duck
```


#### dbplyr

:::: {.callout-note appearance="simple"}
Here, `dbplyr` will leave the `decp` call as-is in the SQL translation, but since we have previously defined a `decp` SQL function, this function will get called when the SQL query is executed.
::::

```{r}
#| label: acc_dbp
#| output: false
#| time_it: true

(tbl(con_duck, "rides_clean_l_dbp") 
  |> filter(if_all(c(lat, lng), \(x) decp(x)))
  |> compute("rides_clean_l_acc_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$acc_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| column: page
#| output: asis

rides_clean_l_acc
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Exploring the data
***

<!-------------------------------------------------------->
## Known stations missing coordinates

**Stations missing either `lat` or `lng`, but having an `id` or `name`:**

:::{.panel-tabset}

#### data.table

```{r}
#| eval: false

rides_clean_l_acc[(is.na(lat) | is.na(lng)) & (!is.na(id) | !is.na(name)), ]
```


#### SQL (DuckDB)

```{sql connection = "con_duck", eval = FALSE}
SELECT * FROM rides_clean_l_acc 
WHERE ((lat IS NULL) OR (lng IS NULL) 
  AND (NOT((id IS NULL)) OR NOT((name IS NULL))))
```


#### dbplyr

```{r}
rides_clean_l_no_coords <- (tbl(con_duck, "rides_clean_l_acc_dbp")
  |> filter(if_any(c(lat, lng), \(v) is.na(v)) & (!is.na(id) | !is.na(name)))
  |> collect()
)
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_clean_l_no_coords

rm(rides_clean_l_no_coords)
```


<!-------------------------------------------------------->
## Unknown stations (accuracy-filtered)

**Stations missing either `name` or `id`, but having coordinates with sufficient accuracy:**

:::{.panel-tabset}

#### data.table

```{r}
rides_clean_l_acc[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name))] -> rides_clean_l_acc_unk
```


#### SQL (DuckDB)

```{sql connection = "con_duck"}
CREATE TABLE rides_clean_l_acc_unk AS
SELECT * FROM rides_clean_l_acc
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "con_duck", echo = FALSE, output.var = "rides_clean_l_acc_unk_count"}
SELECT COUNT(*) AS N FROM rides_clean_l_acc_unk
```


#### dbplyr

```{r}
#| output: false

(tbl(con_duck, "rides_clean_l_acc_dbp") 
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> compute("rides_clean_l_acc_unk_dbp")
)
```


:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_clean_l_acc_unk_count$N

DBI::dbGetQuery(con_duck, "SELECT * FROM rides_clean_l_acc_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

It seems there are only `r rides_clean_l_acc_unk_count$N` entries missing identification that could be matched based on their coordinates, at least at the level of precision we require (11m / 4 decimals).

If we look at the unfiltered data (i.e. disregarding the accuracy of the coordinates), there will most likely be many more:

<!-------------------------------------------------------->
## Unknown stations (unfiltered)

**Stations missing either `name` or `id`, but having coordinates:**

:::{.panel-tabset}

#### data.table

```{r}
rides_clean_l[(!is.na(lat) & !is.na(lng)) & (is.na(id) | is.na(name))] -> rides_clean_l_unk
```


#### SQL (DuckDB)

```{sql connection = "con_duck"}
CREATE TABLE rides_clean_l_unk AS
SELECT * FROM rides_clean_l
WHERE ((id IS NULL) OR (name IS NULL)) 
  AND (NOT((lat IS NULL)) AND NOT((lng IS NULL)))
```

```{sql connection = "con_duck", echo = FALSE, output.var = "rides_clean_l_unk_count"}
SELECT COUNT(*) AS N FROM rides_clean_l_unk
```


#### dbplyr

```{r}
#| output: false

(tbl(con_duck, "rides_clean_l_dbp") 
  |> filter(if_all(c(lat, lng), \(v) !is.na(v)) & (is.na(id) | is.na(name)))
  |> compute("rides_clean_l_unk_dbp")
)
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page
#| total_rows: !expr rides_clean_l_unk_count$N

DBI::dbGetQuery(con_duck, "SELECT * FROM rides_clean_l_unk ORDER BY ride_id", n = getOption("knitr.max_rows_print"))
```

There are `r scales::label_comma()(rides_clean_l_unk_count$N)` stations missing an `id` or `name` in the original data. 

The fact that we only have `r rides_clean_l_acc_unk_count$N` unknown entries left after filtering insufficiently accurate coordinates (i.e. less than 4 decimals of precision) probably means that most of those `r scales::label_comma()(rides_clean_l_unk_count$N)` entries were missing their `id`/`name` *because* their coordinates were too imprecise to be matched to any station in the first place.


We will attempt to position-match those unknown stations (both filtered and not) in the following section.

<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Stations data
***

First, we need to assemble a reference/look-up table, i.e. a dataset linking each unique station `id` (and `name`) with a set of coordinates (here, we use the average `lat` & `lng`.

:::{.panel-tabset}

#### data.table

```{r}
#| label: stations_clean_dt
#| time_it: true

stations_clean <- (rides_clean_l_acc
  |> na.omit(cols = c("id", "name"))
  |> dcast(id+name ~ ., fun.agg = list(min, max, mean), value.var = c("lat", "lng"))
  |> bind(x, setcolorder(x, c("id", "name", str_subset(colnames(x), "lat_|_lng"))))
  |> unique(by = "id")
  |> setorder(id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_dt
```

#### SQL (DuckDB)

```{sql connection = "con_duck", label = "stations_clean_sql", time_it = TRUE}
CREATE TABLE stations_clean AS 
SELECT
  id
  , FIRST("name") AS "name"
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_clean_l_acc
WHERE (NOT((id IS NULL))) AND (NOT(("name" IS NULL)))
GROUP BY id
ORDER BY id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_sql
```

#### dbplyr

```{r}
#| output: false
#| label: stations_clean_dbp
#| time_it: true

(tbl(con_duck, "rides_clean_l_acc_dbp")
  |> filter(!is.na(id), !is.na(name))
  |> summarize(
    name = first(name),
    across(c(lat, lng), list("min" = min, "max" = max, "mean" = mean), .names = "{.col}_{.fn}"),
    .by = id
  )
  |> arrange(id)
  |> compute("stations_clean_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_clean_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr DBI::dbGetQuery(con_duck, "SELECT COUNT(*) AS N FROM stations_clean")$N

dbGetQuery(con_duck, "SELECT * FROM stations_clean ORDER BY id", n = getOption("knitr.max_rows_print"))
```



<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Spatial join
***

Let's match the entries of `rides_clean_l_acc` with `stations_clean` by proximity:

<!-------------------------------------------------------->
## Matching on the cleaned data

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. `rides_clean_l_acc_unk`, the ones having coordinates but missing either `name` or `id`). There are `r scales::label_comma()(nrow(rides_clean_l_acc_unk))` entries from `rides_clean_l_acc` that could be position-matched to a known station.


:::{.panel-tabset}

#### data.table

```{r}
#| label: match_clean_manual
#| time_it: true
#| output: false

stations_clean[, c(k = 1, .SD)
             ][rides_clean_l_acc_unk[, c(k = 1, .SD)], on = "k", allow.cartesian = TRUE
             ][, dist := haversine(lng, lat, lng_mean, lat_mean)
             ][dist <= 11, .SD[which.min(dist)], by = c("ride_id", "way")
             ][, let(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
             ][, .SD, .SDcols = c(colnames(rides_clean_l_acc_unk), "dist")]
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_manual
```


#### fuzzyjoin

```{r}
#| label: match_clean_fuzzy
#| time_it: true
#| output: false

matched_clean <- (geo_inner_join(
    as.data.frame(rides_clean_l_acc_unk),
    as.data.frame(stations_clean),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> lazy_dt()
  |> filter(which.min(dist), .by = c(ride_id, way))
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_clean_l_acc_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_fuzzy
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "match_clean_sql", time_it = TRUE}
CREATE TABLE matched_clean AS
SELECT ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h, way, name, id, lat, lng, dist
FROM (
  SELECT *, RANK() OVER (PARTITION BY ride_id, way ORDER BY dist) AS dist_rank
  FROM (
    SELECT
      ride_id, rideable_type, member_casual, way, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h  
      , COALESCE(r.name, s.name) AS name
      , COALESCE(r.id, s.id) AS id
      , r.lat AS lat
      , r.lng AS lng
      , haversine(s.lng_mean, s.lat_mean, r.lng, r.lat) AS dist
    FROM rides_clean_l_acc_unk r, stations_clean s
    WHERE dist <= 11
  )
)
WHERE dist_rank <= 1
ORDER BY ride_id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_sql
```


#### dbplyr

```{r}
#| label: match_clean_dbp
#| time_it: true
#| output: false

(inner_join(
    tbl(con_duck, "rides_clean_l_acc_unk_dbp"),
    tbl(con_duck, "stations_clean_dbp"),
    sql_on = "haversine(LHS.lng, LHS.lat, RHS.lng_mean, RHS.lat_mean) <= 11"
  )
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> slice_min(dist, by = c(ride_id, way))
  |> select(colnames(tbl(con_duck, "rides_clean_l_acc_unk_dbp")), dist)
  |> arrange(ride_id)
  |> compute("matched_clean_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_dbp
```

#### sf

```{r}
#| label: match_clean_sf
#| time_it: true
#| output: false

(st_join(
    st_as_sf(rides_clean_l_acc_unk, coords = c("lat", "lng"), remove = FALSE, crs = 4326),
    st_as_sf(stations_clean, coords = c("lat_mean", "lng_mean"), remove = FALSE, crs = 4326),
    join = st_is_within_distance,
    dist = 11, # In meters
    left = FALSE # Does an inner_join
  )
  |> st_drop_geometry()
  |> lazy_dt()
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> filter(which.min(dist), .by = c(ride_id, way))
  |> select(colnames(rides_clean_l_acc_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_clean_sf
```

:::

```{r}
#| echo: false
#| output: asis
#| column: page

matched_clean
```

And our `r nrow(matched_clean)` rows missing an id or name get matched !


:::{.callout-note appearance="simple"}
However, those `r nrow(matched_clean)` already had an `id` (they were only missing a `name`). We could have filled their missing `name` with a join to `stations_clean`, instead of a convoluted proximity-based matching (which is more resource intensive and less accurate).

Let's check that both methods yield the same results:

```{r}
#| output: asis

stations_clean[matched_clean, on = .(id)
             ][, .(id, name.stations = name, name.proximity = i.name, lat, lng)]
```

At least, we can see that the proximity-based matched name and the one associated to that station in `stations_clean` are the same, meaning our proximity-matching method works reasonably well.

:::


<!-------------------------------------------------------->
## Matching on the original data

What if we did the same procedure on the non-cleaned data (the one with coordinates less precise than our criteria for matching) ?

### Unfiltered `stations` data

First, we need to recompute the `stations` data from `rides_clean_l` (i.e. rides data before removing inaccurate coordinates):

:::{.panel-tabset}

#### data.table

```{r}
#| label: stations_dt
#| time_it: true

stations <- (rides_clean_l
  |> na.omit(cols = c("id", "name"))
  |> dcast(id+name ~ ., fun.agg = list(min, max, mean), value.var = c("lat", "lng"))
  |> bind(x, setcolorder(x, c("id", "name", str_subset(colnames(x), "lat_|_lng"))))
  |> unique(by = "id")
  |> setorder(id)
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_dt
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "stations_sql", time_it = TRUE}
CREATE TABLE stations AS 
SELECT
  id
  , FIRST("name") AS "name"
  , MIN(lat) AS lat_min
  , MAX(lat) AS lat_max
  , AVG(lat) AS lat_mean
  , MIN(lng) AS lng_min
  , MAX(lng) AS lng_max
  , AVG(lng) AS lng_mean
FROM rides_clean_l
WHERE (NOT((id IS NULL))) AND (NOT(("name" IS NULL)))
GROUP BY id
ORDER BY id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_sql
```


#### dbplyr

```{r}
#| output: false
#| label: stations_dbp
#| time_it: true

(tbl(con_duck, "rides_clean_l_dbp")
  |> filter(!is.na(id), !is.na(name))
  |> summarize(
    name = first(name),
    across(c(lat, lng), list("min" = min, "max" = max, "mean" = mean), .names = "{.col}_{.fn}"),
    .by = id
  )
  |> arrange(id)
  |> compute("stations_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$stations_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| total_rows: !expr dbGetQuery(con_duck, "SELECT COUNT(*) AS N FROM stations")$N

dbGetQuery(con_duck, "SELECT * FROM stations ORDER BY id", n = getOption("knitr.max_rows_print"))
```


**Cleaning the results:**

Notice we get a lot more entries in our `stations`: `r nrow(stations)` entries vs `r nrow(stations_clean)` entries in the filtered version.

Which entries are in `stations` but not in `stations_clean` ?

```{r}
#| output: asis

(stations_diff <- stations[!stations_clean, on = .(id, name)])
```

And which of those `r nrow(stations_diff)` entries have the necessary coordinate precision to be used later on to match against the unknown stations ?

```{r}
#| output: asis

stations_diff[stations_diff[, Reduce(`&`, lapply(.SD, decp)), .SDcols = patterns("^lat|^lng")]]
```

As expected, none. But we're still going to do the matching, for posterity !


### Position-matching on `stations`

To save time, let's only apply the procedure to the entries that actually need to be matched (i.e. the ones having coordinates but missing either `name` or `id`). There are `r scales::label_comma()(nrow(rides_clean_l_unk))` entries from `rides_clean_l` that could be position-matched to a known station.

:::{.panel-tabset}

#### data.table

We have to split the `rides_clean_l_unk` dataset in chunks of 10k rows, because its full cross-join with `stations` would be 1.3 billion rows (with 12 columns), which would definitely not fit in my RAM. The chunks are processed in parallel to speed it up (4 at a time, to avoid saturating my RAM). You might need to adjust those values on your machine.

```{r}
#| label: match_manual
#| time_it: true
#| output: false

future::plan(future::multisession, workers = 4)

future_map_dfr(
  copy(rides_clean_l_unk)[, let(ID = ceiling(.I/10000), k = 1)] |> split(by = "ID"),
  \(dat) 
    stations[, c(k = 1, .SD)][dat, on = "k", allow.cartesian = TRUE][
            , dist := haversine(lng, lat, lng_mean, lat_mean)
            ][dist <= 11, .SD[which.min(dist)], by = c("ride_id", "way")
            ][, let(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
            ][, .SD, .SDcols = c(colnames(rides_clean_l_unk), "dist")]
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_manual
```


#### fuzzyjoin

```{r}
#| label: match_fuzzy
#| time_it: true
#| output: false

matched <- (geo_inner_join(
    as.data.frame(rides_clean_l_unk),
    as.data.frame(stations),
    by = c("lng" = "lng_mean", "lat" = "lat_mean"),
    method = "haversine",
    unit = "km",
    max_dist = 0.011, # 11 meters
    distance_col = "dist"
  ) 
  |> lazy_dt()
  |> filter(which.min(dist), .by = c(ride_id, way))
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y)) 
  |> select(colnames(rides_clean_l_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_fuzzy
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "match_sql", time_it = TRUE}
CREATE TABLE matched AS
SELECT ride_id, rideable_type, member_casual, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h, way, name, id, lat, lng, dist
FROM (
  SELECT *, RANK() OVER (PARTITION BY ride_id, way ORDER BY dist) AS dist_rank
  FROM (
    SELECT
      ride_id, rideable_type, member_casual, way, is_looped, ride_dur_min, ride_dist_m, ride_speed_km_h  
      , COALESCE(r.name, s.name) AS name
      , COALESCE(r.id, s.id) AS id
      , r.lat AS lat
      , r.lng AS lng
      , haversine(s.lng_mean, s.lat_mean, r.lng, r.lat) AS dist
    FROM rides_clean_l_unk r, stations s
    WHERE dist <= 11
  )
)
WHERE dist_rank <= 1
ORDER BY ride_id;
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_sql
```

#### dbplyr

```{r}
#| label: match_dbp
#| time_it: true
#| output: false

(inner_join(
    tbl(con_duck, "rides_clean_l_unk_dbp"),
    tbl(con_duck, "stations_dbp"),
    sql_on = "haversine(LHS.lng, LHS.lat, RHS.lng_mean, RHS.lat_mean) <= 11"
  )
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> slice_min(dist, by = c(ride_id, way))
  |> select(colnames(tbl(con_duck, "rides_clean_l_unk_dbp")), dist)
  |> arrange(ride_id)
  |> compute("matched_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_dbp
```

#### sf

```{r}
#| label: match_sf
#| time_it: true
#| output: false

(st_join(
    st_as_sf(rides_clean_l_unk, coords = c("lat", "lng"), remove = FALSE, crs = 4326),
    st_as_sf(stations, coords = c("lat_mean", "lng_mean"), remove = FALSE, crs = 4326),
    join = st_is_within_distance,
    dist = 11, # In meters
    left = FALSE # Does an inner_join
  )
  |> st_drop_geometry()
  |> lazy_dt()
  |> mutate(
    dist = haversine(lng, lat, lng_mean, lat_mean),
    name = coalesce(name.x, name.y), 
    id = coalesce(id.x, id.y)
  )
  |> filter(which.min(dist), .by = c(ride_id, way))
  |> select(colnames(rides_clean_l_unk), dist)
  |> arrange(ride_id)
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$match_sf
```

:::

<!-- Results --->

```{r}
#| echo: false
#| output: asis
#| column: page

matched
```


**What's inside those matches ?**

```{r}
#| output: asis
#| echo: false
#| eval: false

matched[, .(`Number of matches for an entry` = .N), by = .(ride_id, way)
      ][, .(`Number of times it happens` = .N), by = `Number of matches for an entry`]


# We can see that ... of the matches are unknown stations which matched 2 or more known ones, which we should definitely discard.
```

Among the unknown stations that got a single match, how many have coordinates precise enough to ensure they have not been potentially matched to a neighboring station ? (i.e. have 4 or more decimals or precision)

```{r}
matched[, if(.N == 1) .SD, by = .(ride_id, way)][decp(lat) & decp(lng)] -> matched_valid
```

```{r}
#| echo: false
#| output: asis
#| column: page

matched_valid
```


As it turns out ? Only `r nrow(matched_valid)`. And those are the same `r nrow(matched_clean)` matches we got from the filtered data.  

In the end, those `r nrow(matched_clean)` are the only position-based matches we should reasonably keep !


<!-- Cleaning to free up memory --->

```{r}
#| echo: false
#| output: false

tables_to_remove <- c("stations", "stations_clean", "rides_clean_l_unk", "rides_clean_l_acc_unk")

walk(tables_to_remove, \(x) dbRemoveTable(con_duck, x, fail_if_missing = FALSE))

rm(rides_clean_l_unk)
rm(rides_clean_l_acc_unk)
rm(stations)
rm(stations_clean)
rm(stations_diff)
rm(matched)
rm(matched.sql)
```


<!-------------------------------------------------------->
<!-------------------------------------------------------->
# Updating the original dataset
***

Finally, we need to update the original dataset (`rides_clean_l`) with the entries that were position-matched (`matched_clean`):


<!-------------------------------------------------------->
## Merging the two datasets

:::{.callout-note appearance="simple"}
We can update the initial (cleaned) data with the position-matched in two ways:  
- A `join` + `coalesce`: join the two tables and merge the two `name` & `id` columns (new and old) together.  
- A `rows_patch`: only replace the missing values (`name` & `id`) by matching new and old rows together.
:::

### join + coalesce

:::{.panel-tabset}

#### data.table

```{r}
#| label: merge_dt
#| time_it: true
#| output: false

matched_clean[rides_clean_l_acc, on = setdiff(colnames(rides_clean_l_acc), c("id", "name"))
            ][, let(name = fcoalesce(name, i.name), id = fcoalesce(id, i.id))
            ][, nms, env = list(nms = I(colnames(rides_clean_l_acc)))]
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dt
```


#### dtplyr

```{r}
#| label: merge_dtp
#| output: false
#| time_it: true

rides_clean_l_merged <- (right_join(
    matched_clean,
    rides_clean_l_acc,
    by = setdiff(colnames(rides_clean_l_acc), c("id", "name"))
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y))
  |> select(-matches("\\.x|\\.y"))
  |> as.data.table()
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "merge_sql", time_it = TRUE}
CREATE TABLE rides_clean_l_merged AS
SELECT 
  ride_id, rideable_type, member_casual, way, ride_dur_min, ride_dist_m, ride_speed_km_h, is_looped  
  , COALESCE(id_x, id_y) AS id
  , COALESCE(name_x, name_y) AS name
  , lat, lng
FROM (
  SELECT
    r.ride_id AS ride_id
    , r.rideable_type AS rideable_type
    , r.member_casual AS member_casual
    , r.way AS way
    , r.ride_dur_min AS ride_dur_min
    , r.ride_dist_m AS ride_dist_m
    , r.ride_speed_km_h AS ride_speed_km_h
    , r.is_looped AS is_looped
    , m.name AS name_x
    , m.id AS id_x
    , r.lat AS lat
    , r.lng AS lng
    , r.name AS name_y
    , r.id AS id_y
  FROM matched_clean AS m
  RIGHT JOIN rides_clean_l AS r
  ON m.ride_id = r.ride_id 
     AND m.rideable_type = r.rideable_type 
     AND m.member_casual = r.member_casual
     AND m.way = r.way
);
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_sql
```


#### dbplyr

```{r}
#| label: merge_dbp
#| output: false
#| time_it: true

(right_join(
    tbl(con_duck, "matched_clean_dbp"),
    tbl(con_duck, "rides_clean_l_dbp"),
    by = tbl(con_duck, "rides_clean_l_dbp") |> select(-name, -id) |> colnames()
  ) 
  |> mutate(name = coalesce(name.x, name.y), id = coalesce(id.x, id.y))
  |> select(-matches("\\.x|\\.y"))
  |> compute("rides_clean_l_merged_dbp")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$merge_dbp
```

:::

### rows_patch

```{r}
#| label: merge_dp_row
#| output: false
#| time_it: true

rows_patch(
  rides_clean_l,
  matched_clean[, !"dist"],
  by = setdiff(colnames(rides_clean_l), c("name", "id")),
  unmatched = "ignore"
)
```

```{r}
#| echo: false

rm(rides_clean_l)

if (!interactive()) TIMES$merge_dp_row
```


<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: page

rides_clean_l_merged
```


<!-------------------------------------------------------->
## Validating the merge:

```{r}
#| output: asis

rides_clean_l_merged[(is.na(id) | is.na(name)) & (!is.na(lat) & !is.na(lng))]
```

We can see that the resulting dataset no longer has any entries that have coordinates but miss a `name` or an `id`, whereas there were `r nrow(matched_clean)` before. We have successfully updated them !


<!-------------------------------------------------------->
## Pivoting back to the original (wide) format

To finish, let's pivot the resulting data back into the wider format it was originally in:

:::{.panel-tabset}

#### data.table

```{r}
#| label: wide_dt
#| time_it: true

rides_merged <- dcast(
  rides_clean_l_merged, ... ~ way, sep = "_station_", value.var = c("name", "id", "lat", "lng")
)
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_dt
```


#### dtplyr

```{r}
#| label: wide_dtp
#| output: false
#| time_it: true

pivot_wider(
  rides_clean_l_merged, names_from = "way", names_glue = "{way}_station_{.value}",
  values_from = c("name", "id", "lat", "lng")
) |> as.data.table()
```

```{r}
#| echo: false

rm(rides_clean_l_merged)

if (!interactive()) TIMES$wide_dtp
```


#### SQL (DuckDB)

```{sql connection = "con_duck", label = "wide_sql", time_it = TRUE}
CREATE TABLE rides_merged AS
SELECT
  ride_id, rideable_type, member_casual, ride_dur_min, ride_dist_m, ride_speed_km_h
  , MAX(CASE WHEN (way = 'start') THEN "name" END) AS start_station_name
  , MAX(CASE WHEN (way = 'end') THEN "name" END) AS end_station_name
  , MAX(CASE WHEN (way = 'start') THEN id END) AS start_station_id
  , MAX(CASE WHEN (way = 'end') THEN id END) AS end_station_id
  , MAX(CASE WHEN (way = 'start') THEN lat END) AS start_lat
  , MAX(CASE WHEN (way = 'end') THEN lat END) AS end_lat
  , MAX(CASE WHEN (way = 'start') THEN lng END) AS start_lng
  , MAX(CASE WHEN (way = 'end') THEN lng END) AS end_lng
FROM rides_clean_l_merged
GROUP BY ride_id, rideable_type, member_casual, ride_dur_min, ride_dist_m, ride_speed_km_h;
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_sql
```


#### dbplyr

```{r}
#| label: wide_dbp
#| output: false
#| time_it: true

pivot_wider(
  tbl(con_duck, "rides_clean_l_merged_dbp"), names_from = "way", 
  names_glue = "{way}_station_{.value}", values_from = c("name", "id", "lat", "lng")
) |> compute("rides_merged_dbp")
```

```{r}
#| echo: false

if (!interactive()) TIMES$wide_dbp
```

:::

<!-- RESULTS --->

```{r}
#| echo: false
#| output: asis
#| column: screen-inset

rides_merged
```


```{r}
#| echo: false

dbDisconnect(con_duck, shutdown = TRUE)
if (exists("duckdb_path") && file_exists(duckdb_path)) file_delete(duckdb_path)
```

***

![](http://vignette2.wikia.nocookie.net/creepypasta/images/1/11/Thats_all_folks.svg.png)