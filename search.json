[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marc-Aurèle Rivière",
    "section": "",
    "text": "I’m a recovering academic who worked on several research projects at the intersection between Cognitive Neurosciences and Biomedical Engineering. Those projects aimed to develop and evaluate wearable assistive devices for Visually Impaired People (VIP), providing them with a non-visual experience of their surroundings through the use of Computer Vision and Augmented Reality, within the Sensory Substitution framework.\nI have since retrained as a Data Scientist with a fondness for statistical modeling (especially of the Bayesian sort), and an unhealthy obsession with the R ecosystem and its community. I also dabble in more generic programming languages such as Java, C# (Unity), and Javascript.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/sci-com/talks/SKERI/index.html",
    "href": "content/sci-com/talks/SKERI/index.html",
    "title": "Sensory substitution as a framework to access visual and spatial information for VIP",
    "section": "",
    "text": "Project (ACCESSPACE)\n  \n  \n      Project (TETMOST)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/sci-com/talks/ProbAI19/index.html",
    "href": "content/sci-com/talks/ProbAI19/index.html",
    "title": "Spatial Cognition and Computer Vision for better assistive devices",
    "section": "",
    "text": "Project (ACCESSPACE)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/sci-com/outreach/Exp/index.html",
    "href": "content/sci-com/outreach/Exp/index.html",
    "title": "Developing assistive decives for VIP: an overview",
    "section": "",
    "text": "My Expérimentarium record ()\n  \n\n      \n\n    \n    \n  \n\n\n\nFor 3 years (2017 to 2020), I participated in the Experimentarium program: a French initiative to popularize research to the wider public. During the year, they would organize single-day interventions of multiple PhD students into local middle & high-schools, where we would each present our research in an accessible manner to multiple groups of students, and answer their questions about the academic world.\nIn addition to that, once a year, they would organize a 3-days long science fair in one French city, where all the PhD students involved in the program (from all around France) would gather and present their research in different settings and in different formats (from interactive presentations in schools to “speed searching” in cafés, or linking our research to relevant Art pieces in Museums, …).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/sci-com/index.html",
    "href": "content/sci-com/index.html",
    "title": " Communications",
    "section": "",
    "text": "Talks\n\nScientific communication for experts & researchers during conferences, seminars & invited talks\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nACCESSPACE: overview and future prospects\n\n\nRencontres Universitaires Numériques Normandes (RUNN’19)\n\n\n\nAssistive Devices\n\n\nAccessibility\n\n\nSensory Substitution\n\n\nSpatial Cognition\n\n\n\nAn overview of the ACCESSPACE project’s progress and future endeavors ()\n\n\n\n\n\nNov 21, 2019\n\n\nCaen, France\n\n\n\n\n\n\n\nSpatial Cognition and Computer Vision for better assistive devices\n\n\nFrench-Norwegian Workshop Day (2019)\n\n\n\nAssistive Devices\n\n\nAccessibility\n\n\nSensory Substitution\n\n\nSpatial Cognition\n\n\nComputer Vision\n\n\n\nAcademic talk presenting my work on using Cognitive Neurosciences and Computer Vision to develop better assistive devices for VIP\n\n\n\n\n\nJun 2, 2019\n\n\nTrondheim, Norway\n\n\n\n\n\n\n\nSensory substitution as a framework to access visual and spatial information for VIP\n\n\nSKERI Brown Bag\n\n\n\nAssistive Devices\n\n\nAccessibility\n\n\nSpatial Cognition\n\n\nSensory Substitution\n\n\nComputer Vision\n\n\n\nSeminar hosted at Smith-Kettlewell Eye-Research Institute, presenting the Sensory Substitution framework and its use in designing non-visual interfaces for VIP\n\n\n\n\n\nJan 9, 2019\n\n\nSan Francisco, USA\n\n\n\n\n\n\n\nTactiBelt: an Electronic Travel Aid prototype for VIP\n\n\nInt. Conf. on Computers Helping People (ICCHP’18)\n\n\n\nAssistive Devices\n\n\nAccessibility\n\n\nSpatial Cognition\n\n\nSensory Substitution\n\n\nVirtual Reality\n\n\n\nShort academic talk given during the ICCHP 2018 international conference to present the TactiBelt\n\n\n\n\n\nJul 12, 2018\n\n\nLinz, Austria\n\n\n\n\n\n\nNo matching items\n\n\n\n\n Outreach\n\nScientific communication for laymen during outreach events\n\n\n\n\n\n\n\nPercieving our world without vision\n\n\nCollege Fair of the University of Rouen\n\n\nPresenting my doctoral research project to highschool students during the College Fair of 2017 ()\n\n\n\n\n\n\n\n\n\n\nDeveloping assistive decives for VIP: an overview\n\n\nExpérimentarium Science Fair\n\n\nPresenting my doctoral research projects to a wide-ranging public of all backgrounds (from middle-school students to adults), during a yearly 3-days long science fair…\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/pubs/NER19/index.html",
    "href": "content/pubs/NER19/index.html",
    "title": "NAV-VIR: an audio-tactile virtual environment to assist visually impaired people",
    "section": "",
    "text": "Article (online)\n  \n  \n      Project (NAV-VIR)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@inproceedings{rivière2019,\n  author = {Rivière, Marc-Aurèle and Gay, Simon and Romeo, Katerine and\n    Pissaloux, Edwige and Bujacz, Michal and Skulimowski, Piotr and\n    Strumillo, Pawel},\n  publisher = {IEEE},\n  title = {NAV-VIR: An Audio-Tactile Virtual Environment to Assist\n    Visually Impaired People},\n  booktitle = {Proceedings of the International IEEE/EMBS Conference on\n    Neural Engineering},\n  pages = {1038-1041},\n  date = {2019-05-20},\n  url = {https://ieeexplore.ieee.org/document/8717086},\n  doi = {10.1109/NER.2019.8717086},\n  isbn = {978-1-5386-7921-0},\n  langid = {en},\n  abstract = {This paper introduces the\n    {[}NAV-VIR{]}(/content/projects/NAV-VIR) system, a multimodal\n    virtual environment to assist visually impaired people in virtually\n    discovering and exploring unknown areas from the safety of their\n    home. The originality of NAV-VIR resides in (1) an optimized\n    representation of the surrounding topography, the spatial gist,\n    based on human spatial cognition models and the sensorimotor\n    supplementation framework, and (2) a multimodal orientation-aware\n    immersive virtual environment relying on two synergetic interfaces:\n    an interactive force feedback tablet, the F2T, and an immersive\n    HRTF-based 3D audio simulation relying on binaural recordings of\n    real environments. This paper presents NAV-VIR functionalities and\n    its preliminary evaluation through a simple shape and movement\n    perception task.}\n}\nFor attribution, please cite this work as:\nRivière, M.-A., Gay, S., Romeo, K., Pissaloux, E., Bujacz, M.,\nSkulimowski, P., & Strumillo, P. (2019). NAV-VIR: an audio-tactile\nvirtual environment to assist visually impaired people. Proceedings\nof the International IEEE/EMBS Conference on Neural Engineering,\n1038–1041. https://doi.org/10.1109/NER.2019.8717086"
  },
  {
    "objectID": "content/pubs/ICISP20/index.html",
    "href": "content/pubs/ICISP20/index.html",
    "title": "Towards the Tactile Discovery of Cultural Heritage with Multi-approach Segmentation",
    "section": "",
    "text": "Article (online)\n  \n  \n      Project (TETMOST)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@inproceedings{souradi2020,\n  author = {Souradi, Ali and Lecomte, Christele and Romeo, Katerine and\n    Gay, Simon and Rivière, Marc-Aurèle and El Moataz, Abderrahim and\n    Pissaloux, Edwige},\n  editor = {El Moataz, Abderrahim and Mammass, Driss and Mansouri,\n    Alamin and Nouboud, Fathallah},\n  publisher = {Springer International Publishing},\n  title = {Towards the {Tactile} {Discovery} of {Cultural} {Heritage}\n    with {Multi-approach} {Segmentation}},\n  booktitle = {Lecture Notes in Computer Science},\n  volume = {12119},\n  pages = {14-23},\n  date = {2020-07-08},\n  url = {http://link.springer.com/10.1007/978-3-030-51935-3_2},\n  doi = {10.1007/978-3-030-51935-3_2},\n  isbn = {978-3-030-51934-6 978-3-030-51935-3},\n  langid = {en},\n  abstract = {This paper presents a new way to access visual information\n    in museums through tactile exploration, and related techniques to\n    efficiently transform visual data into tactile objects.\n    Accessibility to cultural heritage and artworks for people with\n    visual impairments requires the segmentation of images and paintings\n    to extract and classify their contents into meaningful elements\n    which can then be presented through a tactile medium. In this paper,\n    we investigate the feasibility and how to optimize the tactile\n    discovery of an image. First, we study the emergence of image\n    comprehension through tactile discovery, using 3D-printed objects\n    extracted from paintings. Later, we present a dynamic Force Feedback\n    Tablet (F2T) used to convey the 2D shape and texture information of\n    objects through haptic feedback. We then explore several image\n    segmentation methods to automate the extraction of meaningful\n    objects from selected artworks, to be presented to visually impaired\n    people through the F2T. Finally, we evaluate how to best combine the\n    F2T’s haptic effects in order to convey the extracted objects and\n    features to the users, with the aim of facilitating the\n    comprehension of the represented objects and their affordances.}\n}\nFor attribution, please cite this work as:\nSouradi, A., Lecomte, C., Romeo, K., Gay, S., Rivière, M.-A., El Moataz,\nA., & Pissaloux, E. (2020). Towards the Tactile Discovery of\nCultural Heritage with Multi-approach Segmentation. In A. El Moataz, D.\nMammass, A. Mansouri, & F. Nouboud (Eds.), Lecture Notes in\nComputer Science (Vol. 12119, pp. 14–23). Springer International\nPublishing. https://doi.org/10.1007/978-3-030-51935-3_2"
  },
  {
    "objectID": "content/pubs/ICCHP18-TactiBelt/index.html",
    "href": "content/pubs/ICCHP18-TactiBelt/index.html",
    "title": "TactiBelt: integrating spatial cognition and mobility theories into the design of a novel orientation and mobility assistive device for the blind",
    "section": "",
    "text": "Article (online)\n  \n  \n      Project (ACCESSPACE)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@inproceedings{rivière2018,\n  author = {Rivière, Marc-Aurèle and Gay, Simon and Pissaloux, Edwige},\n  editor = {Klaus, Miesenberger and Georgios, Kouroupetroglou},\n  publisher = {Springer International Publishing},\n  title = {TactiBelt: Integrating Spatial Cognition and Mobility\n    Theories into the Design of a Novel Orientation and Mobility\n    Assistive Device for the Blind},\n  booktitle = {Lecture Notes in Computer Science},\n  volume = {10897},\n  pages = {110-113},\n  date = {2018-07-12},\n  url = {http://link.springer.com/10.1007/978-3-319-94274-2_16},\n  doi = {10.1007/978-3-319-94274-2_16},\n  isbn = {978-3-319-94273-5 978-3-319-94274-2},\n  langid = {en},\n  abstract = {The aim of this paper is to introduce a novel functional\n    design for an indoor and outdoor mobility assistive device for the\n    visually impaired, based on the theoretical frameworks of mobility\n    and spatial cognition. The originality of the proposed approach\n    comes from the integration of two main aspects of navigation,\n    locomotion and wayfinding. The cognitive theories which underpin the\n    design of the proposed sensory substitution device, called\n    TactiBelt, are identified and discussed in the framework of spatial\n    knowledge acquisition. The paper is organized as follows: section 1\n    gives a brief overview of the sensory substitution framework, while\n    sections 2 \\& 3 introduce the importance of navigation and spatial\n    cognition models for the design of mobility aids. Section 4 details\n    the functional design of the TactiBelt.}\n}\nFor attribution, please cite this work as:\nRivière, M.-A., Gay, S., & Pissaloux, E. (2018). TactiBelt:\nintegrating spatial cognition and mobility theories into the design of a\nnovel orientation and mobility assistive device for the blind. In M.\nKlaus & K. Georgios (Eds.), Lecture Notes in Computer\nScience (Vol. 10897, pp. 110–113). Springer International\nPublishing. https://doi.org/10.1007/978-3-319-94274-2_16"
  },
  {
    "objectID": "content/pubs/CRN23/index.html",
    "href": "content/pubs/CRN23/index.html",
    "title": "Apnea of Prematurity induces short and long-term development-related transcriptional changes in the murine cerebellum",
    "section": "",
    "text": "Article (online)\n  \n  \n      Project (DE-AoP)\n  \n  \n      Data & Analyses\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to topReuseCC BY-NC-ND 4.0CitationBibTeX citation:@article{rodriguez-duboc2023,\n  author = {Rodriguez-Duboc, Agalic and Basille-Duguay, Magali and\n    Debonne, Aurélien and Rivière, Marc-Aurèle and Vaudry, David and\n    Burel, Delphine},\n  title = {Apnea of {Prematurity} Induces Short and Long-Term\n    Development-Related Transcriptional Changes in the Murine\n    Cerebellum},\n  journal = {Current Research in Neurobiology},\n  volume = {5},\n  pages = {100113},\n  date = {2023-10-20},\n  url = {https://www.sciencedirect.com/science/article/pii/S2665945X23000414},\n  doi = {10.1016/j.crneur.2023.100113},\n  issn = {2665-945X},\n  langid = {en},\n  abstract = {Apnea of prematurity (AOP) occurs in over 50\\% of preterm\n    infants and induces a perinatal intermittent hypoxia (IH) which\n    represents a leading cause of morbimortality worldwide. At birth,\n    the human cerebellar cortex is still immature, making it vulnerable\n    to perinatal events. Moreover, a correlation between cerebellar\n    functions and the deficits observed in children having suffered from\n    AOP has been demonstrated. Yet, the cerebellar alterations\n    underpinning this link remain poorly understood. To shed light on\n    the involvement of the cerebellum in perinatal hypoxia-related\n    sequelae, we developed a mouse model of AOP. In previous works, we\n    found that IH induces oxidative stress in the developing cerebellum\n    as shown by the overexpression of genes involved in reactive oxygen\n    species production, and the under-expression of genes encoding\n    antioxidant enzymes. These alterations suggest a failure of the\n    defense system against oxidative stress and could be responsible for\n    neuronal death in the cerebellum. Based on these results, we\n    performed a transcriptomic study of the genes involved in the\n    processes that occur during cerebellar development. We analyzed the\n    expression of these genes at various developmental stages and in\n    different cell types, by real time PCR. This enabled us to pinpoint\n    a timeframe of vulnerability at P8, which represents the age with\n    the highest number of downregulated genes in the cerebellum.\n    Moreover, we identified several molecular pathways that are impacted\n    by our IH protocol, such as proliferation, migration, and\n    differentiation. This suggests that IH can modify the development of\n    various cells, and then contribute to the histological and\n    behavioral deficits already observed in this model. Overall, our\n    data indicate that the cerebellum is highly sensitive to IH, and\n    provide elements to better understand the pathophysiology of AOP by\n    deciphering its cellular and molecular causal mechanisms. In the\n    long term, the present results could lead to the identification of\n    novel therapeutic targets to improve the clinical management of this\n    highly prevalent pathology.}\n}\nFor attribution, please cite this work as:\nRodriguez-Duboc, A., Basille-Duguay, M., Debonne, A., Rivière, M.-A.,\nVaudry, D., & Burel, D. (2023). Apnea of Prematurity induces short\nand long-term development-related transcriptional changes in the murine\ncerebellum. Current Research in Neurobiology, 5,\n100113. https://doi.org/10.1016/j.crneur.2023.100113"
  },
  {
    "objectID": "content/projects/TETMOST/index.html",
    "href": "content/projects/TETMOST/index.html",
    "title": "TETMOST",
    "section": "",
    "text": "Article (ICCHP’18)\n  \n  \n    \n     Article (ICISP’20)\n  \n  \n    \n     Poster"
  },
  {
    "objectID": "content/projects/TETMOST/index.html#exploring-existing-haptic-interfaces",
    "href": "content/projects/TETMOST/index.html#exploring-existing-haptic-interfaces",
    "title": "TETMOST",
    "section": "1.1 Exploring existing haptic interfaces",
    "text": "1.1 Exploring existing haptic interfaces\nWe researched and tried different categories of haptic interfaces in order to asses their strengths and weaknesses for our purposes:\n\n\n\n\n\n\n\n\n\n\n\n(a) Taxel mechanical interfaces\n\n\n\n\n\n\n\n\n\n\n\n(b) Electro-friction interfaces\n\n\n\n\n\n\n\n\n\n\n\n(c) Vibrational interfaces\n\n\n\n\n\n\n\nFigure 1: The three main categories of haptic interfaces\n\n\n\nOur experience with the existing categories of haptic interfaces allowed us to designed one best adapted to our need: the Force-Feedback Tablet (F2T)"
  },
  {
    "objectID": "content/projects/TETMOST/index.html#our-interface-f2t-v1",
    "href": "content/projects/TETMOST/index.html#our-interface-f2t-v1",
    "title": "TETMOST",
    "section": "2.1 Our interface: F2T (v1)",
    "text": "2.1 Our interface: F2T (v1)\nThe first prototype of the F2T was assembled with legos and a camera to better asses the position of the joystick within the frame of the device."
  },
  {
    "objectID": "content/projects/TETMOST/index.html#software-tools",
    "href": "content/projects/TETMOST/index.html#software-tools",
    "title": "TETMOST",
    "section": "2.2 Software tools",
    "text": "2.2 Software tools\n1) We developed a Java application to create or convert images into simplified tactile representations, which can then be explored using the F2T:\n\n2) In order to display an image haptically, we first needed a way to simplify the image’s content without losing its meaning. To do so, we explored various Computer Vision techniques such as image segmentation and edge detection:"
  },
  {
    "objectID": "content/projects/NAV-VIR/index.html",
    "href": "content/projects/NAV-VIR/index.html",
    "title": "NAV-VIR",
    "section": "",
    "text": "Article\n  \n  \n    \n     Poster"
  },
  {
    "objectID": "content/projects/NAV-VIR/index.html#our-interface-f2t-v2",
    "href": "content/projects/NAV-VIR/index.html#our-interface-f2t-v2",
    "title": "NAV-VIR",
    "section": "1.1 Our interface: F2T (v2)",
    "text": "1.1 Our interface: F2T (v2)\nDuring this project, we improved upon the first iteration of the Force Feedback Tablet (F2T) from the TETMOST project to design the finalized prototype of this interface:"
  },
  {
    "objectID": "content/projects/DE-AoP/index.html",
    "href": "content/projects/DE-AoP/index.html",
    "title": "DE-AoP",
    "section": "",
    "text": "Article (in press)\n  \n  \n      Data & Analyses\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n1 My role in this project\n\n1) Developed tools to assist the project’s researchers in exploring their data. Among those tools, I coded and hosted a modular Shiny dashboard to assist in the data exploration process.\n2) Handled the RT-qPCR & IHC data processing and analysis, as well as documenting & open-sourcing the resulting code.\n3) Participated in writing the journal article summarizing the results of this project.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/projects/AdViS/index.html",
    "href": "content/projects/AdViS/index.html",
    "title": "AdViS",
    "section": "",
    "text": "Official homepage []\n  \n  \n      Article (2015)\n  \n  \n      Article (2018)\n  \n  \n    \n     Poster\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n1 Introduction\n\nAdViS aims to explore various ways to provide visuo-spatial information through auditory feedback, based on the Sensory Substitution framework. Since its inception, the project investigated visuo-auditive substitution possibilities for multiple tasks in which vision plays a crucial role:\n\nNavigating a small maze using a depth-map-to-auditory-cues transcoding\n\nFinger-guided image exploration (on a touch screen)\n\nEye-movement guided image exploration (on a turned off computer screen)\n\nPointing towards and reaching a virtual target in 3D space using a motion capture environment\n\n\n\n\n\n\n\n\n\nAdViS - Depth Map navigation\n\n\n\n\n\nAdViS - Motion Capture\n\n\n\n\nThe AdViS system is coded in C++, uses PureData for complex sound generation, and relies on the VICON system to track participant’s movements in an augmented reality environment.\n\n\n2 My role in this project\n\n1) Proposed a new model for image exploration relying on a touch-mediated visuo-auditive feedback loop, where a VIP explores an image by moving its finger across a screen and gets audio feedback based on the contents of the explored region.\n2) Modified the existing AdViS system to include the ability to transcode grey-scale images into soundscapes, based on captured finger-motion information on a touchscreen.\n3) Organized experimental evaluations with blindfolded students, tasked with recognizing geometrical shapes on a touchscreen, and analyzed the results.\n\n4) Participated in implementing an occular-motion-to-audio-feedback loop in order to evaluate the possibility of exploring images (on a turned off screen) with eye-movements (which are still controllable by most of the non-congenital VIP).\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/code/projects/LT-AoP/index.html",
    "href": "content/code/projects/LT-AoP/index.html",
    "title": "LT-AoP: Data & Analyses",
    "section": "",
    "text": "Back to topReferences\n\nBrooks, M. E., Kristensen, K., van Benthem, K. J., Magnusson, A., Berg, C. W., Nielsen, A., Skaug, H. J., Maechler, M., & Bolker, B. M. (2017). glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling. The R Journal, 9(2), 378–400. https://journal.r-project.org/archive/2017/RJ-2017-066/index.html\n\n\nHartig, F. (2022). DHARMa: Residual diagnostics for hierarchical (multi-level / mixed) regression models. https://CRAN.R-project.org/package=DHARMa\n\n\nLenth, R. V. (2022). Emmeans: Estimated marginal means, aka least-squares means. https://CRAN.R-project.org/package=emmeans\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#stan-setup",
    "href": "content/code/posts/big-bayes/index.html#stan-setup",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "Stan setup",
    "text": "Stan setup\n\nInstalling CmdStancmdstanr::check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)\n\ncpp_opts &lt;- list(\n  stan_threads = TRUE\n  , STAN_CPP_OPTIMS = TRUE\n  , STAN_NO_RANGE_CHECKS = TRUE # WARN: remove this if you haven't tested the model\n  , PRECOMPILED_HEADERS = TRUE\n  # , CXXFLAGS_OPTIM = \"-march=native -mtune=native\"\n  , CXXFLAGS_OPTIM_TBB = \"-mtune=native -march=native\"\n  , CXXFLAGS_OPTIM_SUNDIALS = \"-mtune=native -march=native\"\n)\n\ncmdstanr::install_cmdstan(cpp_options = cpp_opts, quiet = TRUE)\n\n\n\nLoading CmdStan (if already installed)highest_cmdstan_version &lt;- dir_ls(config$cmdstan_path) |&gt; \n  path_file() |&gt; \n  keep(\\(e) str_detect(e, \"cmdstan-\")) |&gt; \n  bind(x, str_split(x, '-', simplify = TRUE)[,2]) |&gt; \n  reduce(\\(x, y) ifelse(utils::compareVersion(x, y) == 1, x, y))\n\nset_cmdstan_path(glue::glue(\"{config$cmdstan_path}cmdstan-{highest_cmdstan_version}\"))\n\n\n\nSetting up knitr’s engine for CmdStan## Inspired by: https://mpopov.com/blog/2020/07/30/replacing-the-knitr-engine-for-stan/\n\n## Note: We could haved use cmdstanr::register_knitr_engine(), \n##       but it wouldn't include compiler optimizations & multi-threading by default\n\nknitr::knit_engines$set(\n  cmdstan = function(options) {\n    output_var &lt;- options$output.var\n    if (!is.character(output_var) || length(output_var) != 1L) {\n      stop(\n        \"The chunk option output.var must be a character string \",\n        \"providing a name for the returned `CmdStanModel` object.\"\n      )\n    }\n    if (options$eval) {\n      if (options$cache) {\n        cache_path &lt;- options$cache.path\n        if (length(cache_path) == 0L || is.na(cache_path) || cache_path == \"NA\") {\n          cache_path &lt;- \"\"\n        }\n        dir &lt;- paste0(cache_path, options$label)\n      } else {\n        dir &lt;- tempdir()\n      }\n      file &lt;- cmdstanr::write_stan_file(options$code, dir = dir, force_overwrite = TRUE)\n      mod &lt;- cmdstanr::cmdstan_model(\n        stan_file = file, \n        cpp_options = list(\n          stan_threads = TRUE\n          , STAN_CPP_OPTIMS = TRUE\n          , STAN_NO_RANGE_CHECKS = TRUE # The model was already tested\n          , PRECOMPILED_HEADERS = TRUE\n          # , CXXFLAGS_OPTIM = \"-march=native -mtune=native\"\n          , CXXFLAGS_OPTIM_TBB = \"-mtune=native -march=native\"\n          , CXXFLAGS_OPTIM_SUNDIALS = \"-mtune=native -march=native\"\n        ),\n        stanc_options = list(\"O1\"),\n        force_recompile = TRUE\n      )\n      assign(output_var, mod, envir = knitr::knit_global())\n    }\n    options$engine &lt;- \"stan\"\n    code &lt;- paste(options$code, collapse = \"\\n\")\n    knitr::engine_output(options, code, '')\n  }\n)\n\n\n\n\n\n\n\n\n\n💻 Expand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting        value\n version        R version 4.3.1 (2023-06-16)\n os             Ubuntu 22.04.3 LTS\n system         x86_64, linux-gnu\n ui             X11\n language       (EN)\n collate        C.UTF-8\n ctype          C.UTF-8\n tz             Europe/Paris\n date           2023-11-28\n pandoc         3.1.9\n Quarto         1.4.513\n Stan (CmdStan) 2.33.1\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package    * version date (UTC) lib source\n P archive    * 1.1.6   2023-09-18 [?] CRAN (R 4.3.1)\n P bayesplot  * 1.10.0  2022-11-16 [?] CRAN (R 4.3.0)\n P cmdstanr   * 0.6.1   2023-09-13 [?] local\n P crayon     * 1.5.2   2022-09-29 [?] CRAN (R 4.3.0)\n P data.table * 1.14.9  2023-05-05 [?] Github (Rdatatable/data.table@8803918)\n P dplyr      * 1.1.3   2023-09-03 [?] RSPM (R 4.3.0)\n P fs         * 1.6.3   2023-07-20 [?] CRAN (R 4.3.0)\n P ggplot2    * 3.4.4   2023-10-12 [?] CRAN (R 4.3.1)\n P gt         * 0.10.0  2023-10-07 [?] CRAN (R 4.3.1)\n P here       * 1.0.1   2020-12-13 [?] CRAN (R 4.3.0)\n P htmltools  * 0.5.6.1 2023-10-06 [?] CRAN (R 4.3.1)\n P knitr      * 1.44    2023-09-11 [?] CRAN (R 4.3.0)\n P lubridate  * 1.9.3   2023-09-27 [?] CRAN (R 4.3.1)\n P patchwork  * 1.1.3   2023-08-14 [?] CRAN (R 4.3.0)\n P pipebind   * 0.1.2   2023-08-30 [?] CRAN (R 4.3.0)\n P posterior  * 1.4.1   2023-03-14 [?] CRAN (R 4.3.0)\n P purrr      * 1.0.2   2023-08-10 [?] CRAN (R 4.3.0)\n P reactable  * 0.4.4   2023-03-12 [?] CRAN (R 4.3.0)\n P readr      * 2.1.4   2023-02-10 [?] CRAN (R 4.3.0)\n P stringr    * 1.5.0   2022-12-02 [?] CRAN (R 4.3.0)\n P tibble     * 3.2.1   2023-03-20 [?] CRAN (R 4.3.0)\n P tidyr      * 1.3.0   2023-01-24 [?] CRAN (R 4.3.0)\n\n [1] /home/mar/Dev/Projects/R/ma-riviere.com/renv/library/R-4.3/x86_64-pc-linux-gnu\n [2] /home/mar/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#matches-data",
    "href": "content/code/posts/big-bayes/index.html#matches-data",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n1.1 Matches data",
    "text": "1.1 Matches data\n  Download the matches’ data \nLoading the matches’ data:\n\nmatches_data_raw &lt;- (\n  read_csv(dir_ls(matches_data_path, regexp = \"atp_matches_(.*).csv\"), show_col_types = FALSE)\n  |&gt; select(tourney_date, tourney_level, round, winner_id, winner_name, loser_id, loser_name, score)\n  |&gt; mutate(tourney_date = lubridate::ymd(tourney_date))\n)\n\nFiltering and cleaning the combined matches’ data (based on the original post’s data processing):\n\nround_numbers = list(\n  \"R128\" = 1,\n  \"RR\" = 1,\n  \"R64\" = 2,\n  \"R32\" = 3,\n  \"R16\" = 4,\n  \"QF\" = 5,\n  \"SF\" = 6,\n  \"F\" = 7\n)\n\n(matches_data_clean &lt;- matches_data_raw \n  |&gt; filter(\n    tourney_date %between% c(\"1968-01-01\", \"2021-06-20\"),\n    str_detect(score, \"RET|W/O|DEF|nbsp|Def.\", negate = TRUE),\n    str_length(score) &gt; 4,\n    tourney_level != \"D\",\n    round %in% names(round_numbers)\n  )\n  |&gt; mutate(\n    round_number = recode(round, !!!round_numbers),\n    label = 1\n  )\n  |&gt; arrange(tourney_date, round_number)\n  |&gt; select(-round, -tourney_level)\n)\ndata.frame [160,399 x 8]\n\n\n\n\ntourney_date\n\n\nwinner_id\n\n\nwinner_name\n\n\nloser_id\n\n\nloser_name\n\n\nscore\n\n\nround_number\n\n\nlabel\n\n\n\n\n\n1968-01-19\n\n\n110 023\n\n\nRichard Coulthard\n\n\n107 760\n\n\nMax Senior\n\n\n12-10 7-5 4-6 7-5\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n109 803\n\n\nJohn Brown\n\n\n106 964\n\n\nErnie Mccabe\n\n\n6-3 6-2 6-4\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 257\n\n\nRoss Case\n\n\n110 024\n\n\nGondo Widjojo\n\n\n6-4 3-6 6-3 7-5\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 105\n\n\nAllan Stone\n\n\n110 025\n\n\nRobert Layton\n\n\n6-4 6-2 6-1\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n109 966\n\n\nWarren Jacques\n\n\n110 026\n\n\nBert Kearney\n\n\n6-4 6-1 7-5\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n107 759\n\n\nMax Pettman\n\n\n110 027\n\n\nTakesji Tsujimoto\n\n\n6-4 6-1 6-2\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 101\n\n\nMike Belkin\n\n\n110 028\n\n\nM Marchment\n\n\n6-2 3-6 6-4 9-7\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 025\n\n\nBarry Phillips Moore\n\n\n108 430\n\n\nTony Dawson\n\n\n6-3 6-0 6-3\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n108 519\n\n\nWilliam Coghlan\n\n\n110 029\n\n\nPeter Oatey\n\n\n6-0 6-2 9-11 6-3\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n109 799\n\n\nGeoff Pollard\n\n\n110 030\n\n\nChristian Janssens\n\n\n6-4 6-2 6-4\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 146\n\n\nJun Kamiwazumi\n\n\n110 031\n\n\nBrian Connor\n\n\n8-6 6-4 6-2\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 180\n\n\nBill Lloyd\n\n\n110 032\n\n\nH Nielson\n\n\n6-2 7-5 6-4\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 013\n\n\nNeale Fraser\n\n\n110 033\n\n\nR Harvey\n\n\n6-1 6-1 6-0\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n110 034\n\n\nMerv Guse\n\n\n110 035\n\n\nJ May\n\n\n6-1 6-2 6-2\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 174\n\n\nManuel Orantes\n\n\n110 036\n\n\nL Weatherhog\n\n\n6-4 6-0 6-2\n\n\n2\n\n\n1\n\n\n\n\n\n[ omitted 160,384 entries ]"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#player-data",
    "href": "content/code/posts/big-bayes/index.html#player-data",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n1.2 Player data",
    "text": "1.2 Player data\n  Download the players’ data \nLoading the raw player data:\n\nplayer_data_path &lt;- here(\"res\", \"data\", \"tennis\", \"atp_players.csv\")\n\n(player_data_raw &lt;- read_csv(player_data_path, show_col_types = FALSE) \n |&gt; mutate(player_name = str_c(name_first, name_last, sep = \" \"))\n |&gt; select(player_id, player_name)\n)\ndata.frame [55,649 x 2]\n\n\n\n\nplayer_id\n\n\nplayer_name\n\n\n\n\n\n100 001\n\n\nGardnar Mulloy\n\n\n\n\n100 002\n\n\nPancho Segura\n\n\n\n\n100 003\n\n\nFrank Sedgman\n\n\n\n\n100 004\n\n\nGiuseppe Merlo\n\n\n\n\n100 005\n\n\nRichard Gonzalez\n\n\n\n\n100 006\n\n\nGrant Golden\n\n\n\n\n100 007\n\n\nAbe Segal\n\n\n\n\n100 008\n\n\nKurt Nielsen\n\n\n\n\n100 009\n\n\nIstvan Gulyas\n\n\n\n\n100 010\n\n\nLuis Ayala\n\n\n\n\n100 011\n\n\nTorben Ulrich\n\n\n\n\n100 012\n\n\nNicola Pietrangeli\n\n\n\n\n100 013\n\n\nNeale Fraser\n\n\n\n\n100 014\n\n\nTrevor Fancutt\n\n\n\n\n100 015\n\n\nSammy Giammalva\n\n\n\n\n\n[ omitted 55,634 entries ]\n\n\n\n\n\n\nFiltering player_data to only keep the players actually present in our data, and updating their IDs:\n\n(player_data &lt;- with(matches_data_clean, tibble(player_id = union(winner_id, loser_id)))\n |&gt; arrange(player_id)\n |&gt; summarize(player_idx = cur_group_id(), .by = player_id)\n |&gt; left_join(player_data_raw, join_by(player_id))\n)\ndata.frame [4,830 x 3]\n\n\n\n\nplayer_id\n\n\nplayer_idx\n\n\nplayer_name\n\n\n\n\n\n100 001\n\n\n1\n\n\nGardnar Mulloy\n\n\n\n\n100 002\n\n\n2\n\n\nPancho Segura\n\n\n\n\n100 003\n\n\n3\n\n\nFrank Sedgman\n\n\n\n\n100 004\n\n\n4\n\n\nGiuseppe Merlo\n\n\n\n\n100 005\n\n\n5\n\n\nRichard Gonzalez\n\n\n\n\n100 006\n\n\n6\n\n\nGrant Golden\n\n\n\n\n100 007\n\n\n7\n\n\nAbe Segal\n\n\n\n\n100 009\n\n\n8\n\n\nIstvan Gulyas\n\n\n\n\n100 010\n\n\n9\n\n\nLuis Ayala\n\n\n\n\n100 011\n\n\n10\n\n\nTorben Ulrich\n\n\n\n\n100 012\n\n\n11\n\n\nNicola Pietrangeli\n\n\n\n\n100 013\n\n\n12\n\n\nNeale Fraser\n\n\n\n\n100 014\n\n\n13\n\n\nTrevor Fancutt\n\n\n\n\n100 015\n\n\n14\n\n\nSammy Giammalva\n\n\n\n\n100 016\n\n\n15\n\n\nKen Rosewall\n\n\n\n\n\n[ omitted 4,815 entries ]"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#matches-player-data",
    "href": "content/code/posts/big-bayes/index.html#matches-player-data",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n1.3 Matches + Player data",
    "text": "1.3 Matches + Player data\nAllocating the new player IDs (player_idx) to the winner_id and loser_id from matches_data:\n\n(matches_data &lt;- left_join(matches_data_clean, player_data, by = c(\"winner_id\" = \"player_id\")) \n |&gt; rename(winner_idx = player_idx) \n |&gt; relocate(winner_idx, .after = winner_id) \n |&gt; left_join(player_data, by = c(\"loser_id\" = \"player_id\")) \n |&gt; rename(loser_idx = player_idx) \n |&gt; relocate(loser_idx, .after = loser_id)\n |&gt; drop_na(winner_idx, loser_idx)\n |&gt; select(-matches(\"player_name\"))\n)\ndata.frame [160,399 x 10]\n\n\n\n\ntourney_date\n\n\nwinner_id\n\n\nwinner_idx\n\n\nwinner_name\n\n\nloser_id\n\n\nloser_idx\n\n\nloser_name\n\n\nscore\n\n\nround_number\n\n\nlabel\n\n\n\n\n\n1968-01-19\n\n\n110 023\n\n\n3 655\n\n\nRichard Coulthard\n\n\n107 760\n\n\n3 129\n\n\nMax Senior\n\n\n12-10 7-5 4-6 7-5\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n109 803\n\n\n3 440\n\n\nJohn Brown\n\n\n106 964\n\n\n2 909\n\n\nErnie Mccabe\n\n\n6-3 6-2 6-4\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 257\n\n\n253\n\n\nRoss Case\n\n\n110 024\n\n\n3 656\n\n\nGondo Widjojo\n\n\n6-4 3-6 6-3 7-5\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 105\n\n\n103\n\n\nAllan Stone\n\n\n110 025\n\n\n3 657\n\n\nRobert Layton\n\n\n6-4 6-2 6-1\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n109 966\n\n\n3 600\n\n\nWarren Jacques\n\n\n110 026\n\n\n3 658\n\n\nBert Kearney\n\n\n6-4 6-1 7-5\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n107 759\n\n\n3 128\n\n\nMax Pettman\n\n\n110 027\n\n\n3 659\n\n\nTakesji Tsujimoto\n\n\n6-4 6-1 6-2\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 101\n\n\n99\n\n\nMike Belkin\n\n\n110 028\n\n\n3 660\n\n\nM Marchment\n\n\n6-2 3-6 6-4 9-7\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 025\n\n\n24\n\n\nBarry Phillips Moore\n\n\n108 430\n\n\n3 325\n\n\nTony Dawson\n\n\n6-3 6-0 6-3\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n108 519\n\n\n3 349\n\n\nWilliam Coghlan\n\n\n110 029\n\n\n3 661\n\n\nPeter Oatey\n\n\n6-0 6-2 9-11 6-3\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n109 799\n\n\n3 436\n\n\nGeoff Pollard\n\n\n110 030\n\n\n3 662\n\n\nChristian Janssens\n\n\n6-4 6-2 6-4\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 146\n\n\n144\n\n\nJun Kamiwazumi\n\n\n110 031\n\n\n3 663\n\n\nBrian Connor\n\n\n8-6 6-4 6-2\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 180\n\n\n178\n\n\nBill Lloyd\n\n\n110 032\n\n\n3 664\n\n\nH Nielson\n\n\n6-2 7-5 6-4\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 013\n\n\n12\n\n\nNeale Fraser\n\n\n110 033\n\n\n3 665\n\n\nR Harvey\n\n\n6-1 6-1 6-0\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n110 034\n\n\n3 666\n\n\nMerv Guse\n\n\n110 035\n\n\n3 667\n\n\nJ May\n\n\n6-1 6-2 6-2\n\n\n2\n\n\n1\n\n\n\n\n1968-01-19\n\n\n100 174\n\n\n172\n\n\nManuel Orantes\n\n\n110 036\n\n\n3 668\n\n\nL Weatherhog\n\n\n6-4 6-0 6-2\n\n\n2\n\n\n1\n\n\n\n\n\n[ omitted 160,384 entries ]"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#stan-code",
    "href": "content/code/posts/big-bayes/index.html#stan-code",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n2.1 Stan code",
    "text": "2.1 Stan code\n\n\n\n\n\n\nUpdated Stan code with within-chain parallelization\n\n\n\n\n\n\ntennis_model\n\nfunctions {\n  array[] int sequence(int start, int end) {\n    array[end - start + 1] int seq;\n    for (n in 1 : num_elements(seq)) {\n      seq[n] = n + start - 1;\n    }\n    return seq;\n  }\n\n  // Compute partial sums of the log-likelihood\n  real partial_log_lik_lpmf(array[] int seq, int start, int end,\n                            data array[] int labels, \n                            data array[] int winner_ids, \n                            data array[] int loser_ids, \n                            vector player_skills) {\n    real ptarget = 0;\n    int N = end - start + 1;\n\n    vector[N] mu = rep_vector(0.0, N);\n    for (n in 1 : N) {\n      int nn = n + start - 1;\n      mu[n] += player_skills[winner_ids[nn]] - player_skills[loser_ids[nn]];\n    }\n    ptarget += bernoulli_logit_lpmf(labels[start : end] | mu);\n    return ptarget;\n  }\n}\ndata {\n    int n_players;\n    int n_matches;\n    \n    array[n_matches] int&lt;lower=1, upper=n_players&gt; winner_ids; // Winner of game n\n    array[n_matches] int&lt;lower=1, upper=n_players&gt; loser_ids;  // Loser of game n\n    array[n_matches] int&lt;lower=0, upper=1&gt; labels;             // Always 1 in this model\n    \n    int grainsize;\n}\ntransformed data {\n    array[n_matches] int seq = sequence(1, n_matches);\n}\nparameters {\n    real&lt;lower=0&gt; player_sd;          // Scale of ability variation (hierarchical prior)\n    vector[n_players] player_skills;  // Ability of player k\n}\nmodel {   \n  player_sd ~ std_normal();\n  player_skills ~ normal(0, player_sd);\n    \n  target += reduce_sum(\n    partial_log_lik_lpmf, seq, grainsize, \n    labels, winner_ids, loser_ids, player_skills\n  );\n}"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#stan-data",
    "href": "content/code/posts/big-bayes/index.html#stan-data",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n2.2 Stan data",
    "text": "2.2 Stan data\n\ntennis_stan_data &lt;- list(\n  n_matches = nrow(matches_data),\n  n_players = with(matches_data, length(union(winner_id, loser_id))),\n  winner_ids = matches_data$winner_idx,\n  loser_ids = matches_data$loser_idx,\n  labels = matches_data$label,\n  grainsize = max(100L, round(nrow(matches_data) / 60))\n)"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#model-fit",
    "href": "content/code/posts/big-bayes/index.html#model-fit",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n2.3 Model fit",
    "text": "2.3 Model fit\n\ntennis_mod_fit &lt;- tennis_model$sample(\n  data = tennis_stan_data, seed = 256,\n  iter_warmup = 1000, iter_sampling = 1000, refresh = 0,\n  chains = 4, parallel_chains = 4, threads_per_chain = 7\n)\n\n\n\n\n\n\n\nNote\n\n\n\nSampling takes ~2.69 minutes on my CPU (Ryzen 5950X, 16 Cores/32 Threads), on WSL2 (Ubuntu 22)\n\ndata.table [4 x 2]\n\n\n\n\nChain\n\n\nTime\n\n\n\n\n\n1\n\n\n162.915s (~2.72 minutes)\n\n\n\n\n2\n\n\n162.299s (~2.7 minutes)\n\n\n\n\n3\n\n\n162.62s (~2.71 minutes)\n\n\n\n\n4\n\n\n157.238s (~2.62 minutes)"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#posterior-data",
    "href": "content/code/posts/big-bayes/index.html#posterior-data",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n4.1 Posterior data",
    "text": "4.1 Posterior data\nGetting our Posterior Predictions into long format and joining the result with player_data:\n\n(player_skills &lt;- tennis_mod_fit$draws(variables = \"player_skills\")\n  |&gt; bind(x, subset_draws(x, \"player_skills\", regex = TRUE, draw = sample.int(ndraws(x), size = 500)))\n  |&gt; as.data.table()\n  |&gt; _[, .(player_skills = list(value)), by = variable\n  ][, let(player_idx = as.integer(str_extract(variable, \"\\\\d{1,4}\")), variable = NULL)\n  ][, let(skill_mean = sapply(player_skills, mean), skill_sd = sapply(player_skills, sd))\n  ][as.data.table(player_data), on = \"player_idx\", nomatch = NULL\n  ][order(-skill_mean), .(player_name, player_id, player_idx, skill_mean, skill_sd, player_skills)]\n)\ndata.table [4,830 x 6]\n\n\n\n\nplayer_name\n\n\nplayer_id\n\n\nplayer_idx\n\n\nskill_mean\n\n\nskill_sd\n\n\nplayer_skills\n\n\n\n\n\nNovak Djokovic\n\n\n104 925\n\n\n2 415\n\n\n3.524\n\n\n0.092\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nRafael Nadal\n\n\n104 745\n\n\n2 367\n\n\n3.422\n\n\n0.094\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nRoger Federer\n\n\n103 819\n\n\n2 082\n\n\n3.313\n\n\n0.079\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nBjorn Borg\n\n\n100 437\n\n\n423\n\n\n3.247\n\n\n0.102\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nIvan Lendl\n\n\n100 656\n\n\n613\n\n\n3.23\n\n\n0.084\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nJohn McEnroe\n\n\n100 581\n\n\n553\n\n\n3.187\n\n\n0.09\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nJimmy Connors\n\n\n100 284\n\n\n280\n\n\n3.163\n\n\n0.079\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nRod Laver\n\n\n100 029\n\n\n28\n\n\n3.021\n\n\n0.119\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nPete Sampras\n\n\n101 948\n\n\n1 408\n\n\n2.922\n\n\n0.091\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nAndy Murray\n\n\n104 918\n\n\n2 413\n\n\n2.922\n\n\n0.092\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nBoris Becker\n\n\n101 414\n\n\n1 127\n\n\n2.827\n\n\n0.088\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nAndre Agassi\n\n\n101 736\n\n\n1 310\n\n\n2.776\n\n\n0.081\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nStefan Edberg\n\n\n101 222\n\n\n993\n\n\n2.73\n\n\n0.083\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nAndy Roddick\n\n\n104 053\n\n\n2 157\n\n\n2.704\n\n\n0.09\n\n\n&lt;numeric [500]&gt;\n\n\n\n\nJuan Martin del Potro\n\n\n105 223\n\n\n2 503\n\n\n2.679\n\n\n0.108\n\n\n&lt;numeric [500]&gt;\n\n\n\n\n\n[ omitted 4,815 entries ]"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#posterior-plots",
    "href": "content/code/posts/big-bayes/index.html#posterior-plots",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "\n4.2 Posterior plots",
    "text": "4.2 Posterior plots\n\nridgeline_plotridgeline_plot &lt;- function(dat, var) {\n  dat &lt;- dat[, .(player_skills = unlist(player_skills)), by = setdiff(names(dat), 'player_skills')\n     ][, player_name := factor(player_name, levels = rev(unique(player_name)))]\n  \n  return(\n    ggplot(dat, aes(player_skills, y = {{ var }}, fill = {{ var }}))\n    + geom_ribbon(\n      stat = \"density\", outline.type = \"upper\", color = \"grey30\",\n      aes(\n        fill = stage({{ var }}, after_scale = alpha(fill, 0.5)),\n        ymin = after_stat(group),\n        ymax = after_stat(group + ndensity * 1.6)\n      )\n    )\n    * ggblend::blend(\"multiply\")\n    + labs(x = \"Player Skills\", y = \"\")\n    + scale_y_discrete(position = \"right\", labels = \\(x) str_replace_all(x, \"\\\\s\", \"\\n\"))\n    + theme(legend.position = \"none\", axis.line.y = element_blank())\n  )\n}\n\n\nPlotting the player_skills posteriors of the top 10 players:\n\nhead(player_skills, 10) |&gt; ridgeline_plot(player_name)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💻 Expand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting        value\n version        R version 4.3.1 (2023-06-16)\n os             Ubuntu 22.04.3 LTS\n system         x86_64, linux-gnu\n ui             X11\n language       (EN)\n collate        C.UTF-8\n ctype          C.UTF-8\n tz             Europe/Paris\n date           2024-02-05\n pandoc         3.1.11\n Quarto         1.4.533\n Stan (CmdStan) 2.33.1\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package    * version date (UTC) lib source\n P archive    * 1.1.6   2023-09-18 [?] CRAN (R 4.3.1)\n P bayesplot  * 1.10.0  2022-11-16 [?] CRAN (R 4.3.0)\n P cmdstanr   * 0.6.1   2023-09-13 [?] local\n P crayon     * 1.5.2   2022-09-29 [?] CRAN (R 4.3.0)\n P data.table * 1.14.9  2023-05-05 [?] Github (Rdatatable/data.table@8803918)\n P dplyr      * 1.1.3   2023-09-03 [?] RSPM (R 4.3.0)\n P fs         * 1.6.3   2023-07-20 [?] CRAN (R 4.3.0)\n P ggplot2    * 3.4.4   2023-10-12 [?] CRAN (R 4.3.1)\n P gt         * 0.10.0  2023-10-07 [?] CRAN (R 4.3.1)\n P here       * 1.0.1   2020-12-13 [?] CRAN (R 4.3.0)\n P htmltools  * 0.5.6.1 2023-10-06 [?] CRAN (R 4.3.1)\n P knitr      * 1.44    2023-09-11 [?] CRAN (R 4.3.0)\n P lubridate  * 1.9.3   2023-09-27 [?] CRAN (R 4.3.1)\n P patchwork  * 1.1.3   2023-08-14 [?] CRAN (R 4.3.0)\n P pipebind   * 0.1.2   2023-08-30 [?] CRAN (R 4.3.0)\n P posterior  * 1.4.1   2023-03-14 [?] CRAN (R 4.3.0)\n P purrr      * 1.0.2   2023-08-10 [?] CRAN (R 4.3.0)\n P reactable  * 0.4.4   2023-03-12 [?] CRAN (R 4.3.0)\n P readr      * 2.1.4   2023-02-10 [?] CRAN (R 4.3.0)\n P stringr    * 1.5.0   2022-12-02 [?] CRAN (R 4.3.0)\n P tibble     * 3.2.1   2023-03-20 [?] CRAN (R 4.3.0)\n P tidyr      * 1.3.0   2023-01-24 [?] CRAN (R 4.3.0)\n\n [1] /home/mar/Dev/Projects/R/ma-riviere.com/renv/library/R-4.3/x86_64-pc-linux-gnu\n [2] /home/mar/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "\n About Me",
    "section": "",
    "text": "In a nutshell:\n\n\n\n◈ Cognitive Neuroscience Researcher turned Data Scientist.\n◈ Identify issues, conceptualize theories, engineer solutions, design experiments, wrangle data, and extract insights.\n◈ Proficient R programmer, decent software developer."
  },
  {
    "objectID": "content/about.html#work-experience",
    "href": "content/about.html#work-experience",
    "title": "\n About Me",
    "section": "\n Work Experience",
    "text": "Work Experience\n\nData Science Consultant (2020-Present)Self-employed\n♦ Creating an R app combining LLMs and web scraping to automate lead acquisition and management for a leasing company\n♦ Statistical modeling of bioinformatics data with R, automatic generation of reports (Quarto) and Dashboards (Shiny)\nResearch Engineer (2021-2023)LITIS Lab\n♦ Designed & developed an AR platform in Unity (C#), and wearable haptic interfaces for Visually Impaired People (Java/Arduino)\n♦ Developed and tested camera-based solutions for indoor localization (Python/C++)\nGraduate Teaching Fellow (2017-2020)University of Rouen-Normandy\n♦ Gave university classes on Web Development (JS), Image Processing (Python), GUI design (Java), and ML/DL for Computer Vision"
  },
  {
    "objectID": "content/about.html#education",
    "href": "content/about.html#education",
    "title": "\n About Me",
    "section": "\n Education",
    "text": "Education\n\nPhD in Cognitive Neurosciences (2017-2020)Unfinished - COVIDUniversity of Rouen-Normandy\nMSc in Cognitive Neurosciences (2015-2016)Grenoble INP - Phelma\nMSc in Organisational Psychology (2013-2015)University of Strasbourg"
  },
  {
    "objectID": "content/about.html#contact-me",
    "href": "content/about.html#contact-me",
    "title": "\n About Me",
    "section": "\n Contact Me",
    "text": "Contact Me\nYou can send me an email, or directly message me on twitter."
  },
  {
    "objectID": "content/about.html#about-this-site",
    "href": "content/about.html#about-this-site",
    "title": "\n About Me",
    "section": "\n About this site",
    "text": "About this site\nThis website was made with Quarto and R.\n\n\n\n\n\n\nExpand for Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C.UTF-8\n ctype    C.UTF-8\n tz       Europe/Paris\n date     2023-11-28\n pandoc   3.1.9\n Quarto   1.4.513\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version  date (UTC) lib source\n P archive     * 1.1.6    2023-09-18 [?] CRAN (R 4.3.1)\n P arrow       * 13.0.0.1 2023-09-22 [?] CRAN (R 4.3.1)\n P bayesplot   * 1.10.0   2022-11-16 [?] CRAN (R 4.3.0)\n P brms        * 2.20.4   2023-09-25 [?] CRAN (R 4.3.1)\n P broom       * 1.0.5    2023-06-09 [?] CRAN (R 4.3.0)\n P cmdstanr    * 0.6.1    2023-09-13 [?] local\n P crayon      * 1.5.2    2022-09-29 [?] CRAN (R 4.3.0)\n P data.table  * 1.14.9   2023-05-05 [?] Github (Rdatatable/data.table@8803918)\n P datawizard  * 0.9.0    2023-09-15 [?] CRAN (R 4.3.1)\n P DBI         * 1.1.3    2022-06-18 [?] CRAN (R 4.3.0)\n P dbplyr      * 2.4.0    2023-10-26 [?] CRAN (R 4.3.1)\n P downlit     * 0.4.3    2023-06-29 [?] CRAN (R 4.3.0)\n P dplyr       * 1.1.3    2023-09-03 [?] RSPM (R 4.3.0)\n P duckdb      * 0.9.1    2023-10-13 [?] CRAN (R 4.3.1)\n P fuzzyjoin   * 0.1.6    2020-05-15 [?] CRAN (R 4.3.0)\n P ggblend     * 0.1.0    2023-05-22 [?] CRAN (R 4.3.1)\n P ggplot2     * 3.4.4    2023-10-12 [?] CRAN (R 4.3.1)\n P ggtext      * 0.1.2    2022-09-16 [?] CRAN (R 4.3.0)\n P gt          * 0.10.0   2023-10-07 [?] CRAN (R 4.3.1)\n P gtools      * 3.9.4    2022-11-27 [?] CRAN (R 4.3.0)\n P here        * 1.0.1    2020-12-13 [?] CRAN (R 4.3.0)\n P leaflet     * 2.2.0    2023-08-31 [?] CRAN (R 4.3.0)\n P lubridate   * 1.9.3    2023-09-27 [?] CRAN (R 4.3.1)\n P nplyr       * 0.2.0    2023-02-14 [?] CRAN (R 4.3.0)\n P patchwork   * 1.1.3    2023-08-14 [?] CRAN (R 4.3.0)\n P plotly      * 4.10.2   2023-06-03 [?] CRAN (R 4.3.0)\n P posterior   * 1.4.1    2023-03-14 [?] CRAN (R 4.3.0)\n P purrr       * 1.0.2    2023-08-10 [?] CRAN (R 4.3.0)\n P quarto      * 1.3      2023-09-19 [?] CRAN (R 4.3.1)\n P Rcpp        * 1.0.11   2023-07-06 [?] CRAN (R 4.3.0)\n P readr       * 2.1.4    2023-02-10 [?] CRAN (R 4.3.0)\n P sessioninfo * 1.2.2    2021-12-06 [?] CRAN (R 4.3.0)\n P sf          * 1.0-14   2023-07-11 [?] CRAN (R 4.3.0)\n P stringr     * 1.5.0    2022-12-02 [?] CRAN (R 4.3.0)\n P tibble      * 3.2.1    2023-03-20 [?] CRAN (R 4.3.0)\n P tidybayes   * 3.0.6    2023-08-12 [?] CRAN (R 4.3.0)\n P tidyr       * 1.3.0    2023-01-24 [?] CRAN (R 4.3.0)\n P xml2        * 1.3.5    2023-07-06 [?] CRAN (R 4.3.0)\n\n [1] /home/mar/Dev/Projects/R/ma-riviere.com/renv/library/R-4.3/x86_64-pc-linux-gnu\n [2] /home/mar/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "content/code/index.html",
    "href": "content/code/index.html",
    "title": " Code",
    "section": "",
    "text": "Code Projects\n\n\n\n\n\n\n\n\n\n\n\nDE-AoP: Data & Analyses\n\n\nData and R code for the DE-AoP project and paper\n\n\n\nBiostatistics\n\n\nBioinformatics\n\n\nTranscriptomics\n\n\nImmunohistochemistry\n\n\nR\n\n\n\nRT-qPCR and immunohistochemistry data for the DE-AoP project were analysed in R, hosted on GitHub, and archived through Zenodo.\n\n\n\n\n\nJul 13, 2023\n\n\nMarc-Aurèle Rivière, Agalic Rodriguez-Duboc\n\n\n\n\n\n\n\n\n\n\n\n\nLT-AoP: Data & Analyses\n\n\nData and R code for the LT-AoP project and paper\n\n\n\nBiostatistics\n\n\nBioinformatics\n\n\nTranscriptomics\n\n\nImmunohistochemistry\n\n\nBehavioral\n\n\nR\n\n\n\nBehavioral, immunohistochemistry and RT-qPCR data for the LT-AoP project were analysed in R, hosted on GitHub, and archived through Zenodo.\n\n\n\n\n\nMay 1, 2022\n\n\nMarc-Aurèle Rivière, Agalic Rodriguez-Duboc\n\n\n\n\n\n\nNo matching items\n\n\n\n\n Blog Posts\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nMCMC for ‘Big Data’ with Stan\n\n\nFaster sampling with CmdStan using within-chain parallelization\n\n\n\nStatistics\n\n\nML\n\n\nBayesian Modeling\n\n\nBig Data\n\n\nStan\n\n\nR\n\n\n\nThis post is an extension (and a translation to R) of PyMC-Labs’ benchmarking of MCMC for “Big Data”.\nThe Stan code was updated to use within-chain parallelization and compiler optimization for faster CPU sampling. Stan was able to achieve similar sampling speeds as PyMC’s JAX + GPU solution, purely on CPU. \n\n\n\n\n\nJun 5, 2022\n\n\n10 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to topReuseCC BY 4.0"
  },
  {
    "objectID": "content/code/projects/DE-AoP/index.html",
    "href": "content/code/projects/DE-AoP/index.html",
    "title": "DE-AoP: Data & Analyses",
    "section": "",
    "text": "Back to topReferences\n\nBrooks, M. E., Kristensen, K., van Benthem, K. J., Magnusson, A., Berg, C. W., Nielsen, A., Skaug, H. J., Maechler, M., & Bolker, B. M. (2017). glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling. The R Journal, 9(2), 378–400. https://journal.r-project.org/archive/2017/RJ-2017-066/index.html\n\n\nHartig, F. (2022). DHARMa: Residual diagnostics for hierarchical (multi-level / mixed) regression models. https://CRAN.R-project.org/package=DHARMa\n\n\nLenth, R. V. (2022). Emmeans: Estimated marginal means, aka least-squares means. https://CRAN.R-project.org/package=emmeans\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nR Core Team. (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/"
  },
  {
    "objectID": "content/projects/ACCESSPACE/index.html",
    "href": "content/projects/ACCESSPACE/index.html",
    "title": "ACCESSPACE",
    "section": "",
    "text": "Official homepage []\n  \n  \n    \n     Article\n  \n  \n      Talk []\n  \n  \n    \n     Poster []"
  },
  {
    "objectID": "content/projects/ACCESSPACE/index.html#our-interface-the-tactibelt",
    "href": "content/projects/ACCESSPACE/index.html#our-interface-the-tactibelt",
    "title": "ACCESSPACE",
    "section": "2.1 Our interface: the TactiBelt",
    "text": "2.1 Our interface: the TactiBelt\nTo provide the proposed egocentric encoding scheme to the user, we designed a vibro-tactile belt, the TactiBelt: it comprises of 46 ERM motors spread into three layers, controlled by an Arduino Mega, through a specialized software written in Java:\n\n\n\n\n\n\nFigure 2: First prototype of the TactiBelt"
  },
  {
    "objectID": "content/projects/ACCESSPACE/index.html#software-tools",
    "href": "content/projects/ACCESSPACE/index.html#software-tools",
    "title": "ACCESSPACE",
    "section": "2.2 Software tools",
    "text": "2.2 Software tools\nTo capture and extract the information we need from the VIP’s environment, we devised a series of software tools relying mostly on Computer Vision:\n1) Obstacle detection and indoor localisation using the ORB-SLAM algorithm:\n\n2) Depth estimation from a monocular RGB camera, using the MonoDepth algorithm:\n\n3) Generating a mobility graph of the environment during movement using Reinforcement Learning:\nApplied to an artificial agent exploring a virtual maze, looking for food:\n\nApplied to a real agent (human pushing a cart with a camera and the computer running the algorithm around a meeting table):\n\n4) A virtual environment to test the TactiBelt and our candidate spatial encoding schemes:"
  },
  {
    "objectID": "content/projects/ACCESSPACE/index.html#project-dissemination",
    "href": "content/projects/ACCESSPACE/index.html#project-dissemination",
    "title": "ACCESSPACE",
    "section": "2.3 Project dissemination",
    "text": "2.3 Project dissemination\nThe ACCESSPACE project was promoted in various mainstream and technical media, such as:\n- On RTL, a French national radio station ()\n- On PhDTalent, a platform and network for PhD Students who wish to transition to industry ()\n- On Guide Néret, a specialized website on Handicap in France ()\n- On Acuité, a specialized website dedicated to Opticians and news around visual impairment ()\n- On Oxytude, a weekly podcast reviewing news related to visual impairment ()\n- On FIRAH, the French Foundation on Applied Research for Handicap ()\n\n\n\n\n\n\nThis project has been warmly welcomed by the VIP community and was awarded the “Applied research on disability” award from the CCAH in 2017 🥇"
  },
  {
    "objectID": "content/projects/CamIO/index.html",
    "href": "content/projects/CamIO/index.html",
    "title": "CamIO",
    "section": "",
    "text": "Official homepage\n  \n  \n    \n     Article (conference)\n  \n  \n      Article (journal)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n1 Introduction\n\n\n\n\n2 My role in this project\n\n1) Explore new solutions to improve the localisation & tracking capabilities of CamIO:\nTheir existing solution, iLocalize (Fusco & Coughlan, 2018) (Swift / iOS), used a combination of Visuo-Inertial Odometry (VIO) through Apple’s ARKit, particle filtering based on a simplified map of the environment, and drift-correction through visual identification of known landmarks (using a gradient boosting algorithm).\n\nI developed a web app to send the live camera stream from a mobile phone (JavaScript / socket.io) to a backend server (Python / Flask). The goal of the application was to facilitate the exploration of new Computer Vision algorithms to process the captured video and IMU data, which would send back location or navigational information.\nI also explored existing 3rd-party services for indoor localization, such as Indoor Atlas (which combines VIO, GPS data, WiFi & geomagnetic fingerprinting, dead-reckoning, and barometric readings for altitude changes), for which I made a small demo.\n\n\n\n\n\n\n\n\n\n\n\n(a) Indoor Atlas’ localization\n\n\n\n\n\n\n\n\n\n\n\n(b) Indoor Atlas’ navigation graph\n\n\n\n\n\n\n\nFigure 1: Indoor Atlas\n\n\n\n2) Assist in analyzing the data and writing a scientific paper presenting the project.\n\n\n\n\n\n\n Back to topReferences\n\nFusco, G., & Coughlan, J. M. (2018). Indoor localization using computer vision and visual-inertial odometry (pp. 86–93). Springer International Publishing. https://doi.org/10.1007/978-3-319-94274-2_13"
  },
  {
    "objectID": "content/projects/LT-AoP/index.html",
    "href": "content/projects/LT-AoP/index.html",
    "title": "LT-AoP",
    "section": "",
    "text": "Article\n  \n  \n      Data & Analyses\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n1 My role in this project\n\n1) Handled the data processing and analysis, for both immunohistochemistry and RT-qPCR data. The code for those analyses was made open-source and registered on Zenodo, while the results of the project were published in Cell & Bioscience.\n2) Made a website documenting and showcasing the project’s data, analyses, and results. The website uses Quarto and relies on templates to automatically generates documentation for each of the ~70 variables analyzed during the project.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/projects/SAM-Guide/index.html",
    "href": "content/projects/SAM-Guide/index.html",
    "title": "SAM-Guide",
    "section": "",
    "text": "Official homepage\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n1 Introduction\n\n\nInteracting with space is a constant challenge for Visually Impaired People (VIP) since spatial information in Humans is typically provided by vision. Sensory Substitution Devices (SSDs) have been promising Human-Machine Interfaces (HMI) to assist VIP. They re-code missing visual information as stimuli for other sensory channels. Our project redirects somehow from SSD’s initial ambition for a single universal integrated device that would replace the whole sense organ, towards common encoding schemes for multiple applications.\nSAM-Guide will search for the most natural way to give online access to geometric variables that are necessary to achieve a range of tasks without eyes. Defining such encoding schemes requires selecting a crucial set of geometrical variables, and building efficient and comfortable auditory and/or tactile signals to represent them. We propose to concentrate on action-perception loops representing target-reaching affordances, where spatial properties are defined as ego-centered deviations from selected beacons.\nThe same grammar of cues could better help VIP to get autonomy along with a range of vital or leisure activities. Among such activities, the consortium has advances in orienting and navigating, object locating and reaching, laser shooting. Based on current neurocognitive models of human action-perception and spatial cognition, the design of the encoding schemes will lay on common theoretical principles: parsimony (minimum yet sufficient information for a task), congruency (leverage existing sensorimotor control laws), and multimodality (redundant or complementary signals across modalities). To ensure an efficient collaboration all partners will develop and evaluate their transcoding schemes based on common principles, methodology, and tools. An inclusive user-centered “living-lab” approach will ensure constant adequacy of our solutions with VIP’s needs.\nFive labs (three campuses) comprising ergonomists, neuroscientists, engineers, and mathematicians, united by their interest and experience with designing assistive devices for VIP, will duplicate, combine and share their pre-existing SSDs prototypes: a vibrotactile navigation belt, an audio-spatialized virtual guide for jogging, and an object-reaching sonic pointer. Using those prototypes, they will iteratively evaluate and improve their transcoding schemes in a 3-phase approach: First, in controlled experimental settings through augmented-reality serious games in motion capture (virtual prototyping indeed facilitates the creation of ad-hoc environments, and gaming eases the participants’ engagement). Next, spatial interaction subtasks will be progressively combined and tested in wider and more ecological indoor and outdoor environments. Finally, SAM-Guide’s system will be fully transitioned to real-world conditions through a friendly sporting event of laser-run, a novel handi-sport, which will involve each subtask.\nSAM-Guide will develop action-perception and spatial cognition theories relevant to non-visual interfaces. It will provide guidelines for the efficient representation of spatial interactions to facilitate the emergence of spatial awareness in a task-oriented perspective. Our portable modular transcoding libraries are independent of hardware consideration. The principled experimental platform offered by AR games will be a tool for evaluating VIP spatial cognition, and novel strategies for mobility training.\n\n\n\n2 My role in this project\n\n1) I was a major actor behind the birth of this project, by connecting the consortium members together and writing most of the grant proposal (ANR AAPG 2021, funding of 609k€). This project will last 4 years and allow the recruitment of 2 PhD students, one post-doc, and one Research Engineer.\n2) I designed and participated in the development of the second prototype of our vibro-tactile belt, which features wireless communication (thanks to an ESP32 module) and amovible vibrators:\n\n\n\n\n\n\nFigure 1: Second prototype of the TactiBelt\n\n\n\n3) I lead the design and development of the project’s experimental platform. The platform uses Unity, connects to various motion tracking devices used by the consortium (Polhemus, VICON, pozyx), uses PureData for sound-wave generation and Steam Audio for 3D audio modeling, and communicates with the consortium’s non-visual interfaces wirelessly.\n\n\n\n\n\n\n\n\n\n\n\n(a) Testing environment with a PureData audio beacon\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Auto-generated maze with 3D audio beacons on waypoints\n\n\n\n\n\n\n\nFigure 2: Screenshots from SAM-Guide’s experimental platform (in development)\n\n\n\nThis platform allows one to easily spin up experimental trials by specifying the desired characteristics in a JSON file (based on the OpenMaze project). Unity will automatically generate the trial’s environment according to those specifications and populate it with the relevant items (e.g. a tactile-signal emitting beacon signalling a target to reach in a maze), handle the transition between successive trials and blocks of trials, and log all the relevant user metrics into a data file.\n\n\n\n\n\n\n\n\n\n\n\n(a) Specifying the avatar and the experimental blocks’ characteristics\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Specifying experimental trials, which can be repeated and randomized within blocks\n\n\n\n\n\n\n\nFigure 3: Examples of settings used to generate experimental trials on the fly.\n\n\n\n4) Handled the experimental design of the first wave of experiments using the TactiBelt for “blind” navigation.\n5) Designed the project’s website using Quarto.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/projects/index.html",
    "href": "content/projects/index.html",
    "title": " Past & Current Projects",
    "section": "",
    "text": "SAM-Guide\n\n\nSpatial Awareness for Multimodal Guidance\n\n\n\nResearch\n\n\nSoftware Engineering\n\n\nData Science\n\n\nHuman-Computer Interaction\n\n\nAugmented Reality\n\n\nSensory Substitution\n\n\nAuditory Interface\n\n\nHaptic Interface\n\n\n\nDesigning an efficient multi-modal interface to help VIP during spatial interactions and sports.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDE-AoP\n\n\nDevelopmental effects of Apnea of Prematurity\n\n\n\nResearch\n\n\nData Science\n\n\nSoftware Engineering\n\n\nBiostatistics\n\n\nTranscriptomics\n\n\nCerebellum\n\n\nHypoxia\n\n\nRT-qPCR\n\n\n\nThis project studies the underlying molecular and cellular mechanisms of apnea of prematurity at play during cerebellar development, using intermittent hypoxia in a mouse model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNAV-VIR\n\n\nVirtual Map exploration for Visually Impaired People\n\n\n\nResearch\n\n\nSoftware Engineering\n\n\nHuman-Computer Interaction\n\n\nVirtual Reality\n\n\nHaptic Interface\n\n\nAuditory Interface\n\n\nSensory Substitution\n\n\n\nDeveloping a multi-modal interface for Visually Impaired People to virtually explore a map in order to prepare for a journey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCamIO\n\n\nCamera Input-Output\n\n\n\nResearch\n\n\nSoftware Engineering\n\n\nData Science\n\n\nHuman-Computer Interaction\n\n\nAugmented Reality\n\n\nAuditory Interface\n\n\nSensory Substitution\n\n\nComputer Vision\n\n\n\nSmart pen providing real-time audio-feedback on objects using the smartphone’s sensors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTETMOST\n\n\nMaking Art more accessible to Visually Impaired People\n\n\n\nResearch\n\n\nSoftware Engineering\n\n\nHuman-Computer Interaction\n\n\nVirtual Reality\n\n\nSensory Substitution\n\n\nHaptic Interface\n\n\n\nDevelopping a haptic interface and studying ways to intuitively represent images and Art pieces haptically for Visually Impaired People.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nACCESSPACE\n\n\nHelping Visually Impaired People travel autonomously\n\n\n\nResearch\n\n\nSoftware Engineering\n\n\nData Science\n\n\nHuman-Computer Interaction\n\n\nAugmented Reality\n\n\nHaptic Interface\n\n\nSensory Substitution\n\n\nComputer Vision\n\n\n\nDeveloping a wearable vibro-tactile electronic Orientation & Travel Aid for the autonomous navigation of VIP, based on Spatial Cognition models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLT-AoP\n\n\nLong Term effects of Apnea of Prematurity\n\n\n\nResearch\n\n\nSoftware Engineering\n\n\nData Science\n\n\nBiostatistics\n\n\nTranscriptomics\n\n\nCerebellum\n\n\nHypoxia\n\n\nRT-qPCR\n\n\n\nThis project studied the impact of apnea of prematurity on cerebellar development and the long-term functional deficits resulting from it, using intermittent hypoxia in a mouse model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdViS\n\n\nAdaptative Visual Substitution\n\n\n\nResearch\n\n\nSoftware Engineering\n\n\nData Science\n\n\nHuman-Computer Interaction\n\n\nAugmented Reality\n\n\nAuditory Interface\n\n\nSensory Substitution\n\n\nComputer Vision\n\n\n\nDeveloping a wearable visuo-auditive substitution system to assist Visually Impaired People in navigation and object-reaching tasks.\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/pubs/ICCHP18-F2T/index.html",
    "href": "content/pubs/ICCHP18-F2T/index.html",
    "title": "Towards Haptic Surface Devices with Force Feedback for Visually Impaired People",
    "section": "",
    "text": "Article (PDF)\n  \n  \n      Project (TETMOST)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@inproceedings{gay2018,\n  author = {Gay, Simon and Rivière, Marc-Aurèle and Pissaloux, Edwige},\n  editor = {Klaus, Miesenberger and Georgios, Kouroupetroglou},\n  publisher = {Springer International Publishing},\n  title = {Towards {Haptic} {Surface} {Devices} with {Force} {Feedback}\n    for {Visually} {Impaired} {People}},\n  booktitle = {Lecture Notes in Computer Science},\n  volume = {10897},\n  pages = {258-266},\n  date = {2018-07-12},\n  url = {http://link.springer.com/10.1007/978-3-319-94274-2_36},\n  doi = {10.1007/978-3-319-94274-2_36},\n  isbn = {978-3-319-94273-5 978-3-319-94274-2},\n  langid = {en},\n  abstract = {This paper presents a new haptic surface tablet that can\n    provide force feedback to the user. Force feedback means that the\n    device can react to the user’s movements and apply a force against\n    or in-line with these movements, according to the tactile properties\n    of a displayed image. The device consists of a frame attached to a\n    tactile tablet that generates a force feedback to user’s finger when\n    exploring the surface, providing haptic informations about the\n    displayed image. The experimental results suggest the relevance of\n    this tablet as an assistive device for visually impaired people in\n    perceiving and understanding the content of a displayed image.\n    Several potential applications are briefly presented.}\n}\nFor attribution, please cite this work as:\nGay, S., Rivière, M.-A., & Pissaloux, E. (2018). Towards Haptic\nSurface Devices with Force Feedback for Visually Impaired People. In M.\nKlaus & K. Georgios (Eds.), Lecture Notes in Computer\nScience (Vol. 10897, pp. 258–266). Springer International\nPublishing. https://doi.org/10.1007/978-3-319-94274-2_36"
  },
  {
    "objectID": "content/pubs/ICCHP20/index.html",
    "href": "content/pubs/ICCHP20/index.html",
    "title": "An Audio-Based 3D Spatial Guidance AR System for Blind Users",
    "section": "",
    "text": "Article (PDF)\n  \n  \n    \n     Code\n  \n  \n      Project (CamIO)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@inproceedings{coughlan2020,\n  author = {Coughlan, James and Biggs, Brandon and Rivière, Marc-Aurèle\n    and Shen, Huiying},\n  editor = {Klaus, Miesenberger and Roberto, Manduchi and Rodriguez\n    Mario, Covarrubias and Petr, Peňáz},\n  publisher = {Springer International Publishing},\n  title = {An {Audio-Based} {3D} {Spatial} {Guidance} {AR} {System} for\n    {Blind} {Users}},\n  booktitle = {Lecture Notes in Computer Science},\n  volume = {12376},\n  pages = {475-484},\n  date = {2020-09-12},\n  url = {https://link.springer.com/chapter/10.1007/978-3-030-58796-3_55},\n  doi = {10.1007/978-3-030-58796-3_55},\n  isbn = {978-3-030-58795-6 978-3-030-58796-3},\n  langid = {en},\n  abstract = {Augmented reality (AR) has great potential for blind users\n    because it enables a range of applications that provide audio\n    information about specific locations or directions in the user’s\n    environment. For instance, the {[}CamIO{]}(/content/projects/CamIO)\n    (“Camera Input-Output”) AR app makes physical objects (such as\n    documents, maps, devices and 3D models) accessible to blind and\n    visually impaired persons by providing real-time audio feedback in\n    response to the location on an object that the user is touching\n    (using an inexpensive stylus). An important feature needed by blind\n    users of AR apps such as CamIO is a 3D spatial guidance feature that\n    provides real-time audio feedback to help the user find a desired\n    location on an object. We have devised a simple audio interface to\n    provide verbal guidance towards a target of interest in 3D. The\n    experiment we report with blind participants using this guidance\n    interface demonstrates the feasibility of the approach and its\n    benefit for helping users find locations of interest.}\n}\nFor attribution, please cite this work as:\nCoughlan, J., Biggs, B., Rivière, M.-A., & Shen, H. (2020). An\nAudio-Based 3D Spatial Guidance AR System for Blind Users. In M. Klaus,\nM. Roberto, C. Rodriguez Mario, & P. Petr (Eds.), Lecture Notes\nin Computer Science (Vol. 12376, pp. 475–484). Springer\nInternational Publishing. https://doi.org/10.1007/978-3-030-58796-3_55"
  },
  {
    "objectID": "content/pubs/JEP22/index.html",
    "href": "content/pubs/JEP22/index.html",
    "title": "Spatiotemporal influences on the recognition of two-dimensional vibrotactile patterns on the abdomen",
    "section": "",
    "text": "Article (PDF)\n  \n  \n      Data\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{faugloire2022,\n  author = {Faugloire, Elise and Lejeune, Laure and Rivière, Marc-Aurèle\n    and Mantel, Bruno},\n  title = {Spatiotemporal Influences on the Recognition of\n    Two-Dimensional Vibrotactile Patterns on the Abdomen},\n  journal = {Journal of Experimental Psychology: Applied},\n  volume = {28},\n  number = {3},\n  pages = {606-628},\n  date = {2022-09-02},\n  url = {https://psycnet.apa.org/record/2022-01207-001},\n  doi = {10.1037/xap0000404},\n  issn = {1939-2192, 1076-898X},\n  langid = {en},\n  abstract = {Spatial and temporal factors are known to highly influence\n    tactile perception, but their role has been largely unexplored in\n    the case of two-dimensional (2D) pattern recognition. We\n    investigated whether recognition is facilitated by the spatial\n    and/or temporal separation of pattern elements, or by conditions\n    known to favor perceptual integration, such as the ones eliciting\n    apparent movement. 2D vibrotactile patterns were presented to the\n    abdomen of novice participants. In Experiment 1, we manipulated the\n    spatial (inter-tactor distance) and temporal (burst duration and\n    inter-burst interval) parameters applied to the tracing mode\n    (sequential activation of pattern elements). In Experiment 2, we\n    compared display modes differing in their level of temporal overlap\n    in the presentation of pattern elements: the static mode\n    (simultaneous activation of pattern elements), the slit-scan mode\n    (pattern revealed line by line), and the tracing mode. The results\n    of both experiments reveal that (a) recognition performance\n    increases with the isolation of pattern elements in space and/or in\n    time, (b) spatial and temporal factors interact in pattern\n    recognition, and (c) conditions leading to apparent movement tend to\n    be associated with lower recognition accuracy. These results further\n    our understanding of tactile perception and provide guidance for the\n    design of future vibrotactile communication systems.}\n}\nFor attribution, please cite this work as:\nFaugloire, E., Lejeune, L., Rivière, M.-A., & Mantel, B. (2022).\nSpatiotemporal influences on the recognition of two-dimensional\nvibrotactile patterns on the abdomen. Journal of Experimental\nPsychology: Applied, 28(3), 606–628. https://doi.org/10.1037/xap0000404"
  },
  {
    "objectID": "content/pubs/index.html",
    "href": "content/pubs/index.html",
    "title": " Scientific Publications",
    "section": "",
    "text": "Apnea of Prematurity induces short and long-term development-related transcriptional changes in the murine cerebellum\n\n\nCurrent Research in Neurobiology\n\n\n\nNeuroscience\n\n\nMolecular Neuroscience\n\n\nNeurodevelopment\n\n\nCerebellum\n\n\nApnea of Prematurity\n\n\nRT-qPCR\n\n\n\nThis paper aims to present transcriptomic data that participates in understanding the molecular basis for the histological and behavioral abnormalities caused by intermittent hypoxia.\n\n\n\n\n\nOct 20, 2023\n\n\nAgalic Rodriguez-Duboc, Magali Basille-Duguay, Aurélien Debonne, Marc-Aurèle Rivière, David Vaudry, Delphine Burel\n\n\n\n\n\n\n\nSpatiotemporal influences on the recognition of two-dimensional vibrotactile patterns on the abdomen\n\n\nJournal of Experimental Psychology: Applied\n\n\n\nCognitive Psychology\n\n\nAccessibility\n\n\nSensory Substitution\n\n\nHaptic Interface\n\n\nPsychophysics\n\n\n\nThis study reveals that patterns made up of several vibration points are better recognized when pattern elements are clearly isolated in time and space. The feeling of a single point moving continuously along the skin, as if the pattern was manually drawn on the skin, does not appear to favor the recognition of patterns’ shape\n\n\n\n\n\nSep 2, 2022\n\n\nElise Faugloire, Laure Lejeune, Marc-Aurèle Rivière, Bruno Mantel\n\n\n\n\n\n\n\nAn Audio-Based 3D Spatial Guidance AR System for Blind Users\n\n\nLecture Notes in Computer Science\n\n\n\nEngineering\n\n\nComputer Vision\n\n\nAuditory Interface\n\n\nAugmented Reality\n\n\nCognitive Psychology\n\n\nAccessibility\n\n\nSensory Substitution\n\n\n\nWe introduce CamIO, an AR app for Visually Impaired People (VIP) to reach objects in their immediate environment through real-time 3D audio guidance\n\n\n\n\n\nSep 12, 2020\n\n\nJames Coughlan, Brandon Biggs, Marc-Aurèle Rivière, Huiying Shen\n\n\n\n\n\n\n\nTowards the Tactile Discovery of Cultural Heritage with Multi-approach Segmentation\n\n\nLecture Notes in Computer Science\n\n\n\nEngineering\n\n\nComputer Vision\n\n\nSensory Substitution\n\n\nHaptic Interface\n\n\nAccessibility\n\n\n\nWe introduce preliminary work on using multi-approach image segmentation, combined with a force-feedback interface, to provide access to Artworks to Visually Impaired People\n\n\n\n\n\nJul 8, 2020\n\n\nAli Souradi, Christele Lecomte, Katerine Romeo, Simon Gay, Marc-Aurèle Rivière, Abderrahim El Moataz, Edwige Pissaloux\n\n\n\n\n\n\n\nNAV-VIR: an audio-tactile virtual environment to assist visually impaired people\n\n\nInternational IEEE/EMBS Conference on Neural Engineering\n\n\n\nCognitive Psychology\n\n\nSensory Substitution\n\n\nAccessibility\n\n\nEngineering\n\n\nAuditory Interface\n\n\nHaptic Interface\n\n\nAugmented Reality\n\n\n\nWe introduce NAV-VIR, a multimodal interface for the interactive exploration of maps by Visually Impaired People\n\n\n\n\n\nMay 20, 2019\n\n\nMarc-Aurèle Rivière, Simon Gay, Katerine Romeo, Edwige Pissaloux, Michal Bujacz, Piotr Skulimowski, Pawel Strumillo\n\n\n\n\n\n\n\nTactiBelt: integrating spatial cognition and mobility theories into the design of a novel orientation and mobility assistive device for the blind\n\n\nLecture Notes in Computer Science\n\n\n\nCognitive Psychology\n\n\nSensory Substitution\n\n\nAccessibility\n\n\nNeuroscience\n\n\nSpatial Cognition\n\n\nHaptic Interface\n\n\nAugmented Reality\n\n\n\nWe introduce spatial cognition models as a framework to design novel mobility assistive devices for Visually Impaired People\n\n\n\n\n\nJul 12, 2018\n\n\nMarc-Aurèle Rivière, Simon Gay, Edwige Pissaloux\n\n\n\n\n\n\n\nTowards Haptic Surface Devices with Force Feedback for Visually Impaired People\n\n\nLecture Notes in Computer Science\n\n\n\nCognitive Psychology\n\n\nSensory Substitution\n\n\nAccessibility\n\n\nEngineering\n\n\nHaptic Interface\n\n\nAugmented Reality\n\n\n\nWe introduce the principles of a haptic interface for finger-movement based exploration of image content through force-feedback\n\n\n\n\n\nJul 12, 2018\n\n\nSimon Gay, Marc-Aurèle Rivière, Edwige Pissaloux\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "content/sci-com/outreach/Exp-JPO/index.html",
    "href": "content/sci-com/outreach/Exp-JPO/index.html",
    "title": "Percieving our world without vision",
    "section": "",
    "text": "Project (ACCESSPACE)\n  \n\n      \n\n    \n    \n  \n\n\n\nHere is a video recording of a talk I gave to high-school students during my University’s College Fair, in collaboration with the Expérimentarium program, filmed by the University:\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/sci-com/talks/ICCHP18/index.html",
    "href": "content/sci-com/talks/ICCHP18/index.html",
    "title": "TactiBelt: an Electronic Travel Aid prototype for VIP",
    "section": "",
    "text": "Project (ACCESSPACE)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/sci-com/talks/RUNN19/index.html",
    "href": "content/sci-com/talks/RUNN19/index.html",
    "title": "ACCESSPACE: overview and future prospects",
    "section": "",
    "text": "Project (ACCESSPACE)\n  \n\n      \n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThis talk got the best presentation award of RUNN’19 🥇\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/code/posts/big-bayes/index.html#quartoknitr-setup",
    "href": "content/code/posts/big-bayes/index.html#quartoknitr-setup",
    "title": "MCMC for ‘Big Data’ with Stan",
    "section": "Quarto/knitr setup",
    "text": "Quarto/knitr setup\n\nTest#---------------------------#\n####🔺ggplot knit_prints ####\n#---------------------------#\n\nlibrary(knitr)\nlibrary(ggplot2)\n\n## Inspired by: https://debruine.github.io/quarto_demo/dark_mode.html\nknit_print.ggplot &lt;- function(x, options, ...) {\n  if(any(grepl(\"patchwork\", class(x)))) {\n    plot_dark &lt;- x & dark_addon_mar\n    plot_light &lt;- x & light_addon_mar\n  } else {\n    plot_dark &lt;- x + dark_addon_mar\n    plot_light &lt;- x + light_addon_mar\n  }\n  \n  cat('\\n&lt;div class=\"light-mode\"&gt;\\n')\n  print(plot_light)\n  cat('&lt;/div&gt;\\n')\n  cat('&lt;div class=\"dark-mode\"&gt;\\n')\n  print(plot_dark)\n  cat('&lt;/div&gt;\\n\\n')\n}\nregisterS3method(\"knit_print\", \"ggplot\", knit_print.ggplot)\n#-----------------------#\n####🔺gt knit_prints ####\n#-----------------------#\n\nlibrary(knitr)\nlibrary(gt)\n\nknit_print.grouped_df &lt;- function(x, options, ...) {\n  if (\"grouped_df\" %in% class(x)) x &lt;- ungroup(x)\n  \n  cl &lt;- intersect(class(x), c(\"data.table\", \"data.frame\"))[1]\n  nrows &lt;- ifelse(!is.null(options$total_rows), as.numeric(options$total_rows), dim(x)[1])\n  is_open &lt;- ifelse(!is.null(options[[\"details-open\"]]), as.logical(options[[\"details-open\"]]), FALSE)\n  \n  cat(glue::glue(\"\\n&lt;details{ifelse(is_open, ' open', '')}&gt;\\n\"))\n  cat(\"&lt;summary&gt;\\n\")\n  cat(glue::glue(\"\\n*{cl} [{scales::label_comma()(nrows)} x {dim(x)[2]}]*\\n\"))\n  cat(\"&lt;/summary&gt;\\n&lt;br&gt;\\n\")\n  print(gt::as_raw_html(style_table(x, nrows)))\n  cat(\"&lt;/details&gt;\\n\\n\")\n}\nregisterS3method(\"knit_print\", \"grouped_df\", knit_print.grouped_df)\n\nknit_print.data.frame &lt;- function(x, options, ...) {\n  cl &lt;- intersect(class(x), c(\"data.table\", \"data.frame\"))[1]\n  nrows &lt;- ifelse(!is.null(options$total_rows), as.numeric(options$total_rows), dim(x)[1])\n  is_open &lt;- ifelse(!is.null(options[[\"details-open\"]]), as.logical(options[[\"details-open\"]]), FALSE)\n  \n  cat(glue::glue(\"\\n&lt;details{ifelse(is_open, ' open', '')}&gt;\\n\"))\n  cat(\"&lt;summary&gt;\\n\")\n  cat(glue::glue(\"\\n*{cl} [{scales::label_comma()(nrows)} x {dim(x)[2]}]*\\n\"))\n  cat(\"&lt;/summary&gt;\\n&lt;br&gt;\\n\")\n  print(gt::as_raw_html(style_table(x, nrows)))\n  cat(\"&lt;/details&gt;\\n\\n\")\n}\nregisterS3method(\"knit_print\", \"data.frame\", knit_print.data.frame)\nlibrary(htmltools)\nlibrary(reactable)\n\n## Getting list to display nicely in rendered documents\nmake_list_reactable &lt;- function(list_dat) {\n  \n  list_name &lt;- deparse(substitute(list_dat))\n  \n  get_list_elt_dim &lt;- function(elt) {\n    list_elt &lt;- list_dat[[elt]]\n    list_elt_dim &lt;- if (any(c(\"data.frame\", \"matrix\") %in% class(list_elt))) dim(list_elt) else length(list_elt)\n    \n    return(paste0(list_elt_dim, collapse = \", \"))\n  }\n  \n  dat &lt;- data.frame(names(list_dat)) |&gt; \n    set_names(list_name) |&gt; \n    mutate(\n      Type = unlist(pick(list_name)) |&gt; map_chr(\\(x) class(list_dat[[x]]) |&gt; paste0(collapse = \", \")),\n      Dimensions = unlist(pick(list_name)) |&gt; map_chr(get_list_elt_dim)\n    )\n  \n  get_list_details &lt;- function(dat, idx, max_print = 200, max_digits = 3) {\n    Element &lt;- dat[[idx]]\n    style &lt;- \"padding: 0.5rem\"\n    \n    if (any(c(\"data.frame\", \"matrix\") %in% class(Element))) {\n      reactable(data.frame(Element), outlined = TRUE, striped = TRUE, highlight = TRUE, compact = TRUE) |&gt; \n        htmltools::div(style = style) \n    }\n    else if (\"list\" %in% class(Element)) \n      make_list_reactable(Element)\n    else if (length(Element) &gt; max_print) {\n      htmltools::div(\n        htmltools::p(head(Element, max_print) |&gt; round(max_digits) |&gt; paste0(collapse = \", \") |&gt; paste(\"...\", sep = \", \")),\n        htmltools::p(stringr::str_glue(\"[ omitted {length(Element) - max_print} entries ]\"), style = \"font-style: italic\"),\n        style = style\n      )\n    }\n    else htmltools::div(round(Element, max_digits) |&gt; paste0(collapse = \", \"), style = style)\n  }\n  \n  reactable(\n    dat\n    , defaultColDef = colDef(vAlign = \"center\", headerVAlign = \"center\")\n    , details = \\(idx) get_list_details(list_dat, idx)\n    , outlined = TRUE\n    , striped = TRUE\n    , highlight = TRUE\n    , compact = FALSE\n    , fullWidth = TRUE\n    , defaultPageSize = 15\n  )\n}\nlibrary(knitr)\n\n## Adding the `time_it` code chunk option\nknitr::knit_hooks$set(time_it = local({\n  assign(\"TIMES\", list(), .GlobalEnv)\n  start &lt;- NULL\n  function(before, options) {\n    if (before) start &lt;&lt;- Sys.time()\n    else TIMES[[options$label]] &lt;&lt;- difftime(Sys.time(), start)\n  }\n}))"
  }
]